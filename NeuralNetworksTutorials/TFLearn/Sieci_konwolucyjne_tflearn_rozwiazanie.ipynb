{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sieci konwolucyjne tflearn rozwiazanie.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skrzypczykt/MAchineLearningProjects/blob/main/NeuralNetworksTutorials/TFLearn/Sieci_konwolucyjne_tflearn_rozwiazanie.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1KATslZWXXN"
      },
      "source": [
        "# Runtime i instalacja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0r_G8i3xZ6X"
      },
      "source": [
        "Zacznijmy od włączenia wsparcia obliczeń kartą graficzną (GPU), która znacznie przyspieszy nasze działania. Może się zdażyć, że nie będzie dostępnych GPU w Colaboratory – wtedy można pracować bez niego, jedynie obliczenia potrwają kilka razy dłużej.\n",
        "\n",
        "Wybieramy w menu **`Runtime`** -> **`Change runtime type`** , a następnie w podmenu **`Hardware accelerator`** wybieramy **`GPU`**.\n",
        "\n",
        "\n",
        "<img style=\"float:left\" src=\"https://www.mimuw.edu.pl/~mm319369/priv/d73890416bec03ff3e2b3756af8c941c/images/change-runtime1.png\">\n",
        "<img src=\"https://www.mimuw.edu.pl/~mm319369/priv/d73890416bec03ff3e2b3756af8c941c/images/change-runtime2.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KtARNGjy38J"
      },
      "source": [
        "## TFlearn\n",
        "\n",
        "W tym zadaniu będziemy uczyć sieci neuronowe i obserwować jak zachowują się, gdy zmieniamy liczbę neuronów w sieci, funkcje aktywacji i co się dzieje, gdy sieć jest za duża względem danych. Wasze zadanie będzie polegało na odpowiedniej konfiguracji sieci, a następnie narysowaniu wykresów zaobserwowanych rezultatów.\n",
        "\n",
        "Będziemy korzystać z biblioteki [*tflearn*](http://tflearn.org/getting_started/), która jest prostym interfejsem do `tensorflow`.\n",
        "\n",
        "Poniżej ją instalujemy i importujemy potrzebne biblioteki."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsR41LUrWXXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "280c3d9a-a9f3-4b93-9a7b-f45631dd1058"
      },
      "source": [
        "!pip install tflearn \"tensorflow<2\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tflearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/3c/0b156d08ef3d4e2a8009ecab2af1ad2e304f6fb99562b6271c68a74a4397/tflearn-0.5.0.tar.gz (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 7.2MB/s \n",
            "\u001b[?25hCollecting tensorflow<2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/51/99abd43185d94adaaaddf8f44a80c418a91977924a7bc39b8dacd0c495b0/tensorflow-1.15.5-cp37-cp37m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 36kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 48.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow<2) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2) (1.1.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2) (0.12.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 34.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2) (1.12.1)\n",
            "Collecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2) (0.8.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2) (3.3.0)\n",
            "Collecting h5py<=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 36.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2) (1.1.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2) (1.34.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow<2) (57.0.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2) (4.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow<2) (3.7.4.3)\n",
            "Building wheels for collected packages: tflearn, gast\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-cp37-none-any.whl size=127300 sha256=cc9ccdd0444695d7c11de25cc0ad749d2169aeb798d7719adcc1b3222fcf19b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d2/ed/fb9a0d301dd9586c11e9547120278e624227f22fd5f4baf744\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=3474f55b6294243caeed4b889a922d78d141ca70314902fd6796f737f6cd03b9\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built tflearn gast\n",
            "\u001b[31mERROR: tensorflow 1.15.5 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tflearn, tensorboard, tensorflow-estimator, h5py, keras-applications, gast, tensorflow\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "Successfully installed gast-0.2.2 h5py-2.10.0 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.5 tensorflow-estimator-1.15.1 tflearn-0.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zMPID31WXXe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5bf0e1d-bffa-4a71-c19d-391ae8b6e6b1"
      },
      "source": [
        "from __future__ import division, print_function, absolute_import\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "import tflearn\n",
        "from tflearn.layers.core import input_data, dropout, fully_connected\n",
        "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
        "from tflearn.layers.normalization import local_response_normalization\n",
        "from tflearn.layers.estimator import regression\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Monkey-patchujemy tflearn, żeby nie wypisywał samemu statystyk,\n",
        "# któe nie działają dobrze w colaboratory\n",
        "tflearn.callbacks.TermLogger.print_termlogs = lambda *args, **kwargs: None"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhVTdq7Z0ama"
      },
      "source": [
        "# Zbiór danych (dataset)\n",
        "\n",
        "Będziemy pracować na już tradycyjnym zbiorze – rozpoznawania odręcznie pisanych cyfr [MNIST](http://yann.lecun.com/exdb/mnist/).\n",
        "Elementy tego zbioru to obrazki 28x28 pikseli w skali szarości, a ich etykietami są cyfry im odpowiadające. Mamy 55000 przykładów treningowych i 10000 przykładów testowych.\n",
        "Dla wydajności obliczeń, wszystkie dane są umieszczane w dużych macierzach (tablicach).\n",
        "\n",
        "Poniższa komórka pobiera i ładuje zbiór danych do pamięci. Wyjaśnienie wprowadzonych zmiennych poniżej.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7FDEJ_dWXXi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ad7b57-cd0b-447b-9b78-4cdc66216f47"
      },
      "source": [
        "# Tu ładujemy nasz dataset - zestaw odręcznie pisanych cyfr od 0 do 9\n",
        "\n",
        "\n",
        "import tflearn.datasets.mnist as mnist\n",
        "X, Y, testX, testY = mnist.load_data(one_hot=True) # ładowanie datasetu (automatycznie ściąga się z internetu)\n",
        "\n",
        "# X,Y to dane treningowe\n",
        "# testX, textY to dane walidacyjne - sprawdzamy nimi jak dobra jest nasza sieć\n",
        "\n",
        "# nasze obrazy mają mieć wymiar 28x28 pikseli - będziemy używać sieci konwolucyjnych, więc konieczna \n",
        "# jest zmiana wymiarów macierzy poleceniem reshape\n",
        "# [samples, x_size, y_size, channels]\n",
        "# samples - liczba próbek \n",
        "# x_size, y_size - wymiary obrazka\n",
        "# channels - liczba kolorów/kanałów - w naszym przypadku 1, bo obrazki są czarno-białe\n",
        "X = X.reshape([-1, 28, 28, 1])\n",
        "testX = testX.reshape([-1, 28, 28, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading MNIST...\n",
            "Succesfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
            "Extracting mnist/train-images-idx3-ubyte.gz\n",
            "Downloading MNIST...\n",
            "Succesfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
            "Extracting mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading MNIST...\n",
            "Succesfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
            "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading MNIST...\n",
            "Succesfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
            "Extracting mnist/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlUYUqFU10a5"
      },
      "source": [
        "**X** oraz **testX** to 4-wymiarowe macierze z przykładami odpowiednio treningowymi i testowymi. Wymiary tych macierzy to \\[*liczba przykładów*, *rozmiar x*, *rozmiar y*, *liczba kolorów*]. Różne funkcje oczekują odpowiednich wymiarów macierzy. Nie będzie to na szczęście dla nas istotne poza zrozumieniem napisów, które się pojawiają:\n",
        "\n",
        "`X[4]`  - przykład numer 4 (wymiar [28, 28, 1]) \\\\\n",
        "`X[2:3]`  - przykład numer 2, ale w wymiarze [1, 28, 28, 1] - odpowiednie dla funkcji/metod tflearn \\\\\n",
        "`X[:10]`  - przykłady numer 0...9, w wymiarze [10, 28, 28, 1] \\\\\n",
        "`X[2,:,:,0]` - przykład 2 w w wymiarze [28, 28] odpowiednim dla funkcji do rysowania\n",
        "\n",
        "**Y** oraz **testY** to 2-wymiarowe macierze z etykietami odpowiednio treningowymi i testowymi. Wymiary tych macierzy to \\[*liczba przykładów*, *liczba cyfr*]. Te macierze reprezentują tzw. kodowanie *one-hot* etykiet  - dany przykład przedstawiający cyfrę **i** ma **1** na **i**-tej pozycji, a na pozostałych **0**.\n",
        "\n",
        "`Y[0]` - etykieta przykładu numer 0 (wymiar [10]) \\\\\n",
        "`Y[2:3]` - etykieta przykładu numer 2, ale w wymiarze [1, 10] - odpowiednie dla funkcji/metod tflearn \\\\\n",
        "`Y[:10]` - etykiety dla przykładów numer 0..9 (j.w.)\n",
        "\n",
        "Poniżej przykład użycia na przykładach numer 0 i 1 (powinny reprezentować 7 i 3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMQYaB8RWXXm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 549
        },
        "outputId": "4448c654-4349-4159-dcdd-c2ef40396e42"
      },
      "source": [
        "# Kilka przykładów z naszego datasetu\n",
        "print(Y[0]) # etykieta - 1 tam, gdzie poprawna klasa\n",
        "plt.imshow(X[0,:,:,0], cmap='gray') # reprezentacja graficzna obrazka\n",
        "plt.show()\n",
        "\n",
        "print(Y[1])\n",
        "plt.imshow(X[1,:,:,0], cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAOHUlEQVR4nO3dXahd9ZnH8d9PbW/SXsScTAw2Jm2RSB0YK1EGJoZKaXy5SXJTGl/IMOopUqHRuZj4ghViggxjR3MTPUVpOlRLyQtKUVobSuLcSN4cjTlJdSS+hJgXvajFi47mmYu9Uk71rP862e/nPN8PHPbe69nr7Mft+WWtvf57rb8jQgBmvvMG3QCA/iDsQBKEHUiCsANJEHYgiQv6+WK2OfQP9FhEeLLlHW3ZbV9v+4jtt2yv6+R3AegttzvObvt8SX+U9D1J70vaI2l1RBwqrMOWHeixXmzZr5b0VkS8HRF/kfQrSSs6+H0AeqiTsF8s6b0Jj9+vlv0N26O299re28FrAehQzw/QRcSYpDGJ3XhgkDrZsh+TtGDC469VywAMoU7CvkfSpba/bvvLkn4g6fnutAWg29rejY+IT23fJem3ks6X9HREvNG1zgB0VdtDb229GJ/ZgZ7ryZdqAEwfhB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiTR9pTNmBkWLlxYrN9+++3F+v3331+sl2YJtiedbPSvxsfHi/UHHnigWN+xY0exnk1HYbd9VNLHkj6T9GlELOlGUwC6rxtb9msj4nQXfg+AHuIzO5BEp2EPSb+zvc/26GRPsD1qe6/tvR2+FoAOdLobvzQijtn+O0kv2T4cEbsnPiEixiSNSZLt+qM1AHqqoy17RByrbk9K2iHp6m40BaD72g677Vm2v3r2vqTlkg52qzEA3eXSOGhxRfsbam3NpdbHgWciYkPDOuzG98DcuXNra/fee29x3ZtvvrlYnzNnTrHeNFbeyTh709/me++9V6xfddVVtbXTp2fuAFJETPrGtv2ZPSLelvQPbXcEoK8YegOSIOxAEoQdSIKwA0kQdiCJtofe2noxht7a0nQa6fr162trTf9/ez38derUqWK9ZGRkpFhftGhRsX7o0KHa2uWXX95OS9NC3dAbW3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIJx9mlgz549xfqVV15ZW+t0nL00Vi1J1157bbHeyamkS5cuLdZ37dpVrJf+2y+4YOZeRZ1xdiA5wg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2IXDZZZcV603j7B9++GFtrel88qZx8LvvvrtYX7t2bbG+cePG2tq7775bXLdJ09/umTNnamt33nlncd2xsbG2ehoGjLMDyRF2IAnCDiRB2IEkCDuQBGEHkiDsQBKMs08DTePwpbHyTqcmHh0dLdY3b95crJemTd6/f39x3VWrVhXrW7duLdZLf9sXXXRRcd3pPKVz2+Pstp+2fdL2wQnLLrT9ku03q9vZ3WwWQPdNZTf+55Ku/9yydZJ2RsSlknZWjwEMscawR8RuSR99bvEKSVuq+1skrexyXwC6rN0Lcc2LiOPV/Q8kzat7ou1RSeUPfgB6ruOr7kVElA68RcSYpDGJA3TAILU79HbC9nxJqm5Pdq8lAL3Qbtifl7Smur9G0nPdaQdArzTuxtt+VtJ3JI3Yfl/STyQ9IunXtm+T9I6k7/eyyewOHz48sNduOh/+yJEjxXrpXPumc+XXrSsP8jRd876X3z+YjhrDHhGra0rf7XIvAHqIr8sCSRB2IAnCDiRB2IEkCDuQxMydtzaRZcuW1daaTo9tGlobHx8v1hcvXlysv/LKK7W1uXPnFtdtOv26qfcbbrihWM+GLTuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJME4+wxw00031dbuuOOO4rpNp4k2jXU3rV8aS+/kFFVJ2rRpU7HedKnqbNiyA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EASjLPPcJ1Oyd3L9V9++eXiuvfcc0+xzjj6uWHLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM4+AzzzzDO1tYULFxbXHRkZKdabrjs/a9asYr3kwQcfLNYZR++uxi277adtn7R9cMKyh2wfs/1q9XNjb9sE0Kmp7Mb/XNL1kyz/z4i4ovp5obttAei2xrBHxG5JH/WhFwA91MkBurtsv1bt5s+ue5LtUdt7be/t4LUAdKjdsG+W9E1JV0g6LunRuidGxFhELImIJW2+FoAuaCvsEXEiIj6LiDOSfibp6u62BaDb2gq77fkTHq6SdLDuuQCGg6dwXfBnJX1H0oikE5J+Uj2+QlJIOirphxFxvPHF7M5OjkbfNY2zP/zww8X6ypUra2sHDhworts0v3rTdeWziohJL8jf+KWaiFg9yeKnOu4IQF/xdVkgCcIOJEHYgSQIO5AEYQeSaBx66+qLTeOht9LUw6dOnepjJ9PLiy++WFu77rrrius2XUr6sccea6unma5u6I0tO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kwaWkK8uWLSvWH3209mI8Onz4cHHdW2+9ta2eZoINGzbU1pYvX15cd/Hixd1uJzW27EAShB1IgrADSRB2IAnCDiRB2IEkCDuQRJpx9tL56JL0xBNPFOsnT56srWUeR2+asvnJJ5+srdmTnnaNHmHLDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJpBlnX7VqVbHedO70rl27utnOtNE0ZfO2bduK9dL72jRnQdN1AnBuGrfsthfY/oPtQ7bfsP3javmFtl+y/WZ1O7v37QJo11R24z+V9K8R8S1J/yjpR7a/JWmdpJ0RcamkndVjAEOqMewRcTwi9lf3P5Y0LuliSSskbametkXSyl41CaBz5/SZ3fYiSd+W9IqkeRFxvCp9IGlezTqjkkbbbxFAN0z5aLztr0jaJmltRPxpYi1aR1omPdoSEWMRsSQilnTUKYCOTCnstr+kVtB/GRHbq8UnbM+v6vMl1Z8WBmDgGnfj3ToP8SlJ4xHx0wml5yWtkfRIdftcTzrskt27dxfr551X/nevdKnpW265pbju+Ph4sb5v375ivcnChQtra9dcc01x3aYhyZUry4dimk5TLQ2vPf7448V1m+o4N1P5zP5Pkm6V9LrtV6tl96kV8l/bvk3SO5K+35sWAXRDY9gj4r8l1f3z/d3utgOgV/i6LJAEYQeSIOxAEoQdSIKwA0m46TTDrr6Y3b8XO0dbt24t1kvjzZ2MNUvSgQMHivUml1xySW1tzpw5xXU77b1p/dKUzZs2bSque/r06WIdk4uISf+nsGUHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQYZ680Ten8wgsv1NaWLClfhOfMmTPFei/HupvW/eSTT4r1pss5b9y4sVjfsWNHsY7uY5wdSI6wA0kQdiAJwg4kQdiBJAg7kARhB5JgnH2KRkZGamvr16/v6HePjpZnx9q+fXux3sl5303XZmfa5OmHcXYgOcIOJEHYgSQIO5AEYQeSIOxAEoQdSKJxnN32Akm/kDRPUkgai4jHbT8k6Q5Jp6qn3hcR9Sd9a3qPswPTRd04+1TCPl/S/IjYb/urkvZJWqnWfOx/joj/mGoThB3ovbqwT2V+9uOSjlf3P7Y9Luni7rYHoNfO6TO77UWSvi3plWrRXbZfs/207dk164za3mt7b0edAujIlL8bb/srknZJ2hAR223Pk3Rarc/x69Xa1f+Xht/BbjzQY21/Zpck21+S9BtJv42In05SXyTpNxHx9w2/h7ADPdb2iTBuXbr0KUnjE4NeHbg7a5Wkg502CaB3pnI0fqmklyW9LunsNZHvk7Ra0hVq7cYflfTD6mBe6XexZQd6rKPd+G4h7EDvcT47kBxhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgicYLTnbZaUnvTHg8Ui0bRsPa27D2JdFbu7rZ28K6Ql/PZ//Ci9t7I2LJwBooGNbehrUvid7a1a/e2I0HkiDsQBKDDvvYgF+/ZFh7G9a+JHprV196G+hndgD9M+gtO4A+IexAEgMJu+3rbR+x/ZbtdYPooY7to7Zft/3qoOenq+bQO2n74IRlF9p+yfab1e2kc+wNqLeHbB+r3rtXbd84oN4W2P6D7UO237D942r5QN+7Ql99ed/6/pnd9vmS/ijpe5Lel7RH0uqIONTXRmrYPippSUQM/AsYtpdJ+rOkX5ydWsv2v0v6KCIeqf6hnB0R/zYkvT2kc5zGu0e91U0z/s8a4HvXzenP2zGILfvVkt6KiLcj4i+SfiVpxQD6GHoRsVvSR59bvELSlur+FrX+WPquprehEBHHI2J/df9jSWenGR/oe1foqy8GEfaLJb034fH7Gq753kPS72zvsz066GYmMW/CNFsfSJo3yGYm0TiNdz99bprxoXnv2pn+vFMcoPuipRFxpaQbJP2o2l0dStH6DDZMY6ebJX1TrTkAj0t6dJDNVNOMb5O0NiL+NLE2yPdukr768r4NIuzHJC2Y8Phr1bKhEBHHqtuTknao9bFjmJw4O4NudXtywP38VUSciIjPIuKMpJ9pgO9dNc34Nkm/jIjt1eKBv3eT9dWv920QYd8j6VLbX7f9ZUk/kPT8APr4AtuzqgMnsj1L0nIN31TUz0taU91fI+m5AfbyN4ZlGu+6acY14Pdu4NOfR0TffyTdqNYR+f+VdP8geqjp6xuS/qf6eWPQvUl6Vq3duv9T69jGbZLmSNop6U1Jv5d04RD19l9qTe39mlrBmj+g3paqtYv+mqRXq58bB/3eFfrqy/vG12WBJDhAByRB2IEkCDuQBGEHkiDsQBKEHUiCsANJ/D9wJ73GNYCjdQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5SjssyP5ppZ"
      },
      "source": [
        "# Budowa sieci\n",
        "\n",
        "Zdefiniujmy najpierw klasę pomocniczną, która będzie przetrzymywać wyniki obliczeń i przyda nam się do wykresów. Opiszemy ją później."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xx9TUguWXXq"
      },
      "source": [
        "# Definiujemy klasę, która będzie przetrzymywać wyniki obliczeń po każdej epoce\n",
        "class Stats(tflearn.callbacks.Callback):\n",
        "    def __init__(self, examples=0):\n",
        "        self.epoch_data = []\n",
        "        self.data_size = examples\n",
        "        self.last_time = 0\n",
        "        self.start_time = 0\n",
        "  \n",
        "        \n",
        "        self.min_report_secs = 1.\n",
        "\n",
        "    def on_train_begin(self, training_state):\n",
        "        self.start_time = time.time()\n",
        "      \n",
        "    def on_epoch_end(self, training_state):\n",
        "        metrics = {\n",
        "            'loss': training_state.loss_value, # loss (strata) dla danych treningowych\n",
        "            'acc': training_state.acc_value,   # accuracy (dokładność) dla danych treningowych\n",
        "            'val_loss': training_state.val_loss, # loss dla danych walidacyjnych\n",
        "            'val_acc': training_state.val_acc, # accuracy dla danych walidacyjnych\n",
        "            'epoch': training_state.epoch,\n",
        "            'step': training_state.step,\n",
        "            'iter': training_state.current_iter,\n",
        "            'time': time.time() - self.start_time,\n",
        "        }\n",
        "\n",
        "        self.epoch_data.append(metrics)\n",
        "        \n",
        "    def on_batch_end(self, training_state, snapshot=False):\n",
        "      cur_time = time.time()\n",
        "      if cur_time - self.last_time < self.min_report_secs:\n",
        "        return # nie spamujmy za często\n",
        "      \n",
        "      self.last_time = cur_time\n",
        "      epoch = training_state.epoch\n",
        "      step = training_state.step\n",
        "      iter = training_state.current_iter\n",
        "      print(\"Epoch %d, step (batch no.): %d -- acc: %.2f, loss %.2f -- iter %05d/%05d, training for: %.2fs\" % (\n",
        "          epoch, step, training_state.acc_value, training_state.loss_value,\n",
        "          iter, self.data_size, cur_time-self.start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jS6wmQ76KYB"
      },
      "source": [
        "\\\\\n",
        "\n",
        "Poniżej jest przykład budowy sieci przy korzystaniu z `tflearn`. W zadaniu będzie trzeba zmieniać odpowiednie parametry.\n",
        "\n",
        "Sieć budujemy kolejnymi warstwami, gdzie funkcja tworząca następną warstwę przyjmuje zmienną reprezentującą poprzednią. Dla wygody wynik przypisujemy na tą samą zmienną, aby móc łatwo dodawać dodatkowe warstwy poprzez wklejenie kodu w odpowiednie miejsce:\n",
        "\n",
        "`network = ...` <- opis warstwy A \\\\\n",
        "`network = conv_2d(network, ...)` <- tworzymy nową warstwę B za warstwą A i przypisujemy na tą samą zmienną (nie potrzebujemy już odwoływać się do warstwy A) \\\\\n",
        "`network = fully_connected(network, ...)` <- warstwa C za warstwą B\n",
        "\n",
        "W tym momencie zmienna `network` reprezentuje sieć składającą się z warstw `A-B-C`.\n",
        "\n",
        "### Uczenie\n",
        "\n",
        "Uczenie sieci przebiega w fazach. Podajemy sieci przykłady kolejno i stosujemy propagację wstęczną. W momencie, gdy przykłady nam się skończą, po prostu zaczynamy od nowa. Każdy taki obrót będziemy nazywali **epoką**.  \\\\\n",
        "Przykładów nie będziemy podawać sieci pojedynczo – podajemy je w porcjach rozmiaru `batch` (*wsad*), co oznacza ile przykładów sieć będzie przetwarzać jednocześnie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k98UrH789LXm"
      },
      "source": [
        "Przydatne nam będą przede wszystkim funkcje:\n",
        "\n",
        "`conv_2d(network, #filtrów, #rozmiar, activation=f.aktywacji, regularizer=\"L2\")` - tworzy nową warstwę konwolucyjną posiadającą `#filtrów` filtrów (neuronów) o rozmiarze `#rozmiar x #rozmiar` oraz z funkcją aktywacji jak podana. Wartości `regularizer` nie należy zmieniać w tym zadaniu.\n",
        "\n",
        "`max_pool_2d(network, #rozmiar)` - podpróbkowanie danych co `#rozmiar` (zmniejsza wymiar obrazka razy `#rozmiar`)\n",
        "\n",
        "`fully_connected(network, #neuronów, activation=f.aktywacji)` - warstwa pełna  z `#neuronów` neuronów i podaną funkcją aktywacji.\n",
        "\n",
        "**W kodzie zaznaczono, który fragment będzie podlegał modyfikacji.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmB2p2qIWXXt"
      },
      "source": [
        "# Tu definiujemy architekturę naszej sieci\n",
        "\n",
        "# !!! UWAGA - przy każdych nowych obliczeniach należy zresetować graf sieci\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Wyłączamy warningi z tensorflow\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "# Warstwa wejściowa - musi mieć takie same wymiary jak dane\n",
        "network = input_data(shape=[None, 28, 28, 1], name='input') # None oznacza, że ta \n",
        "# wartość będzie uzupełniona automatyzcnie i jest to liczba próbek we wsadzie (batch)\n",
        "\n",
        "### MODYFIKUJEMY OD TĄD\n",
        "# ---\n",
        "# Pierwsza warstwa konwolucyjna - 32 filtry o rozmiarach 3x3\n",
        "# z funkcją aktywacji (activation=) relu.\n",
        "# Regularizer='L2' oznacza narzucenie ograniczenia na wartości wag (nie istotne tutaj).\n",
        "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
        "# max_pool wykonuje podpróbkowanie danych - zmniejsza wymiar obrazka 2-krotnie\n",
        "network = max_pool_2d(network, 2) # teraz obrazki są [14x14]\n",
        "\n",
        "# druga warstwa konwolucyjna - 32 filtry o rozmiarach 3x3 z podpróbkowaniem\n",
        "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
        "network = max_pool_2d(network, 2) # teraz obrazki są [7x7]\n",
        "\n",
        "# warstwa pełna - tu już zaczyna się \"normalna\" sieć neuronowa\n",
        "network = fully_connected(network, 128, activation='relu') # 128 neuronów, aktywacja \"relu\", przyjmuje wejście [7x7] do [128] neuronów\n",
        "network = fully_connected(network, 256, activation='relu') # 256 neuronów, aktywacja \"relu\"\n",
        "# ---\n",
        "### DO TĄD\n",
        "\n",
        "# warstwa wyjściowa - używamy aktywacji softmax, żeby dostać prawdopodobieństwa dla każdej klasy (cyfry)\n",
        "network = fully_connected(network, 10, activation='softmax')\n",
        "\n",
        "# tu definiujemy w jaki sposób optymalizować sieć (regression nie oznacza, że robimy regresję)\n",
        "# nie będziemy modyfikować tych argumentów; stosujemy optymizator Adam\n",
        "network = regression(network, optimizer='adam', learning_rate=0.01,\n",
        "                     batch_size=250,\n",
        "                     loss='categorical_crossentropy', name='target')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abynHGxHGoa7"
      },
      "source": [
        "Powyżej zdefiniowaliśmy sieć z warstwą wejściową, dwiema warstwami konwolucyjnymi z 32 filtrami 3x3 i podpróbkowaniem x2 oraz trzema warstwami pełnymi z odpowiednio 128, 256 i 10 neuronami."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x35TeBBk_ynp"
      },
      "source": [
        "## Trenowanie sieci\n",
        "\n",
        "Trenowanie sieci wywołujemy jak poniżej (wszystkie 3 linijki będą potrzebne). Trenowanie możemy w dowolnym momencie przerwać klikając na kwadrat zatrzymywania.\n",
        "\n",
        "Metoda uczenia ma następującą sygnaturę:\n",
        "`model.fit({'input': X}, {'target': Y}, n_epoch=#epok,\n",
        "           validation_set=({'input': testX}, {'target': testY}), show_metric=True, callbacks=[scores])`\n",
        "  \n",
        " Uczy podaną wyżej sieć, z danymi treningowymi z `X` i etykietami z `Y` przez `#epok` epok. Wyniki walidacji będą obliczane na zbiorze testowym `testX` z etykietami `testY`. Resztę zostawiamy bez zmian – służy do wypisywania i rejestrowania statystyk."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY8evjU6De6R"
      },
      "source": [
        "Podczas uczenia będziemy zbierali miary tego, jak sieć radzi sobie z naszymi danymi:\n",
        "\n",
        " `loss` - loss (strata) dla danych treningowych \\\\\n",
        "`acc` - accuracy (dokładność) dla danych treningowych \\\\\n",
        "`val_loss` -  loss dla danych walidacyjnych \\\\\n",
        "`val_acc` - accuracy dla danych walidacyjnych \\\\\n",
        "`epoch` - numer *epoki* \\\\\n",
        "`step` - numer wsadu (*batch*) \\\\\n",
        "`iter` - liczba widzianych przykładów \\\\\n",
        "`time` - czas jaki upłynął od początku uczenia \\\\\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kTeJDOjEs3m"
      },
      "source": [
        "**Uwaga!** kolejne wywołania poniższej komórki będą *douczać* sieć – aby uczyć ją od nowa musimy ponownie wykonać komórkę z definicją sieci."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAabxPtCWXXy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93ca778e-5de3-42cd-af1f-e4bf921f1974"
      },
      "source": [
        "# uruchamiamy trenowanie naszej sieci\n",
        "# n_epoch - liczba epok, czyli przejść przez cały zbiór danych treningowych\n",
        "scores = Stats(examples=len(X))\n",
        "model = tflearn.DNN(network, tensorboard_verbose=1)\n",
        "model.fit({'input': X}, {'target': Y},\n",
        "          n_epoch=1,  # <-- DO ZMIANY\n",
        "          validation_set=({'input': testX}, {'target': testY}), show_metric=True, callbacks=[scores])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: UNRII0\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "---------------------------------\n",
            "Training samples: 55000\n",
            "Validation samples: 10000\n",
            "--\n",
            "Epoch 1, step (batch no.): 1 -- acc: 0.00, loss 0.00 -- iter 00250/55000, training for: 0.31s\n",
            "Epoch 1, step (batch no.): 8 -- acc: 0.40, loss 1.70 -- iter 02000/55000, training for: 1.47s\n",
            "Epoch 1, step (batch no.): 14 -- acc: 0.73, loss 0.78 -- iter 03500/55000, training for: 2.48s\n",
            "Epoch 1, step (batch no.): 21 -- acc: 0.86, loss 0.44 -- iter 05250/55000, training for: 3.62s\n",
            "Epoch 1, step (batch no.): 28 -- acc: 0.92, loss 0.28 -- iter 07000/55000, training for: 4.74s\n",
            "Epoch 1, step (batch no.): 35 -- acc: 0.93, loss 0.24 -- iter 08750/55000, training for: 5.85s\n",
            "Epoch 1, step (batch no.): 42 -- acc: 0.94, loss 0.21 -- iter 10500/55000, training for: 6.97s\n",
            "Epoch 1, step (batch no.): 49 -- acc: 0.95, loss 0.18 -- iter 12250/55000, training for: 8.09s\n",
            "Epoch 1, step (batch no.): 56 -- acc: 0.95, loss 0.15 -- iter 14000/55000, training for: 9.17s\n",
            "Epoch 1, step (batch no.): 63 -- acc: 0.96, loss 0.13 -- iter 15750/55000, training for: 10.27s\n",
            "Epoch 1, step (batch no.): 70 -- acc: 0.97, loss 0.11 -- iter 17500/55000, training for: 11.38s\n",
            "Epoch 1, step (batch no.): 77 -- acc: 0.96, loss 0.12 -- iter 19250/55000, training for: 12.50s\n",
            "Epoch 1, step (batch no.): 84 -- acc: 0.97, loss 0.11 -- iter 21000/55000, training for: 13.61s\n",
            "Epoch 1, step (batch no.): 91 -- acc: 0.97, loss 0.11 -- iter 22750/55000, training for: 14.73s\n",
            "Epoch 1, step (batch no.): 98 -- acc: 0.97, loss 0.09 -- iter 24500/55000, training for: 15.85s\n",
            "Epoch 1, step (batch no.): 105 -- acc: 0.97, loss 0.09 -- iter 26250/55000, training for: 16.95s\n",
            "Epoch 1, step (batch no.): 112 -- acc: 0.97, loss 0.09 -- iter 28000/55000, training for: 18.05s\n",
            "Epoch 1, step (batch no.): 119 -- acc: 0.97, loss 0.08 -- iter 29750/55000, training for: 19.16s\n",
            "Epoch 1, step (batch no.): 126 -- acc: 0.97, loss 0.10 -- iter 31500/55000, training for: 20.30s\n",
            "Epoch 1, step (batch no.): 133 -- acc: 0.97, loss 0.08 -- iter 33250/55000, training for: 21.41s\n",
            "Epoch 1, step (batch no.): 140 -- acc: 0.98, loss 0.08 -- iter 35000/55000, training for: 22.51s\n",
            "Epoch 1, step (batch no.): 147 -- acc: 0.98, loss 0.08 -- iter 36750/55000, training for: 23.62s\n",
            "Epoch 1, step (batch no.): 154 -- acc: 0.97, loss 0.10 -- iter 38500/55000, training for: 24.72s\n",
            "Epoch 1, step (batch no.): 161 -- acc: 0.97, loss 0.10 -- iter 40250/55000, training for: 25.81s\n",
            "Epoch 1, step (batch no.): 168 -- acc: 0.97, loss 0.09 -- iter 42000/55000, training for: 26.91s\n",
            "Epoch 1, step (batch no.): 175 -- acc: 0.97, loss 0.10 -- iter 43750/55000, training for: 28.02s\n",
            "Epoch 1, step (batch no.): 182 -- acc: 0.98, loss 0.09 -- iter 45500/55000, training for: 29.12s\n",
            "Epoch 1, step (batch no.): 189 -- acc: 0.89, loss 1.33 -- iter 47250/55000, training for: 30.23s\n",
            "Epoch 1, step (batch no.): 196 -- acc: 0.93, loss 0.85 -- iter 49000/55000, training for: 31.32s\n",
            "Epoch 1, step (batch no.): 203 -- acc: 0.95, loss 0.45 -- iter 50750/55000, training for: 32.43s\n",
            "Epoch 1, step (batch no.): 210 -- acc: 0.96, loss 0.28 -- iter 52500/55000, training for: 33.53s\n",
            "Epoch 1, step (batch no.): 217 -- acc: 0.97, loss 0.19 -- iter 54250/55000, training for: 34.64s\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.16333\u001b[0m\u001b[0m | time: 37.074s\n",
            "| Adam | epoch: 001 | loss: 0.16333 - acc: 0.9703 | val_loss: 0.08019 - val_acc: 0.9776 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 1, step (batch no.): 220 -- acc: 0.97, loss 0.16 -- iter 55000/55000, training for: 37.09s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xB_tk_6YDJ4b"
      },
      "source": [
        "## Korzystanie z sieci\n",
        "\n",
        "Poniźej przykład korzystania z sieci do etykietowania danych które sieć nie widziała."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NchiBGf5WXX3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "d9bebc82-cc54-4f43-cb9f-ba80c7e1a611"
      },
      "source": [
        "# Wylosujmy index elementu ze zbioru testowego do sprawdzenia\n",
        "test_index = np.random.randint(len(testX))\n",
        "# Przewidywania modelu na danych walidacyjnych (prawdopodobieństwa/logika rozmyta):\n",
        "predictions = model.predict(testX[test_index:test_index+1])\n",
        "plt.imshow(testX[test_index,:,:,0], cmap='gray')\n",
        "plt.show()\n",
        "print(predictions)\n",
        "print(\"Prawdopodobnie: %s na %.3f%%\" % (predictions.argmax(), predictions.max()*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[9.9999988e-01 3.6848230e-14 1.3649135e-10 1.1837963e-10 4.5985110e-10\n",
            "  8.5800794e-10 1.2958493e-08 6.8878608e-10 4.1201823e-11 1.3562919e-07]]\n",
            "Prawdopodobnie: 0 na 100.000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JszN8lhFpKw"
      },
      "source": [
        "# Programatyczny dostęp do statystyk\n",
        "\n",
        "Statystyki znajdziemy w obiekcie `scores`, który przekazaliśmy podczas uczenia sieci. Jest to lista statystyk po każdej *epoce*. Statystyki reprezentowane są przez słownik z polami jak opisanymi wcześniej."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71uFUDO5WXX7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d4faa61-8676-4317-9096-169f5503012d"
      },
      "source": [
        "# wyświetlenie danych\n",
        "scores.epoch_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'acc': 0.9702860713005066,\n",
              "  'epoch': 1,\n",
              "  'iter': 55000,\n",
              "  'loss': 0.16333448886871338,\n",
              "  'step': 220,\n",
              "  'time': 37.087403774261475,\n",
              "  'val_acc': 0.9775999993085861,\n",
              "  'val_loss': 0.08018675679923035}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwhTLw0PWXX_"
      },
      "source": [
        "# POLECENIA\n",
        "## Do zaliczenia tego zadania można zrobić podzadania A-C lub zadanie alternatywne."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCZmKjv9WXYA"
      },
      "source": [
        "# Uwagi:\n",
        "\n",
        "### Przy każdych nowych obliczeniach należy zresetować graf sieci.\n",
        "### Nie należy zmieniać linijki z optymizatorem przy rozwiązywaniu zadań.\n",
        "### Niektóre rezultaty mogą być nieintuicyjne."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8q5fPv0hWXYB"
      },
      "source": [
        "# Zadanie A: dawne ograniczenia\n",
        "\n",
        "Narysuj wykres zależności accuracy dla danych **walidacyjnych** (val_acc) od liczby epok (max 10 epok)\n",
        "dla sieci w dwóch wariantach:\n",
        "* 1 warstwa konwolucyjna: 32 filtry 3x3 + 2 warstwy pełne (jak w przykładzie: 128,256,10)\n",
        "* j.w. ale dla 3 i 5 warstw konwolucyjnych (podpróbkowanie wykonuj tylko po 2. i 4. warstwie konwolucyjnej)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avyYrZLPeoGf"
      },
      "source": [
        "Wariant 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5ntk1_YiuVy"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "EPOCHS = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVwcGjVremqx"
      },
      "source": [
        "# Tu definiujemy architekturę naszej sieci\n",
        "\n",
        "# !!! UWAGA - przy każdych nowych obliczeniach należy zresetować graf sieci\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Wyłączamy warningi z tensorflow\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "# Warstwa wejściowa - musi mieć takie same wymiary jak dane\n",
        "network = input_data(shape=[None, 28, 28, 1], name='input') # None oznacza, że ta \n",
        "# wartość będzie uzupełniona automatyzcnie i jest to liczba próbek we wsadzie (batch)\n",
        "\n",
        "### MODYFIKUJEMY OD TĄD\n",
        "# ---\n",
        "# Pierwsza warstwa konwolucyjna - 32 filtry o rozmiarach 3x3\n",
        "# z funkcją aktywacji (activation=) relu.\n",
        "# Regularizer='L2' oznacza narzucenie ograniczenia na wartości wag (nie istotne tutaj).\n",
        "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
        "# max_pool wykonuje podpróbkowanie danych - zmniejsza wymiar obrazka 2-krotnie\n",
        "network = max_pool_2d(network, 2) # teraz obrazki są [14x14]\n",
        "\n",
        "\n",
        "# warstwa pełna - tu już zaczyna się \"normalna\" sieć neuronowa\n",
        "network = fully_connected(network, 128, activation='relu') # 128 neuronów, aktywacja \"relu\", przyjmuje wejście [7x7] do [128] neuronów\n",
        "network = fully_connected(network, 256, activation='relu') # 256 neuronów, aktywacja \"relu\"\n",
        "# ---\n",
        "### DO TĄD\n",
        "\n",
        "# warstwa wyjściowa - używamy aktywacji softmax, żeby dostać prawdopodobieństwa dla każdej klasy (cyfry)\n",
        "network = fully_connected(network, 10, activation='softmax')\n",
        "\n",
        "# tu definiujemy w jaki sposób optymalizować sieć (regression nie oznacza, że robimy regresję)\n",
        "# nie będziemy modyfikować tych argumentów; stosujemy optymizator Adam\n",
        "network = regression(network, optimizer='adam', learning_rate=0.01,\n",
        "                     batch_size=250,\n",
        "                     loss='categorical_crossentropy', name='target')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U5vXAMEeqfg",
        "outputId": "e120a3ca-820d-4854-edcd-479740157990"
      },
      "source": [
        "# uruchamiamy trenowanie naszej sieci\n",
        "# n_epoch - liczba epok, czyli przejść przez cały zbiór danych treningowych\n",
        "scores = Stats(examples=len(X))\n",
        "model = tflearn.DNN(network, tensorboard_verbose=0)\n",
        "model.fit({'input': X}, {'target': Y},\n",
        "          n_epoch=EPOCHS,  # <-- DO ZMIANY\n",
        "          validation_set=({'input': testX}, {'target': testY}), show_metric=True, callbacks=[scores])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: UFRG8R\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "---------------------------------\n",
            "Training samples: 55000\n",
            "Validation samples: 10000\n",
            "--\n",
            "Epoch 1, step (batch no.): 1 -- acc: 0.00, loss 0.00 -- iter 00250/55000, training for: 0.19s\n",
            "Epoch 1, step (batch no.): 12 -- acc: 0.75, loss 0.75 -- iter 03000/55000, training for: 1.23s\n",
            "Epoch 1, step (batch no.): 23 -- acc: 0.89, loss 0.37 -- iter 05750/55000, training for: 2.25s\n",
            "Epoch 1, step (batch no.): 35 -- acc: 0.93, loss 0.22 -- iter 08750/55000, training for: 3.33s\n",
            "Epoch 1, step (batch no.): 46 -- acc: 0.95, loss 0.16 -- iter 11500/55000, training for: 4.34s\n",
            "Epoch 1, step (batch no.): 57 -- acc: 0.95, loss 0.16 -- iter 14250/55000, training for: 5.35s\n",
            "Epoch 1, step (batch no.): 69 -- acc: 0.96, loss 0.14 -- iter 17250/55000, training for: 6.43s\n",
            "Epoch 1, step (batch no.): 80 -- acc: 0.96, loss 0.12 -- iter 20000/55000, training for: 7.45s\n",
            "Epoch 1, step (batch no.): 92 -- acc: 0.97, loss 0.11 -- iter 23000/55000, training for: 8.53s\n",
            "Epoch 1, step (batch no.): 104 -- acc: 0.97, loss 0.10 -- iter 26000/55000, training for: 9.60s\n",
            "Epoch 1, step (batch no.): 116 -- acc: 0.97, loss 0.10 -- iter 29000/55000, training for: 10.67s\n",
            "Epoch 1, step (batch no.): 127 -- acc: 0.97, loss 0.11 -- iter 31750/55000, training for: 11.67s\n",
            "Epoch 1, step (batch no.): 139 -- acc: 0.97, loss 0.10 -- iter 34750/55000, training for: 12.74s\n",
            "Epoch 1, step (batch no.): 151 -- acc: 0.97, loss 0.09 -- iter 37750/55000, training for: 13.81s\n",
            "Epoch 1, step (batch no.): 163 -- acc: 0.98, loss 0.08 -- iter 40750/55000, training for: 14.88s\n",
            "Epoch 1, step (batch no.): 175 -- acc: 0.98, loss 0.08 -- iter 43750/55000, training for: 15.94s\n",
            "Epoch 1, step (batch no.): 187 -- acc: 0.98, loss 0.08 -- iter 46750/55000, training for: 17.01s\n",
            "Epoch 1, step (batch no.): 198 -- acc: 0.98, loss 0.06 -- iter 49500/55000, training for: 18.03s\n",
            "Epoch 1, step (batch no.): 209 -- acc: 0.98, loss 0.06 -- iter 52250/55000, training for: 19.05s\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.05312\u001b[0m\u001b[0m | time: 21.402s\n",
            "| Adam | epoch: 001 | loss: 0.05312 - acc: 0.9839 | val_loss: 0.06370 - val_acc: 0.9778 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 1, step (batch no.): 220 -- acc: 0.98, loss 0.05 -- iter 55000/55000, training for: 21.41s\n",
            "Epoch 2, step (batch no.): 231 -- acc: 0.98, loss 0.05 -- iter 02750/55000, training for: 22.42s\n",
            "Epoch 2, step (batch no.): 243 -- acc: 0.98, loss 0.05 -- iter 05750/55000, training for: 23.50s\n",
            "Epoch 2, step (batch no.): 254 -- acc: 0.98, loss 0.05 -- iter 08500/55000, training for: 24.52s\n",
            "Epoch 2, step (batch no.): 265 -- acc: 0.98, loss 0.06 -- iter 11250/55000, training for: 25.54s\n",
            "Epoch 2, step (batch no.): 276 -- acc: 0.98, loss 0.06 -- iter 14000/55000, training for: 26.56s\n",
            "Epoch 2, step (batch no.): 287 -- acc: 0.98, loss 0.05 -- iter 16750/55000, training for: 27.56s\n",
            "Epoch 2, step (batch no.): 298 -- acc: 0.98, loss 0.06 -- iter 19500/55000, training for: 28.60s\n",
            "Epoch 2, step (batch no.): 309 -- acc: 0.98, loss 0.05 -- iter 22250/55000, training for: 29.62s\n",
            "Epoch 2, step (batch no.): 320 -- acc: 0.99, loss 0.05 -- iter 25000/55000, training for: 30.66s\n",
            "Epoch 2, step (batch no.): 331 -- acc: 0.98, loss 0.05 -- iter 27750/55000, training for: 31.67s\n",
            "Epoch 2, step (batch no.): 342 -- acc: 0.98, loss 0.05 -- iter 30500/55000, training for: 32.70s\n",
            "Epoch 2, step (batch no.): 354 -- acc: 0.98, loss 0.05 -- iter 33500/55000, training for: 33.78s\n",
            "Epoch 2, step (batch no.): 365 -- acc: 0.98, loss 0.06 -- iter 36250/55000, training for: 34.80s\n",
            "Epoch 2, step (batch no.): 376 -- acc: 0.98, loss 0.05 -- iter 39000/55000, training for: 35.81s\n",
            "Epoch 2, step (batch no.): 387 -- acc: 0.98, loss 0.06 -- iter 41750/55000, training for: 36.81s\n",
            "Epoch 2, step (batch no.): 398 -- acc: 0.98, loss 0.06 -- iter 44500/55000, training for: 37.81s\n",
            "Epoch 2, step (batch no.): 409 -- acc: 0.98, loss 0.06 -- iter 47250/55000, training for: 38.84s\n",
            "Epoch 2, step (batch no.): 421 -- acc: 0.95, loss 0.52 -- iter 50250/55000, training for: 39.92s\n",
            "Epoch 2, step (batch no.): 433 -- acc: 0.98, loss 0.18 -- iter 53250/55000, training for: 41.01s\n",
            "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.12183\u001b[0m\u001b[0m | time: 21.611s\n",
            "| Adam | epoch: 002 | loss: 0.12183 - acc: 0.9811 | val_loss: 0.09425 - val_acc: 0.9767 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 2, step (batch no.): 440 -- acc: 0.98, loss 0.12 -- iter 55000/55000, training for: 43.04s\n",
            "Epoch 3, step (batch no.): 451 -- acc: 0.98, loss 0.09 -- iter 02750/55000, training for: 44.06s\n",
            "Epoch 3, step (batch no.): 462 -- acc: 0.99, loss 0.06 -- iter 05500/55000, training for: 45.08s\n",
            "Epoch 3, step (batch no.): 474 -- acc: 0.99, loss 0.05 -- iter 08500/55000, training for: 46.17s\n",
            "Epoch 3, step (batch no.): 485 -- acc: 0.99, loss 0.04 -- iter 11250/55000, training for: 47.17s\n",
            "Epoch 3, step (batch no.): 496 -- acc: 0.99, loss 0.03 -- iter 14000/55000, training for: 48.18s\n",
            "Epoch 3, step (batch no.): 507 -- acc: 0.99, loss 0.04 -- iter 16750/55000, training for: 49.20s\n",
            "Epoch 3, step (batch no.): 518 -- acc: 0.99, loss 0.04 -- iter 19500/55000, training for: 50.22s\n",
            "Epoch 3, step (batch no.): 529 -- acc: 0.99, loss 0.03 -- iter 22250/55000, training for: 51.22s\n",
            "Epoch 3, step (batch no.): 540 -- acc: 0.99, loss 0.03 -- iter 25000/55000, training for: 52.23s\n",
            "Epoch 3, step (batch no.): 552 -- acc: 0.99, loss 0.03 -- iter 28000/55000, training for: 53.31s\n",
            "Epoch 3, step (batch no.): 563 -- acc: 0.99, loss 0.04 -- iter 30750/55000, training for: 54.31s\n",
            "Epoch 3, step (batch no.): 574 -- acc: 0.99, loss 0.05 -- iter 33500/55000, training for: 55.32s\n",
            "Epoch 3, step (batch no.): 586 -- acc: 0.99, loss 0.04 -- iter 36500/55000, training for: 56.40s\n",
            "Epoch 3, step (batch no.): 597 -- acc: 0.99, loss 0.04 -- iter 39250/55000, training for: 57.42s\n",
            "Epoch 3, step (batch no.): 608 -- acc: 0.99, loss 0.04 -- iter 42000/55000, training for: 58.43s\n",
            "Epoch 3, step (batch no.): 619 -- acc: 0.99, loss 0.04 -- iter 44750/55000, training for: 59.45s\n",
            "Epoch 3, step (batch no.): 630 -- acc: 0.99, loss 0.03 -- iter 47500/55000, training for: 60.46s\n",
            "Epoch 3, step (batch no.): 641 -- acc: 0.96, loss 0.61 -- iter 50250/55000, training for: 61.46s\n",
            "Epoch 3, step (batch no.): 652 -- acc: 0.98, loss 0.20 -- iter 53000/55000, training for: 62.47s\n",
            "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.10185\u001b[0m\u001b[0m | time: 21.565s\n",
            "| Adam | epoch: 003 | loss: 0.10185 - acc: 0.9895 | val_loss: 0.05983 - val_acc: 0.9849 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 3, step (batch no.): 660 -- acc: 0.99, loss 0.10 -- iter 55000/55000, training for: 64.61s\n",
            "Epoch 4, step (batch no.): 671 -- acc: 0.99, loss 0.05 -- iter 02750/55000, training for: 65.64s\n",
            "Epoch 4, step (batch no.): 682 -- acc: 0.99, loss 0.04 -- iter 05500/55000, training for: 66.67s\n",
            "Epoch 4, step (batch no.): 694 -- acc: 0.99, loss 0.04 -- iter 08500/55000, training for: 67.75s\n",
            "Epoch 4, step (batch no.): 706 -- acc: 0.99, loss 0.03 -- iter 11500/55000, training for: 68.83s\n",
            "Epoch 4, step (batch no.): 718 -- acc: 0.99, loss 0.03 -- iter 14500/55000, training for: 69.91s\n",
            "Epoch 4, step (batch no.): 729 -- acc: 0.99, loss 0.02 -- iter 17250/55000, training for: 70.99s\n",
            "Epoch 4, step (batch no.): 740 -- acc: 0.99, loss 0.02 -- iter 20000/55000, training for: 72.04s\n",
            "Epoch 4, step (batch no.): 751 -- acc: 0.99, loss 0.01 -- iter 22750/55000, training for: 73.08s\n",
            "Epoch 4, step (batch no.): 762 -- acc: 0.99, loss 0.02 -- iter 25500/55000, training for: 74.11s\n",
            "Epoch 4, step (batch no.): 773 -- acc: 0.99, loss 0.02 -- iter 28250/55000, training for: 75.11s\n",
            "Epoch 4, step (batch no.): 784 -- acc: 0.99, loss 0.02 -- iter 31000/55000, training for: 76.13s\n",
            "Epoch 4, step (batch no.): 795 -- acc: 0.99, loss 0.02 -- iter 33750/55000, training for: 77.15s\n",
            "Epoch 4, step (batch no.): 807 -- acc: 0.99, loss 0.02 -- iter 36750/55000, training for: 78.24s\n",
            "Epoch 4, step (batch no.): 818 -- acc: 0.99, loss 0.03 -- iter 39500/55000, training for: 79.26s\n",
            "Epoch 4, step (batch no.): 830 -- acc: 0.99, loss 0.03 -- iter 42500/55000, training for: 80.35s\n",
            "Epoch 4, step (batch no.): 841 -- acc: 0.99, loss 0.03 -- iter 45250/55000, training for: 81.35s\n",
            "Epoch 4, step (batch no.): 852 -- acc: 0.90, loss 1.65 -- iter 48000/55000, training for: 82.36s\n",
            "Epoch 4, step (batch no.): 863 -- acc: 0.96, loss 0.60 -- iter 50750/55000, training for: 83.38s\n",
            "Epoch 4, step (batch no.): 874 -- acc: 0.99, loss 0.20 -- iter 53500/55000, training for: 84.39s\n",
            "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.12409\u001b[0m\u001b[0m | time: 21.664s\n",
            "| Adam | epoch: 004 | loss: 0.12409 - acc: 0.9879 | val_loss: 0.08458 - val_acc: 0.9813 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 4, step (batch no.): 880 -- acc: 0.99, loss 0.12 -- iter 55000/55000, training for: 86.29s\n",
            "Epoch 5, step (batch no.): 891 -- acc: 0.99, loss 0.05 -- iter 02750/55000, training for: 87.29s\n",
            "Epoch 5, step (batch no.): 903 -- acc: 0.99, loss 0.04 -- iter 05750/55000, training for: 88.37s\n",
            "Epoch 5, step (batch no.): 914 -- acc: 0.99, loss 0.04 -- iter 08500/55000, training for: 89.38s\n",
            "Epoch 5, step (batch no.): 925 -- acc: 0.99, loss 0.03 -- iter 11250/55000, training for: 90.42s\n",
            "Epoch 5, step (batch no.): 936 -- acc: 0.99, loss 0.02 -- iter 14000/55000, training for: 91.45s\n",
            "Epoch 5, step (batch no.): 947 -- acc: 0.99, loss 0.03 -- iter 16750/55000, training for: 92.49s\n",
            "Epoch 5, step (batch no.): 958 -- acc: 1.00, loss 0.02 -- iter 19500/55000, training for: 93.53s\n",
            "Epoch 5, step (batch no.): 969 -- acc: 0.99, loss 0.03 -- iter 22250/55000, training for: 94.55s\n",
            "Epoch 5, step (batch no.): 979 -- acc: 0.99, loss 0.03 -- iter 24750/55000, training for: 95.57s\n",
            "Epoch 5, step (batch no.): 990 -- acc: 0.99, loss 0.02 -- iter 27500/55000, training for: 96.60s\n",
            "Epoch 5, step (batch no.): 1000 -- acc: 0.99, loss 0.02 -- iter 30000/55000, training for: 97.60s\n",
            "Epoch 5, step (batch no.): 1011 -- acc: 0.99, loss 0.02 -- iter 32750/55000, training for: 98.64s\n",
            "Epoch 5, step (batch no.): 1022 -- acc: 0.99, loss 0.02 -- iter 35500/55000, training for: 99.70s\n",
            "Epoch 5, step (batch no.): 1032 -- acc: 1.00, loss 0.02 -- iter 38000/55000, training for: 100.71s\n",
            "Epoch 5, step (batch no.): 1043 -- acc: 0.99, loss 0.02 -- iter 40750/55000, training for: 101.76s\n",
            "Epoch 5, step (batch no.): 1054 -- acc: 1.00, loss 0.01 -- iter 43500/55000, training for: 102.85s\n",
            "Epoch 5, step (batch no.): 1065 -- acc: 0.99, loss 0.02 -- iter 46250/55000, training for: 103.94s\n",
            "Epoch 5, step (batch no.): 1076 -- acc: 0.99, loss 0.02 -- iter 49000/55000, training for: 105.03s\n",
            "Epoch 5, step (batch no.): 1087 -- acc: 0.99, loss 0.02 -- iter 51750/55000, training for: 106.11s\n",
            "Epoch 5, step (batch no.): 1098 -- acc: 1.00, loss 0.01 -- iter 54500/55000, training for: 107.15s\n",
            "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.01399\u001b[0m\u001b[0m | time: 22.442s\n",
            "| Adam | epoch: 005 | loss: 0.01399 - acc: 0.9949 | val_loss: 0.05905 - val_acc: 0.9846 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 5, step (batch no.): 1100 -- acc: 0.99, loss 0.01 -- iter 55000/55000, training for: 108.74s\n",
            "Epoch 6, step (batch no.): 1111 -- acc: 1.00, loss 0.01 -- iter 02750/55000, training for: 109.80s\n",
            "Epoch 6, step (batch no.): 1122 -- acc: 1.00, loss 0.02 -- iter 05500/55000, training for: 110.87s\n",
            "Epoch 6, step (batch no.): 1133 -- acc: 1.00, loss 0.01 -- iter 08250/55000, training for: 111.95s\n",
            "Epoch 6, step (batch no.): 1144 -- acc: 1.00, loss 0.01 -- iter 11000/55000, training for: 112.99s\n",
            "Epoch 6, step (batch no.): 1155 -- acc: 1.00, loss 0.02 -- iter 13750/55000, training for: 114.04s\n",
            "Epoch 6, step (batch no.): 1166 -- acc: 0.99, loss 0.02 -- iter 16500/55000, training for: 115.12s\n",
            "Epoch 6, step (batch no.): 1177 -- acc: 0.99, loss 0.02 -- iter 19250/55000, training for: 116.18s\n",
            "Epoch 6, step (batch no.): 1188 -- acc: 1.00, loss 0.01 -- iter 22000/55000, training for: 117.27s\n",
            "Epoch 6, step (batch no.): 1199 -- acc: 1.00, loss 0.01 -- iter 24750/55000, training for: 118.31s\n",
            "Epoch 6, step (batch no.): 1210 -- acc: 1.00, loss 0.01 -- iter 27500/55000, training for: 119.36s\n",
            "Epoch 6, step (batch no.): 1221 -- acc: 1.00, loss 0.01 -- iter 30250/55000, training for: 120.40s\n",
            "Epoch 6, step (batch no.): 1232 -- acc: 0.99, loss 0.02 -- iter 33000/55000, training for: 121.45s\n",
            "Epoch 6, step (batch no.): 1243 -- acc: 1.00, loss 0.01 -- iter 35750/55000, training for: 122.48s\n",
            "Epoch 6, step (batch no.): 1254 -- acc: 1.00, loss 0.01 -- iter 38500/55000, training for: 123.52s\n",
            "Epoch 6, step (batch no.): 1265 -- acc: 0.99, loss 0.02 -- iter 41250/55000, training for: 124.58s\n",
            "Epoch 6, step (batch no.): 1276 -- acc: 1.00, loss 0.02 -- iter 44000/55000, training for: 125.64s\n",
            "Epoch 6, step (batch no.): 1287 -- acc: 1.00, loss 0.01 -- iter 46750/55000, training for: 126.68s\n",
            "Epoch 6, step (batch no.): 1298 -- acc: 0.93, loss 1.12 -- iter 49500/55000, training for: 127.73s\n",
            "Epoch 6, step (batch no.): 1309 -- acc: 0.97, loss 0.40 -- iter 52250/55000, training for: 128.77s\n",
            "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.14102\u001b[0m\u001b[0m | time: 22.514s\n",
            "| Adam | epoch: 006 | loss: 0.14102 - acc: 0.9873 | val_loss: 0.06468 - val_acc: 0.9854 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 6, step (batch no.): 1320 -- acc: 0.99, loss 0.14 -- iter 55000/55000, training for: 131.27s\n",
            "Epoch 7, step (batch no.): 1331 -- acc: 0.99, loss 0.05 -- iter 02750/55000, training for: 132.37s\n",
            "Epoch 7, step (batch no.): 1342 -- acc: 0.99, loss 0.03 -- iter 05500/55000, training for: 133.42s\n",
            "Epoch 7, step (batch no.): 1353 -- acc: 0.99, loss 0.03 -- iter 08250/55000, training for: 134.47s\n",
            "Epoch 7, step (batch no.): 1364 -- acc: 0.99, loss 0.02 -- iter 11000/55000, training for: 135.53s\n",
            "Epoch 7, step (batch no.): 1375 -- acc: 0.99, loss 0.02 -- iter 13750/55000, training for: 136.58s\n",
            "Epoch 7, step (batch no.): 1386 -- acc: 0.99, loss 0.02 -- iter 16500/55000, training for: 137.62s\n",
            "Epoch 7, step (batch no.): 1397 -- acc: 0.99, loss 0.02 -- iter 19250/55000, training for: 138.65s\n",
            "Epoch 7, step (batch no.): 1408 -- acc: 1.00, loss 0.02 -- iter 22000/55000, training for: 139.68s\n",
            "Epoch 7, step (batch no.): 1420 -- acc: 0.99, loss 0.02 -- iter 25000/55000, training for: 140.76s\n",
            "Epoch 7, step (batch no.): 1431 -- acc: 1.00, loss 0.01 -- iter 27750/55000, training for: 141.76s\n",
            "Epoch 7, step (batch no.): 1443 -- acc: 0.99, loss 0.02 -- iter 30750/55000, training for: 142.85s\n",
            "Epoch 7, step (batch no.): 1455 -- acc: 0.99, loss 0.02 -- iter 33750/55000, training for: 143.92s\n",
            "Epoch 7, step (batch no.): 1466 -- acc: 0.99, loss 0.03 -- iter 36500/55000, training for: 144.94s\n",
            "Epoch 7, step (batch no.): 1477 -- acc: 0.99, loss 0.02 -- iter 39250/55000, training for: 145.94s\n",
            "Epoch 7, step (batch no.): 1489 -- acc: 0.99, loss 0.02 -- iter 42250/55000, training for: 147.02s\n",
            "Epoch 7, step (batch no.): 1500 -- acc: 1.00, loss 0.02 -- iter 45000/55000, training for: 148.03s\n",
            "Epoch 7, step (batch no.): 1512 -- acc: 0.99, loss 0.02 -- iter 48000/55000, training for: 149.12s\n",
            "Epoch 7, step (batch no.): 1523 -- acc: 1.00, loss 0.02 -- iter 50750/55000, training for: 150.13s\n",
            "Epoch 7, step (batch no.): 1535 -- acc: 1.00, loss 0.01 -- iter 53750/55000, training for: 151.22s\n",
            "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.00932\u001b[0m\u001b[0m | time: 21.756s\n",
            "| Adam | epoch: 007 | loss: 0.00932 - acc: 0.9976 | val_loss: 0.05589 - val_acc: 0.9858 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 7, step (batch no.): 1540 -- acc: 1.00, loss 0.01 -- iter 55000/55000, training for: 153.04s\n",
            "Epoch 8, step (batch no.): 1552 -- acc: 1.00, loss 0.01 -- iter 03000/55000, training for: 154.13s\n",
            "Epoch 8, step (batch no.): 1563 -- acc: 1.00, loss 0.01 -- iter 05750/55000, training for: 155.14s\n",
            "Epoch 8, step (batch no.): 1575 -- acc: 1.00, loss 0.01 -- iter 08750/55000, training for: 156.24s\n",
            "Epoch 8, step (batch no.): 1586 -- acc: 1.00, loss 0.01 -- iter 11500/55000, training for: 157.27s\n",
            "Epoch 8, step (batch no.): 1597 -- acc: 1.00, loss 0.01 -- iter 14250/55000, training for: 158.28s\n",
            "Epoch 8, step (batch no.): 1608 -- acc: 1.00, loss 0.01 -- iter 17000/55000, training for: 159.28s\n",
            "Epoch 8, step (batch no.): 1619 -- acc: 1.00, loss 0.01 -- iter 19750/55000, training for: 160.30s\n",
            "Epoch 8, step (batch no.): 1631 -- acc: 1.00, loss 0.01 -- iter 22750/55000, training for: 161.39s\n",
            "Epoch 8, step (batch no.): 1642 -- acc: 1.00, loss 0.01 -- iter 25500/55000, training for: 162.43s\n",
            "Epoch 8, step (batch no.): 1654 -- acc: 1.00, loss 0.01 -- iter 28500/55000, training for: 163.51s\n",
            "Epoch 8, step (batch no.): 1666 -- acc: 1.00, loss 0.01 -- iter 31500/55000, training for: 164.58s\n",
            "Epoch 8, step (batch no.): 1678 -- acc: 0.99, loss 0.01 -- iter 34500/55000, training for: 165.67s\n",
            "Epoch 8, step (batch no.): 1690 -- acc: 1.00, loss 0.02 -- iter 37500/55000, training for: 166.74s\n",
            "Epoch 8, step (batch no.): 1702 -- acc: 0.99, loss 0.02 -- iter 40500/55000, training for: 167.84s\n",
            "Epoch 8, step (batch no.): 1714 -- acc: 0.99, loss 0.02 -- iter 43500/55000, training for: 168.92s\n",
            "Epoch 8, step (batch no.): 1725 -- acc: 0.99, loss 0.02 -- iter 46250/55000, training for: 169.93s\n",
            "Epoch 8, step (batch no.): 1737 -- acc: 1.00, loss 0.01 -- iter 49250/55000, training for: 170.98s\n",
            "Epoch 8, step (batch no.): 1749 -- acc: 1.00, loss 0.01 -- iter 52250/55000, training for: 172.07s\n",
            "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.00977\u001b[0m\u001b[0m | time: 21.312s\n",
            "| Adam | epoch: 008 | loss: 0.00977 - acc: 0.9968 | val_loss: 0.07026 - val_acc: 0.9826 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 8, step (batch no.): 1760 -- acc: 1.00, loss 0.01 -- iter 55000/55000, training for: 174.36s\n",
            "Epoch 9, step (batch no.): 1771 -- acc: 1.00, loss 0.01 -- iter 02750/55000, training for: 175.38s\n",
            "Epoch 9, step (batch no.): 1783 -- acc: 1.00, loss 0.01 -- iter 05750/55000, training for: 176.46s\n",
            "Epoch 9, step (batch no.): 1795 -- acc: 0.99, loss 0.02 -- iter 08750/55000, training for: 177.52s\n",
            "Epoch 9, step (batch no.): 1807 -- acc: 1.00, loss 0.01 -- iter 11750/55000, training for: 178.58s\n",
            "Epoch 9, step (batch no.): 1819 -- acc: 1.00, loss 0.01 -- iter 14750/55000, training for: 179.66s\n",
            "Epoch 9, step (batch no.): 1830 -- acc: 1.00, loss 0.01 -- iter 17500/55000, training for: 180.67s\n",
            "Epoch 9, step (batch no.): 1842 -- acc: 1.00, loss 0.01 -- iter 20500/55000, training for: 181.75s\n",
            "Epoch 9, step (batch no.): 1854 -- acc: 1.00, loss 0.01 -- iter 23500/55000, training for: 182.83s\n",
            "Epoch 9, step (batch no.): 1866 -- acc: 1.00, loss 0.01 -- iter 26500/55000, training for: 183.92s\n",
            "Epoch 9, step (batch no.): 1877 -- acc: 1.00, loss 0.01 -- iter 29250/55000, training for: 184.94s\n",
            "Epoch 9, step (batch no.): 1888 -- acc: 1.00, loss 0.01 -- iter 32000/55000, training for: 185.96s\n",
            "Epoch 9, step (batch no.): 1899 -- acc: 1.00, loss 0.01 -- iter 34750/55000, training for: 186.97s\n",
            "Epoch 9, step (batch no.): 1911 -- acc: 1.00, loss 0.01 -- iter 37750/55000, training for: 188.06s\n",
            "Epoch 9, step (batch no.): 1922 -- acc: 0.99, loss 0.02 -- iter 40500/55000, training for: 189.07s\n",
            "Epoch 9, step (batch no.): 1933 -- acc: 0.99, loss 0.02 -- iter 43250/55000, training for: 190.08s\n",
            "Epoch 9, step (batch no.): 1944 -- acc: 1.00, loss 0.02 -- iter 46000/55000, training for: 191.09s\n",
            "Epoch 9, step (batch no.): 1955 -- acc: 0.99, loss 0.02 -- iter 48750/55000, training for: 192.11s\n",
            "Epoch 9, step (batch no.): 1966 -- acc: 1.00, loss 0.02 -- iter 51500/55000, training for: 193.13s\n",
            "Epoch 9, step (batch no.): 1977 -- acc: 1.00, loss 0.01 -- iter 54250/55000, training for: 194.14s\n",
            "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.00963\u001b[0m\u001b[0m | time: 21.424s\n",
            "| Adam | epoch: 009 | loss: 0.00963 - acc: 0.9969 | val_loss: 0.07007 - val_acc: 0.9838 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 9, step (batch no.): 1980 -- acc: 1.00, loss 0.01 -- iter 55000/55000, training for: 195.80s\n",
            "Epoch 10, step (batch no.): 1991 -- acc: 1.00, loss 0.01 -- iter 02750/55000, training for: 196.84s\n",
            "Epoch 10, step (batch no.): 2002 -- acc: 1.00, loss 0.01 -- iter 05500/55000, training for: 197.86s\n",
            "Epoch 10, step (batch no.): 2013 -- acc: 1.00, loss 0.01 -- iter 08250/55000, training for: 198.87s\n",
            "Epoch 10, step (batch no.): 2025 -- acc: 1.00, loss 0.01 -- iter 11250/55000, training for: 199.96s\n",
            "Epoch 10, step (batch no.): 2036 -- acc: 1.00, loss 0.01 -- iter 14000/55000, training for: 200.98s\n",
            "Epoch 10, step (batch no.): 2048 -- acc: 1.00, loss 0.01 -- iter 17000/55000, training for: 202.06s\n",
            "Epoch 10, step (batch no.): 2059 -- acc: 1.00, loss 0.02 -- iter 19750/55000, training for: 203.11s\n",
            "Epoch 10, step (batch no.): 2071 -- acc: 0.99, loss 0.02 -- iter 22750/55000, training for: 204.19s\n",
            "Epoch 10, step (batch no.): 2082 -- acc: 1.00, loss 0.01 -- iter 25500/55000, training for: 205.20s\n",
            "Epoch 10, step (batch no.): 2094 -- acc: 1.00, loss 0.01 -- iter 28500/55000, training for: 206.28s\n",
            "Epoch 10, step (batch no.): 2105 -- acc: 1.00, loss 0.01 -- iter 31250/55000, training for: 207.29s\n",
            "Epoch 10, step (batch no.): 2117 -- acc: 0.99, loss 0.02 -- iter 34250/55000, training for: 208.37s\n",
            "Epoch 10, step (batch no.): 2129 -- acc: 1.00, loss 0.02 -- iter 37250/55000, training for: 209.46s\n",
            "Epoch 10, step (batch no.): 2140 -- acc: 0.99, loss 0.02 -- iter 40000/55000, training for: 210.48s\n",
            "Epoch 10, step (batch no.): 2152 -- acc: 0.99, loss 0.02 -- iter 43000/55000, training for: 211.56s\n",
            "Epoch 10, step (batch no.): 2163 -- acc: 0.99, loss 0.02 -- iter 45750/55000, training for: 212.58s\n",
            "Epoch 10, step (batch no.): 2174 -- acc: 1.00, loss 0.01 -- iter 48500/55000, training for: 213.61s\n",
            "Epoch 10, step (batch no.): 2186 -- acc: 0.95, loss 0.84 -- iter 51500/55000, training for: 214.69s\n",
            "Epoch 10, step (batch no.): 2198 -- acc: 0.98, loss 0.26 -- iter 54500/55000, training for: 215.76s\n",
            "Training Step: 2200  | total loss: \u001b[1m\u001b[32m0.21631\u001b[0m\u001b[0m | time: 21.439s\n",
            "| Adam | epoch: 010 | loss: 0.21631 - acc: 0.9836 | val_loss: 0.07161 - val_acc: 0.9831 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 10, step (batch no.): 2200 -- acc: 0.98, loss 0.22 -- iter 55000/55000, training for: 217.25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "InMNP0nJeqni",
        "outputId": "186302b3-d64c-48b0-de14-72fda1733b56"
      },
      "source": [
        "vall_acc_for_epochs =[metrics['val_acc'] for metrics in scores.epoch_data]\n",
        "\n",
        "data = pd.DataFrame(vall_acc_for_epochs)\n",
        "data.plot(label='accuracy')\n",
        "plt.legend([\"validation_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1242c15410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pA3_uTySiPYS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM3k1lrgeq-v"
      },
      "source": [
        "Wariant 2\n",
        "\n",
        "j.w. ale dla 3 i 5 warstw konwolucyjnych (podpróbkowanie wykonuj tylko po 2. i 4. warstwie konwolucyjnej)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZvOLq4QesVp"
      },
      "source": [
        "\n",
        "# !!! UWAGA - przy każdych nowych obliczeniach należy zresetować graf sieci\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Wyłączamy warningi z tensorflow\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "# Warstwa wejściowa - musi mieć takie same wymiary jak dane\n",
        "network = input_data(shape=[None, 28, 28, 1], name='input') # None oznacza, że ta \n",
        "# wartość będzie uzupełniona automatyzcnie i jest to liczba próbek we wsadzie (batch)\n",
        "\n",
        "### MODYFIKUJEMY OD TĄD\n",
        "# ---\n",
        "# Pierwsza i druga warstwa konwolucyjna - 32 filtry o rozmiarach 3x3\n",
        "\n",
        "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
        "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
        "\n",
        "\n",
        "network = max_pool_2d(network, 2) # teraz obrazki są [14x14]\n",
        "\n",
        "# druga warstwa konwolucyjna - 32 filtry o rozmiarach 3x3 z podpróbkowaniem\n",
        "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
        "# network = max_pool_2d(network, 2) # teraz obrazki są [7x7]\n",
        "\n",
        "# warstwa pełna - tu już zaczyna się \"normalna\" sieć neuronowa\n",
        "network = fully_connected(network, 128, activation='relu') # 128 neuronów, aktywacja \"relu\", przyjmuje wejście [7x7] do [128] neuronów\n",
        "network = fully_connected(network, 256, activation='relu') # 256 neuronów, aktywacja \"relu\"\n",
        "# ---\n",
        "### DO TĄD\n",
        "\n",
        "# warstwa wyjściowa - używamy aktywacji softmax, żeby dostać prawdopodobieństwa dla każdej klasy (cyfry)\n",
        "network = fully_connected(network, 10, activation='softmax')\n",
        "\n",
        "# tu definiujemy w jaki sposób optymalizować sieć (regression nie oznacza, że robimy regresję)\n",
        "# nie będziemy modyfikować tych argumentów; stosujemy optymizator Adam\n",
        "network = regression(network, optimizer='adam', learning_rate=0.01,\n",
        "                     batch_size=250,\n",
        "                     loss='categorical_crossentropy', name='target')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCeyzNLyetOw",
        "outputId": "4609dcca-93ab-4d97-d889-e0c6291c9a1d"
      },
      "source": [
        "# uruchamiamy trenowanie naszej sieci\n",
        "# n_epoch - liczba epok, czyli przejść przez cały zbiór danych treningowych\n",
        "scores = Stats(examples=len(X))\n",
        "model = tflearn.DNN(network, tensorboard_verbose=0)\n",
        "model.fit({'input': X}, {'target': Y},\n",
        "          n_epoch=EPOCHS,  # <-- DO ZMIANY\n",
        "          validation_set=({'input': testX}, {'target': testY}), show_metric=True, callbacks=[scores])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: MX1P07\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "---------------------------------\n",
            "Training samples: 55000\n",
            "Validation samples: 10000\n",
            "--\n",
            "Epoch 1, step (batch no.): 1 -- acc: 0.00, loss 0.00 -- iter 00250/55000, training for: 0.64s\n",
            "Epoch 1, step (batch no.): 4 -- acc: 0.13, loss 2.35 -- iter 01000/55000, training for: 2.08s\n",
            "Epoch 1, step (batch no.): 7 -- acc: 0.23, loss 2.21 -- iter 01750/55000, training for: 3.52s\n",
            "Epoch 1, step (batch no.): 10 -- acc: 0.39, loss 1.87 -- iter 02500/55000, training for: 5.01s\n",
            "Epoch 1, step (batch no.): 12 -- acc: 0.49, loss 1.59 -- iter 03000/55000, training for: 6.01s\n",
            "Epoch 1, step (batch no.): 15 -- acc: 0.64, loss 1.12 -- iter 03750/55000, training for: 7.42s\n",
            "Epoch 1, step (batch no.): 18 -- acc: 0.74, loss 0.85 -- iter 04500/55000, training for: 8.85s\n",
            "Epoch 1, step (batch no.): 21 -- acc: 0.78, loss 0.65 -- iter 05250/55000, training for: 10.27s\n",
            "Epoch 1, step (batch no.): 24 -- acc: 0.84, loss 0.54 -- iter 06000/55000, training for: 11.72s\n",
            "Epoch 1, step (batch no.): 27 -- acc: 0.87, loss 0.42 -- iter 06750/55000, training for: 13.10s\n",
            "Epoch 1, step (batch no.): 30 -- acc: 0.89, loss 0.35 -- iter 07500/55000, training for: 14.55s\n",
            "Epoch 1, step (batch no.): 32 -- acc: 0.89, loss 0.34 -- iter 08000/55000, training for: 15.58s\n",
            "Epoch 1, step (batch no.): 35 -- acc: 0.91, loss 0.29 -- iter 08750/55000, training for: 17.00s\n",
            "Epoch 1, step (batch no.): 38 -- acc: 0.93, loss 0.23 -- iter 09500/55000, training for: 18.38s\n",
            "Epoch 1, step (batch no.): 41 -- acc: 0.93, loss 0.22 -- iter 10250/55000, training for: 19.77s\n",
            "Epoch 1, step (batch no.): 44 -- acc: 0.94, loss 0.20 -- iter 11000/55000, training for: 21.14s\n",
            "Epoch 1, step (batch no.): 47 -- acc: 0.94, loss 0.19 -- iter 11750/55000, training for: 22.57s\n",
            "Epoch 1, step (batch no.): 50 -- acc: 0.94, loss 0.17 -- iter 12500/55000, training for: 23.95s\n",
            "Epoch 1, step (batch no.): 53 -- acc: 0.94, loss 0.18 -- iter 13250/55000, training for: 25.41s\n",
            "Epoch 1, step (batch no.): 56 -- acc: 0.94, loss 0.19 -- iter 14000/55000, training for: 26.86s\n",
            "Epoch 1, step (batch no.): 59 -- acc: 0.94, loss 0.19 -- iter 14750/55000, training for: 28.32s\n",
            "Epoch 1, step (batch no.): 62 -- acc: 0.95, loss 0.18 -- iter 15500/55000, training for: 29.79s\n",
            "Epoch 1, step (batch no.): 65 -- acc: 0.95, loss 0.17 -- iter 16250/55000, training for: 31.23s\n",
            "Epoch 1, step (batch no.): 68 -- acc: 0.95, loss 0.15 -- iter 17000/55000, training for: 32.73s\n",
            "Epoch 1, step (batch no.): 70 -- acc: 0.96, loss 0.15 -- iter 17500/55000, training for: 33.76s\n",
            "Epoch 1, step (batch no.): 72 -- acc: 0.96, loss 0.14 -- iter 18000/55000, training for: 34.77s\n",
            "Epoch 1, step (batch no.): 74 -- acc: 0.96, loss 0.14 -- iter 18500/55000, training for: 35.80s\n",
            "Epoch 1, step (batch no.): 76 -- acc: 0.96, loss 0.13 -- iter 19000/55000, training for: 36.80s\n",
            "Epoch 1, step (batch no.): 79 -- acc: 0.96, loss 0.14 -- iter 19750/55000, training for: 38.27s\n",
            "Epoch 1, step (batch no.): 82 -- acc: 0.96, loss 0.13 -- iter 20500/55000, training for: 39.74s\n",
            "Epoch 1, step (batch no.): 85 -- acc: 0.97, loss 0.12 -- iter 21250/55000, training for: 41.20s\n",
            "Epoch 1, step (batch no.): 87 -- acc: 0.97, loss 0.11 -- iter 21750/55000, training for: 42.22s\n",
            "Epoch 1, step (batch no.): 90 -- acc: 0.97, loss 0.11 -- iter 22500/55000, training for: 43.68s\n",
            "Epoch 1, step (batch no.): 93 -- acc: 0.97, loss 0.11 -- iter 23250/55000, training for: 45.12s\n",
            "Epoch 1, step (batch no.): 96 -- acc: 0.97, loss 0.11 -- iter 24000/55000, training for: 46.54s\n",
            "Epoch 1, step (batch no.): 99 -- acc: 0.97, loss 0.10 -- iter 24750/55000, training for: 47.96s\n",
            "Epoch 1, step (batch no.): 102 -- acc: 0.97, loss 0.11 -- iter 25500/55000, training for: 49.36s\n",
            "Epoch 1, step (batch no.): 105 -- acc: 0.97, loss 0.11 -- iter 26250/55000, training for: 50.74s\n",
            "Epoch 1, step (batch no.): 108 -- acc: 0.97, loss 0.10 -- iter 27000/55000, training for: 52.13s\n",
            "Epoch 1, step (batch no.): 111 -- acc: 0.97, loss 0.10 -- iter 27750/55000, training for: 53.55s\n",
            "Epoch 1, step (batch no.): 114 -- acc: 0.97, loss 0.10 -- iter 28500/55000, training for: 54.95s\n",
            "Epoch 1, step (batch no.): 117 -- acc: 0.97, loss 0.10 -- iter 29250/55000, training for: 56.36s\n",
            "Epoch 1, step (batch no.): 120 -- acc: 0.97, loss 0.09 -- iter 30000/55000, training for: 57.80s\n",
            "Epoch 1, step (batch no.): 123 -- acc: 0.97, loss 0.09 -- iter 30750/55000, training for: 59.24s\n",
            "Epoch 1, step (batch no.): 126 -- acc: 0.97, loss 0.09 -- iter 31500/55000, training for: 60.68s\n",
            "Epoch 1, step (batch no.): 129 -- acc: 0.97, loss 0.10 -- iter 32250/55000, training for: 62.09s\n",
            "Epoch 1, step (batch no.): 132 -- acc: 0.97, loss 0.09 -- iter 33000/55000, training for: 63.51s\n",
            "Epoch 1, step (batch no.): 135 -- acc: 0.97, loss 0.09 -- iter 33750/55000, training for: 64.95s\n",
            "Epoch 1, step (batch no.): 138 -- acc: 0.97, loss 0.10 -- iter 34500/55000, training for: 66.34s\n",
            "Epoch 1, step (batch no.): 141 -- acc: 0.97, loss 0.09 -- iter 35250/55000, training for: 67.74s\n",
            "Epoch 1, step (batch no.): 144 -- acc: 0.97, loss 0.09 -- iter 36000/55000, training for: 69.11s\n",
            "Epoch 1, step (batch no.): 147 -- acc: 0.97, loss 0.09 -- iter 36750/55000, training for: 70.51s\n",
            "Epoch 1, step (batch no.): 150 -- acc: 0.97, loss 0.09 -- iter 37500/55000, training for: 71.91s\n",
            "Epoch 1, step (batch no.): 153 -- acc: 0.98, loss 0.09 -- iter 38250/55000, training for: 73.32s\n",
            "Epoch 1, step (batch no.): 156 -- acc: 0.98, loss 0.08 -- iter 39000/55000, training for: 74.72s\n",
            "Epoch 1, step (batch no.): 159 -- acc: 0.98, loss 0.08 -- iter 39750/55000, training for: 76.17s\n",
            "Epoch 1, step (batch no.): 162 -- acc: 0.98, loss 0.08 -- iter 40500/55000, training for: 77.59s\n",
            "Epoch 1, step (batch no.): 165 -- acc: 0.98, loss 0.08 -- iter 41250/55000, training for: 78.96s\n",
            "Epoch 1, step (batch no.): 168 -- acc: 0.97, loss 0.09 -- iter 42000/55000, training for: 80.33s\n",
            "Epoch 1, step (batch no.): 171 -- acc: 0.98, loss 0.08 -- iter 42750/55000, training for: 81.71s\n",
            "Epoch 1, step (batch no.): 174 -- acc: 0.98, loss 0.08 -- iter 43500/55000, training for: 83.10s\n",
            "Epoch 1, step (batch no.): 177 -- acc: 0.98, loss 0.08 -- iter 44250/55000, training for: 84.49s\n",
            "Epoch 1, step (batch no.): 180 -- acc: 0.98, loss 0.08 -- iter 45000/55000, training for: 85.87s\n",
            "Epoch 1, step (batch no.): 183 -- acc: 0.97, loss 0.09 -- iter 45750/55000, training for: 87.27s\n",
            "Epoch 1, step (batch no.): 186 -- acc: 0.97, loss 0.09 -- iter 46500/55000, training for: 88.68s\n",
            "Epoch 1, step (batch no.): 189 -- acc: 0.88, loss 1.20 -- iter 47250/55000, training for: 90.07s\n",
            "Epoch 1, step (batch no.): 192 -- acc: 0.91, loss 1.01 -- iter 48000/55000, training for: 91.44s\n",
            "Epoch 1, step (batch no.): 195 -- acc: 0.93, loss 0.77 -- iter 48750/55000, training for: 92.92s\n",
            "Epoch 1, step (batch no.): 198 -- acc: 0.94, loss 0.58 -- iter 49500/55000, training for: 94.33s\n",
            "Epoch 1, step (batch no.): 201 -- acc: 0.95, loss 0.45 -- iter 50250/55000, training for: 95.74s\n",
            "Epoch 1, step (batch no.): 204 -- acc: 0.95, loss 0.39 -- iter 51000/55000, training for: 97.14s\n",
            "Epoch 1, step (batch no.): 207 -- acc: 0.96, loss 0.33 -- iter 51750/55000, training for: 98.56s\n",
            "Epoch 1, step (batch no.): 210 -- acc: 0.95, loss 0.30 -- iter 52500/55000, training for: 99.97s\n",
            "Epoch 1, step (batch no.): 213 -- acc: 0.95, loss 0.27 -- iter 53250/55000, training for: 101.40s\n",
            "Epoch 1, step (batch no.): 216 -- acc: 0.96, loss 0.23 -- iter 54000/55000, training for: 102.85s\n",
            "Epoch 1, step (batch no.): 219 -- acc: 0.96, loss 0.21 -- iter 54750/55000, training for: 104.28s\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.19232\u001b[0m\u001b[0m | time: 109.730s\n",
            "| Adam | epoch: 001 | loss: 0.19232 - acc: 0.9593 | val_loss: 0.10333 - val_acc: 0.9668 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 1, step (batch no.): 220 -- acc: 0.96, loss 0.19 -- iter 55000/55000, training for: 109.77s\n",
            "Epoch 2, step (batch no.): 223 -- acc: 0.96, loss 0.17 -- iter 00750/55000, training for: 111.19s\n",
            "Epoch 2, step (batch no.): 226 -- acc: 0.96, loss 0.16 -- iter 01500/55000, training for: 112.60s\n",
            "Epoch 2, step (batch no.): 229 -- acc: 0.97, loss 0.13 -- iter 02250/55000, training for: 114.04s\n",
            "Epoch 2, step (batch no.): 232 -- acc: 0.97, loss 0.12 -- iter 03000/55000, training for: 115.47s\n",
            "Epoch 2, step (batch no.): 235 -- acc: 0.97, loss 0.11 -- iter 03750/55000, training for: 116.93s\n",
            "Epoch 2, step (batch no.): 238 -- acc: 0.97, loss 0.10 -- iter 04500/55000, training for: 118.37s\n",
            "Epoch 2, step (batch no.): 240 -- acc: 0.97, loss 0.10 -- iter 05000/55000, training for: 119.37s\n",
            "Epoch 2, step (batch no.): 243 -- acc: 0.98, loss 0.09 -- iter 05750/55000, training for: 120.82s\n",
            "Epoch 2, step (batch no.): 246 -- acc: 0.98, loss 0.09 -- iter 06500/55000, training for: 122.24s\n",
            "Epoch 2, step (batch no.): 249 -- acc: 0.98, loss 0.08 -- iter 07250/55000, training for: 123.73s\n",
            "Epoch 2, step (batch no.): 252 -- acc: 0.98, loss 0.09 -- iter 08000/55000, training for: 125.20s\n",
            "Epoch 2, step (batch no.): 255 -- acc: 0.98, loss 0.09 -- iter 08750/55000, training for: 126.61s\n",
            "Epoch 2, step (batch no.): 258 -- acc: 0.98, loss 0.09 -- iter 09500/55000, training for: 128.04s\n",
            "Epoch 2, step (batch no.): 261 -- acc: 0.97, loss 0.09 -- iter 10250/55000, training for: 129.46s\n",
            "Epoch 2, step (batch no.): 264 -- acc: 0.98, loss 0.08 -- iter 11000/55000, training for: 130.90s\n",
            "Epoch 2, step (batch no.): 267 -- acc: 0.98, loss 0.09 -- iter 11750/55000, training for: 132.33s\n",
            "Epoch 2, step (batch no.): 270 -- acc: 0.98, loss 0.08 -- iter 12500/55000, training for: 133.76s\n",
            "Epoch 2, step (batch no.): 273 -- acc: 0.98, loss 0.08 -- iter 13250/55000, training for: 135.17s\n",
            "Epoch 2, step (batch no.): 275 -- acc: 0.98, loss 0.08 -- iter 13750/55000, training for: 136.18s\n",
            "Epoch 2, step (batch no.): 278 -- acc: 0.98, loss 0.08 -- iter 14500/55000, training for: 137.64s\n",
            "Epoch 2, step (batch no.): 281 -- acc: 0.98, loss 0.07 -- iter 15250/55000, training for: 139.08s\n",
            "Epoch 2, step (batch no.): 284 -- acc: 0.98, loss 0.06 -- iter 16000/55000, training for: 140.52s\n",
            "Epoch 2, step (batch no.): 287 -- acc: 0.98, loss 0.06 -- iter 16750/55000, training for: 141.94s\n",
            "Epoch 2, step (batch no.): 290 -- acc: 0.98, loss 0.06 -- iter 17500/55000, training for: 143.39s\n",
            "Epoch 2, step (batch no.): 293 -- acc: 0.98, loss 0.06 -- iter 18250/55000, training for: 144.80s\n",
            "Epoch 2, step (batch no.): 296 -- acc: 0.99, loss 0.05 -- iter 19000/55000, training for: 146.19s\n",
            "Epoch 2, step (batch no.): 299 -- acc: 0.99, loss 0.06 -- iter 19750/55000, training for: 147.60s\n",
            "Epoch 2, step (batch no.): 302 -- acc: 0.98, loss 0.06 -- iter 20500/55000, training for: 148.98s\n",
            "Epoch 2, step (batch no.): 305 -- acc: 0.99, loss 0.05 -- iter 21250/55000, training for: 150.41s\n",
            "Epoch 2, step (batch no.): 308 -- acc: 0.98, loss 0.06 -- iter 22000/55000, training for: 151.83s\n",
            "Epoch 2, step (batch no.): 311 -- acc: 0.98, loss 0.06 -- iter 22750/55000, training for: 153.24s\n",
            "Epoch 2, step (batch no.): 314 -- acc: 0.98, loss 0.05 -- iter 23500/55000, training for: 154.63s\n",
            "Epoch 2, step (batch no.): 317 -- acc: 0.98, loss 0.06 -- iter 24250/55000, training for: 156.04s\n",
            "Epoch 2, step (batch no.): 320 -- acc: 0.98, loss 0.06 -- iter 25000/55000, training for: 157.43s\n",
            "Epoch 2, step (batch no.): 323 -- acc: 0.98, loss 0.05 -- iter 25750/55000, training for: 158.85s\n",
            "Epoch 2, step (batch no.): 326 -- acc: 0.98, loss 0.05 -- iter 26500/55000, training for: 160.30s\n",
            "Epoch 2, step (batch no.): 329 -- acc: 0.98, loss 0.05 -- iter 27250/55000, training for: 161.73s\n",
            "Epoch 2, step (batch no.): 332 -- acc: 0.98, loss 0.05 -- iter 28000/55000, training for: 163.15s\n",
            "Epoch 2, step (batch no.): 335 -- acc: 0.98, loss 0.05 -- iter 28750/55000, training for: 164.56s\n",
            "Epoch 2, step (batch no.): 338 -- acc: 0.98, loss 0.06 -- iter 29500/55000, training for: 165.99s\n",
            "Epoch 2, step (batch no.): 341 -- acc: 0.98, loss 0.06 -- iter 30250/55000, training for: 167.42s\n",
            "Epoch 2, step (batch no.): 344 -- acc: 0.98, loss 0.06 -- iter 31000/55000, training for: 168.83s\n",
            "Epoch 2, step (batch no.): 347 -- acc: 0.98, loss 0.06 -- iter 31750/55000, training for: 170.28s\n",
            "Epoch 2, step (batch no.): 350 -- acc: 0.98, loss 0.05 -- iter 32500/55000, training for: 171.71s\n",
            "Epoch 2, step (batch no.): 353 -- acc: 0.98, loss 0.05 -- iter 33250/55000, training for: 173.14s\n",
            "Epoch 2, step (batch no.): 356 -- acc: 0.98, loss 0.05 -- iter 34000/55000, training for: 174.56s\n",
            "Epoch 2, step (batch no.): 359 -- acc: 0.98, loss 0.05 -- iter 34750/55000, training for: 176.01s\n",
            "Epoch 2, step (batch no.): 362 -- acc: 0.98, loss 0.05 -- iter 35500/55000, training for: 177.43s\n",
            "Epoch 2, step (batch no.): 365 -- acc: 0.98, loss 0.05 -- iter 36250/55000, training for: 178.85s\n",
            "Epoch 2, step (batch no.): 368 -- acc: 0.99, loss 0.06 -- iter 37000/55000, training for: 180.29s\n",
            "Epoch 2, step (batch no.): 371 -- acc: 0.99, loss 0.05 -- iter 37750/55000, training for: 181.70s\n",
            "Epoch 2, step (batch no.): 374 -- acc: 0.98, loss 0.05 -- iter 38500/55000, training for: 183.15s\n",
            "Epoch 2, step (batch no.): 377 -- acc: 0.98, loss 0.05 -- iter 39250/55000, training for: 184.58s\n",
            "Epoch 2, step (batch no.): 380 -- acc: 0.98, loss 0.06 -- iter 40000/55000, training for: 186.01s\n",
            "Epoch 2, step (batch no.): 383 -- acc: 0.98, loss 0.06 -- iter 40750/55000, training for: 187.42s\n",
            "Epoch 2, step (batch no.): 386 -- acc: 0.98, loss 0.05 -- iter 41500/55000, training for: 188.83s\n",
            "Epoch 2, step (batch no.): 389 -- acc: 0.99, loss 0.04 -- iter 42250/55000, training for: 190.26s\n",
            "Epoch 2, step (batch no.): 392 -- acc: 0.99, loss 0.04 -- iter 43000/55000, training for: 191.68s\n",
            "Epoch 2, step (batch no.): 395 -- acc: 0.99, loss 0.04 -- iter 43750/55000, training for: 193.10s\n",
            "Epoch 2, step (batch no.): 398 -- acc: 0.99, loss 0.04 -- iter 44500/55000, training for: 194.53s\n",
            "Epoch 2, step (batch no.): 401 -- acc: 0.99, loss 0.04 -- iter 45250/55000, training for: 195.94s\n",
            "Epoch 2, step (batch no.): 404 -- acc: 0.99, loss 0.04 -- iter 46000/55000, training for: 197.36s\n",
            "Epoch 2, step (batch no.): 407 -- acc: 0.99, loss 0.04 -- iter 46750/55000, training for: 198.83s\n",
            "Epoch 2, step (batch no.): 410 -- acc: 0.90, loss 1.46 -- iter 47500/55000, training for: 200.25s\n",
            "Epoch 2, step (batch no.): 413 -- acc: 0.92, loss 1.14 -- iter 48250/55000, training for: 201.71s\n",
            "Epoch 2, step (batch no.): 416 -- acc: 0.94, loss 0.88 -- iter 49000/55000, training for: 203.12s\n",
            "Epoch 2, step (batch no.): 419 -- acc: 0.95, loss 0.65 -- iter 49750/55000, training for: 204.58s\n",
            "Epoch 2, step (batch no.): 422 -- acc: 0.96, loss 0.49 -- iter 50500/55000, training for: 206.00s\n",
            "Epoch 2, step (batch no.): 425 -- acc: 0.96, loss 0.39 -- iter 51250/55000, training for: 207.44s\n",
            "Epoch 2, step (batch no.): 428 -- acc: 0.97, loss 0.31 -- iter 52000/55000, training for: 208.90s\n",
            "Epoch 2, step (batch no.): 431 -- acc: 0.97, loss 0.25 -- iter 52750/55000, training for: 210.30s\n",
            "Epoch 2, step (batch no.): 434 -- acc: 0.97, loss 0.21 -- iter 53500/55000, training for: 211.77s\n",
            "Epoch 2, step (batch no.): 437 -- acc: 0.97, loss 0.18 -- iter 54250/55000, training for: 213.20s\n",
            "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.14578\u001b[0m\u001b[0m | time: 109.705s\n",
            "| Adam | epoch: 002 | loss: 0.14578 - acc: 0.9748 | val_loss: 0.08419 - val_acc: 0.9775 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 2, step (batch no.): 440 -- acc: 0.97, loss 0.15 -- iter 55000/55000, training for: 219.51s\n",
            "Epoch 3, step (batch no.): 443 -- acc: 0.98, loss 0.12 -- iter 00750/55000, training for: 220.95s\n",
            "Epoch 3, step (batch no.): 446 -- acc: 0.97, loss 0.11 -- iter 01500/55000, training for: 222.37s\n",
            "Epoch 3, step (batch no.): 449 -- acc: 0.98, loss 0.09 -- iter 02250/55000, training for: 223.81s\n",
            "Epoch 3, step (batch no.): 452 -- acc: 0.98, loss 0.08 -- iter 03000/55000, training for: 225.23s\n",
            "Epoch 3, step (batch no.): 455 -- acc: 0.98, loss 0.07 -- iter 03750/55000, training for: 226.60s\n",
            "Epoch 3, step (batch no.): 458 -- acc: 0.98, loss 0.07 -- iter 04500/55000, training for: 228.02s\n",
            "Epoch 3, step (batch no.): 461 -- acc: 0.98, loss 0.07 -- iter 05250/55000, training for: 229.38s\n",
            "Epoch 3, step (batch no.): 464 -- acc: 0.98, loss 0.07 -- iter 06000/55000, training for: 230.77s\n",
            "Epoch 3, step (batch no.): 467 -- acc: 0.98, loss 0.06 -- iter 06750/55000, training for: 232.15s\n",
            "Epoch 3, step (batch no.): 470 -- acc: 0.99, loss 0.05 -- iter 07500/55000, training for: 233.54s\n",
            "Epoch 3, step (batch no.): 473 -- acc: 0.98, loss 0.05 -- iter 08250/55000, training for: 234.96s\n",
            "Epoch 3, step (batch no.): 476 -- acc: 0.99, loss 0.05 -- iter 09000/55000, training for: 236.37s\n",
            "Epoch 3, step (batch no.): 479 -- acc: 0.99, loss 0.05 -- iter 09750/55000, training for: 237.78s\n",
            "Epoch 3, step (batch no.): 482 -- acc: 0.99, loss 0.04 -- iter 10500/55000, training for: 239.22s\n",
            "Epoch 3, step (batch no.): 485 -- acc: 0.99, loss 0.04 -- iter 11250/55000, training for: 240.63s\n",
            "Epoch 3, step (batch no.): 488 -- acc: 0.99, loss 0.04 -- iter 12000/55000, training for: 242.08s\n",
            "Epoch 3, step (batch no.): 491 -- acc: 0.99, loss 0.04 -- iter 12750/55000, training for: 243.54s\n",
            "Epoch 3, step (batch no.): 494 -- acc: 0.99, loss 0.04 -- iter 13500/55000, training for: 244.94s\n",
            "Epoch 3, step (batch no.): 497 -- acc: 0.98, loss 0.05 -- iter 14250/55000, training for: 246.34s\n",
            "Epoch 3, step (batch no.): 500 -- acc: 0.99, loss 0.05 -- iter 15000/55000, training for: 247.70s\n",
            "Epoch 3, step (batch no.): 503 -- acc: 0.98, loss 0.05 -- iter 15750/55000, training for: 249.07s\n",
            "Epoch 3, step (batch no.): 506 -- acc: 0.98, loss 0.05 -- iter 16500/55000, training for: 250.47s\n",
            "Epoch 3, step (batch no.): 509 -- acc: 0.98, loss 0.05 -- iter 17250/55000, training for: 251.83s\n",
            "Epoch 3, step (batch no.): 512 -- acc: 0.99, loss 0.05 -- iter 18000/55000, training for: 253.23s\n",
            "Epoch 3, step (batch no.): 515 -- acc: 0.99, loss 0.04 -- iter 18750/55000, training for: 254.59s\n",
            "Epoch 3, step (batch no.): 518 -- acc: 0.99, loss 0.05 -- iter 19500/55000, training for: 255.96s\n",
            "Epoch 3, step (batch no.): 521 -- acc: 0.99, loss 0.04 -- iter 20250/55000, training for: 257.34s\n",
            "Epoch 3, step (batch no.): 524 -- acc: 0.99, loss 0.04 -- iter 21000/55000, training for: 258.71s\n",
            "Epoch 3, step (batch no.): 527 -- acc: 0.99, loss 0.04 -- iter 21750/55000, training for: 260.08s\n",
            "Epoch 3, step (batch no.): 530 -- acc: 0.99, loss 0.04 -- iter 22500/55000, training for: 261.47s\n",
            "Epoch 3, step (batch no.): 533 -- acc: 0.99, loss 0.04 -- iter 23250/55000, training for: 262.89s\n",
            "Epoch 3, step (batch no.): 536 -- acc: 0.99, loss 0.04 -- iter 24000/55000, training for: 264.29s\n",
            "Epoch 3, step (batch no.): 539 -- acc: 0.99, loss 0.04 -- iter 24750/55000, training for: 265.67s\n",
            "Epoch 3, step (batch no.): 542 -- acc: 0.99, loss 0.04 -- iter 25500/55000, training for: 267.08s\n",
            "Epoch 3, step (batch no.): 545 -- acc: 0.99, loss 0.04 -- iter 26250/55000, training for: 268.47s\n",
            "Epoch 3, step (batch no.): 548 -- acc: 0.99, loss 0.04 -- iter 27000/55000, training for: 269.89s\n",
            "Epoch 3, step (batch no.): 551 -- acc: 0.99, loss 0.03 -- iter 27750/55000, training for: 271.31s\n",
            "Epoch 3, step (batch no.): 554 -- acc: 0.99, loss 0.03 -- iter 28500/55000, training for: 272.74s\n",
            "Epoch 3, step (batch no.): 557 -- acc: 0.99, loss 0.03 -- iter 29250/55000, training for: 274.18s\n",
            "Epoch 3, step (batch no.): 560 -- acc: 0.99, loss 0.03 -- iter 30000/55000, training for: 275.60s\n",
            "Epoch 3, step (batch no.): 563 -- acc: 0.99, loss 0.04 -- iter 30750/55000, training for: 276.98s\n",
            "Epoch 3, step (batch no.): 566 -- acc: 0.99, loss 0.04 -- iter 31500/55000, training for: 278.34s\n",
            "Epoch 3, step (batch no.): 569 -- acc: 0.99, loss 0.04 -- iter 32250/55000, training for: 279.73s\n",
            "Epoch 3, step (batch no.): 572 -- acc: 0.99, loss 0.04 -- iter 33000/55000, training for: 281.14s\n",
            "Epoch 3, step (batch no.): 575 -- acc: 0.99, loss 0.04 -- iter 33750/55000, training for: 282.53s\n",
            "Epoch 3, step (batch no.): 578 -- acc: 0.99, loss 0.04 -- iter 34500/55000, training for: 283.94s\n",
            "Epoch 3, step (batch no.): 581 -- acc: 0.99, loss 0.04 -- iter 35250/55000, training for: 285.35s\n",
            "Epoch 3, step (batch no.): 584 -- acc: 0.99, loss 0.04 -- iter 36000/55000, training for: 286.79s\n",
            "Epoch 3, step (batch no.): 587 -- acc: 0.99, loss 0.04 -- iter 36750/55000, training for: 288.19s\n",
            "Epoch 3, step (batch no.): 590 -- acc: 0.99, loss 0.04 -- iter 37500/55000, training for: 289.58s\n",
            "Epoch 3, step (batch no.): 593 -- acc: 0.99, loss 0.04 -- iter 38250/55000, training for: 290.99s\n",
            "Epoch 3, step (batch no.): 596 -- acc: 0.99, loss 0.04 -- iter 39000/55000, training for: 292.41s\n",
            "Epoch 3, step (batch no.): 599 -- acc: 0.99, loss 0.04 -- iter 39750/55000, training for: 293.81s\n",
            "Epoch 3, step (batch no.): 602 -- acc: 0.99, loss 0.03 -- iter 40500/55000, training for: 295.22s\n",
            "Epoch 3, step (batch no.): 605 -- acc: 0.99, loss 0.03 -- iter 41250/55000, training for: 296.62s\n",
            "Epoch 3, step (batch no.): 608 -- acc: 0.99, loss 0.04 -- iter 42000/55000, training for: 298.04s\n",
            "Epoch 3, step (batch no.): 611 -- acc: 0.99, loss 0.04 -- iter 42750/55000, training for: 299.46s\n",
            "Epoch 3, step (batch no.): 614 -- acc: 0.99, loss 0.04 -- iter 43500/55000, training for: 300.88s\n",
            "Epoch 3, step (batch no.): 617 -- acc: 0.99, loss 0.04 -- iter 44250/55000, training for: 302.34s\n",
            "Epoch 3, step (batch no.): 620 -- acc: 0.99, loss 0.04 -- iter 45000/55000, training for: 303.80s\n",
            "Epoch 3, step (batch no.): 623 -- acc: 0.99, loss 0.03 -- iter 45750/55000, training for: 305.25s\n",
            "Epoch 3, step (batch no.): 626 -- acc: 0.99, loss 0.03 -- iter 46500/55000, training for: 306.72s\n",
            "Epoch 3, step (batch no.): 629 -- acc: 0.99, loss 0.04 -- iter 47250/55000, training for: 308.15s\n",
            "Epoch 3, step (batch no.): 632 -- acc: 0.90, loss 1.28 -- iter 48000/55000, training for: 309.57s\n",
            "Epoch 3, step (batch no.): 635 -- acc: 0.92, loss 1.01 -- iter 48750/55000, training for: 310.95s\n",
            "Epoch 3, step (batch no.): 638 -- acc: 0.93, loss 0.78 -- iter 49500/55000, training for: 312.34s\n",
            "Epoch 3, step (batch no.): 641 -- acc: 0.95, loss 0.58 -- iter 50250/55000, training for: 313.76s\n",
            "Epoch 3, step (batch no.): 644 -- acc: 0.96, loss 0.44 -- iter 51000/55000, training for: 315.14s\n",
            "Epoch 3, step (batch no.): 647 -- acc: 0.97, loss 0.33 -- iter 51750/55000, training for: 316.55s\n",
            "Epoch 3, step (batch no.): 650 -- acc: 0.97, loss 0.26 -- iter 52500/55000, training for: 317.96s\n",
            "Epoch 3, step (batch no.): 653 -- acc: 0.98, loss 0.21 -- iter 53250/55000, training for: 319.37s\n",
            "Epoch 3, step (batch no.): 656 -- acc: 0.98, loss 0.16 -- iter 54000/55000, training for: 320.75s\n",
            "Epoch 3, step (batch no.): 659 -- acc: 0.98, loss 0.13 -- iter 54750/55000, training for: 322.12s\n",
            "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.13048\u001b[0m\u001b[0m | time: 107.843s\n",
            "| Adam | epoch: 003 | loss: 0.13048 - acc: 0.9810 | val_loss: 0.10179 - val_acc: 0.9735 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 3, step (batch no.): 660 -- acc: 0.98, loss 0.13 -- iter 55000/55000, training for: 327.38s\n",
            "Epoch 4, step (batch no.): 663 -- acc: 0.98, loss 0.11 -- iter 00750/55000, training for: 328.79s\n",
            "Epoch 4, step (batch no.): 666 -- acc: 0.98, loss 0.09 -- iter 01500/55000, training for: 330.19s\n",
            "Epoch 4, step (batch no.): 669 -- acc: 0.98, loss 0.08 -- iter 02250/55000, training for: 331.60s\n",
            "Epoch 4, step (batch no.): 672 -- acc: 0.98, loss 0.07 -- iter 03000/55000, training for: 333.03s\n",
            "Epoch 4, step (batch no.): 675 -- acc: 0.98, loss 0.07 -- iter 03750/55000, training for: 334.43s\n",
            "Epoch 4, step (batch no.): 678 -- acc: 0.99, loss 0.06 -- iter 04500/55000, training for: 335.85s\n",
            "Epoch 4, step (batch no.): 681 -- acc: 0.99, loss 0.06 -- iter 05250/55000, training for: 337.26s\n",
            "Epoch 4, step (batch no.): 684 -- acc: 0.99, loss 0.05 -- iter 06000/55000, training for: 338.67s\n",
            "Epoch 4, step (batch no.): 687 -- acc: 0.99, loss 0.05 -- iter 06750/55000, training for: 340.12s\n",
            "Epoch 4, step (batch no.): 690 -- acc: 0.99, loss 0.04 -- iter 07500/55000, training for: 341.53s\n",
            "Epoch 4, step (batch no.): 693 -- acc: 0.99, loss 0.05 -- iter 08250/55000, training for: 342.94s\n",
            "Epoch 4, step (batch no.): 696 -- acc: 0.98, loss 0.05 -- iter 09000/55000, training for: 344.35s\n",
            "Epoch 4, step (batch no.): 699 -- acc: 0.99, loss 0.05 -- iter 09750/55000, training for: 345.77s\n",
            "Epoch 4, step (batch no.): 702 -- acc: 0.99, loss 0.04 -- iter 10500/55000, training for: 347.15s\n",
            "Epoch 4, step (batch no.): 705 -- acc: 0.99, loss 0.04 -- iter 11250/55000, training for: 348.59s\n",
            "Epoch 4, step (batch no.): 708 -- acc: 0.99, loss 0.03 -- iter 12000/55000, training for: 349.98s\n",
            "Epoch 4, step (batch no.): 711 -- acc: 0.99, loss 0.04 -- iter 12750/55000, training for: 351.37s\n",
            "Epoch 4, step (batch no.): 714 -- acc: 0.99, loss 0.04 -- iter 13500/55000, training for: 352.79s\n",
            "Epoch 4, step (batch no.): 717 -- acc: 0.99, loss 0.04 -- iter 14250/55000, training for: 354.18s\n",
            "Epoch 4, step (batch no.): 720 -- acc: 0.99, loss 0.04 -- iter 15000/55000, training for: 355.59s\n",
            "Epoch 4, step (batch no.): 723 -- acc: 0.99, loss 0.04 -- iter 15750/55000, training for: 357.03s\n",
            "Epoch 4, step (batch no.): 726 -- acc: 0.99, loss 0.04 -- iter 16500/55000, training for: 358.43s\n",
            "Epoch 4, step (batch no.): 729 -- acc: 0.99, loss 0.04 -- iter 17250/55000, training for: 359.87s\n",
            "Epoch 4, step (batch no.): 732 -- acc: 0.99, loss 0.04 -- iter 18000/55000, training for: 361.29s\n",
            "Epoch 4, step (batch no.): 735 -- acc: 0.99, loss 0.03 -- iter 18750/55000, training for: 362.68s\n",
            "Epoch 4, step (batch no.): 738 -- acc: 0.99, loss 0.03 -- iter 19500/55000, training for: 364.15s\n",
            "Epoch 4, step (batch no.): 741 -- acc: 0.99, loss 0.03 -- iter 20250/55000, training for: 365.54s\n",
            "Epoch 4, step (batch no.): 744 -- acc: 0.99, loss 0.04 -- iter 21000/55000, training for: 367.04s\n",
            "Epoch 4, step (batch no.): 747 -- acc: 0.99, loss 0.04 -- iter 21750/55000, training for: 368.48s\n",
            "Epoch 4, step (batch no.): 750 -- acc: 0.99, loss 0.04 -- iter 22500/55000, training for: 369.92s\n",
            "Epoch 4, step (batch no.): 753 -- acc: 0.99, loss 0.04 -- iter 23250/55000, training for: 371.31s\n",
            "Epoch 4, step (batch no.): 756 -- acc: 0.99, loss 0.04 -- iter 24000/55000, training for: 372.73s\n",
            "Epoch 4, step (batch no.): 759 -- acc: 0.99, loss 0.04 -- iter 24750/55000, training for: 374.18s\n",
            "Epoch 4, step (batch no.): 762 -- acc: 0.99, loss 0.04 -- iter 25500/55000, training for: 375.60s\n",
            "Epoch 4, step (batch no.): 765 -- acc: 0.99, loss 0.04 -- iter 26250/55000, training for: 377.02s\n",
            "Epoch 4, step (batch no.): 768 -- acc: 0.99, loss 0.04 -- iter 27000/55000, training for: 378.44s\n",
            "Epoch 4, step (batch no.): 771 -- acc: 0.99, loss 0.04 -- iter 27750/55000, training for: 379.87s\n",
            "Epoch 4, step (batch no.): 774 -- acc: 0.99, loss 0.03 -- iter 28500/55000, training for: 381.30s\n",
            "Epoch 4, step (batch no.): 777 -- acc: 0.99, loss 0.03 -- iter 29250/55000, training for: 382.70s\n",
            "Epoch 4, step (batch no.): 780 -- acc: 0.99, loss 0.03 -- iter 30000/55000, training for: 384.11s\n",
            "Epoch 4, step (batch no.): 783 -- acc: 0.99, loss 0.03 -- iter 30750/55000, training for: 385.50s\n",
            "Epoch 4, step (batch no.): 786 -- acc: 0.99, loss 0.03 -- iter 31500/55000, training for: 386.94s\n",
            "Epoch 4, step (batch no.): 789 -- acc: 0.99, loss 0.03 -- iter 32250/55000, training for: 388.32s\n",
            "Epoch 4, step (batch no.): 792 -- acc: 0.99, loss 0.04 -- iter 33000/55000, training for: 389.72s\n",
            "Epoch 4, step (batch no.): 795 -- acc: 0.99, loss 0.04 -- iter 33750/55000, training for: 391.14s\n",
            "Epoch 4, step (batch no.): 798 -- acc: 0.99, loss 0.04 -- iter 34500/55000, training for: 392.56s\n",
            "Epoch 4, step (batch no.): 801 -- acc: 0.99, loss 0.04 -- iter 35250/55000, training for: 394.01s\n",
            "Epoch 4, step (batch no.): 804 -- acc: 0.99, loss 0.03 -- iter 36000/55000, training for: 395.41s\n",
            "Epoch 4, step (batch no.): 807 -- acc: 0.99, loss 0.03 -- iter 36750/55000, training for: 396.83s\n",
            "Epoch 4, step (batch no.): 810 -- acc: 0.99, loss 0.03 -- iter 37500/55000, training for: 398.26s\n",
            "Epoch 4, step (batch no.): 813 -- acc: 0.99, loss 0.03 -- iter 38250/55000, training for: 399.67s\n",
            "Epoch 4, step (batch no.): 816 -- acc: 0.99, loss 0.03 -- iter 39000/55000, training for: 401.10s\n",
            "Epoch 4, step (batch no.): 819 -- acc: 0.99, loss 0.03 -- iter 39750/55000, training for: 402.51s\n",
            "Epoch 4, step (batch no.): 822 -- acc: 0.99, loss 0.04 -- iter 40500/55000, training for: 403.94s\n",
            "Epoch 4, step (batch no.): 825 -- acc: 0.99, loss 0.04 -- iter 41250/55000, training for: 405.37s\n",
            "Epoch 4, step (batch no.): 828 -- acc: 0.99, loss 0.04 -- iter 42000/55000, training for: 406.82s\n",
            "Epoch 4, step (batch no.): 831 -- acc: 0.99, loss 0.04 -- iter 42750/55000, training for: 408.23s\n",
            "Epoch 4, step (batch no.): 834 -- acc: 0.99, loss 0.05 -- iter 43500/55000, training for: 409.62s\n",
            "Epoch 4, step (batch no.): 837 -- acc: 0.99, loss 0.04 -- iter 44250/55000, training for: 411.04s\n",
            "Epoch 4, step (batch no.): 840 -- acc: 0.99, loss 0.05 -- iter 45000/55000, training for: 412.45s\n",
            "Epoch 4, step (batch no.): 843 -- acc: 0.98, loss 0.05 -- iter 45750/55000, training for: 413.87s\n",
            "Epoch 4, step (batch no.): 846 -- acc: 0.99, loss 0.05 -- iter 46500/55000, training for: 415.26s\n",
            "Epoch 4, step (batch no.): 849 -- acc: 0.99, loss 0.04 -- iter 47250/55000, training for: 416.64s\n",
            "Epoch 4, step (batch no.): 852 -- acc: 0.90, loss 1.32 -- iter 48000/55000, training for: 418.06s\n",
            "Epoch 4, step (batch no.): 855 -- acc: 0.92, loss 1.01 -- iter 48750/55000, training for: 419.49s\n",
            "Epoch 4, step (batch no.): 858 -- acc: 0.94, loss 0.80 -- iter 49500/55000, training for: 420.94s\n",
            "Epoch 4, step (batch no.): 861 -- acc: 0.95, loss 0.60 -- iter 50250/55000, training for: 422.34s\n",
            "Epoch 4, step (batch no.): 864 -- acc: 0.96, loss 0.45 -- iter 51000/55000, training for: 423.76s\n",
            "Epoch 4, step (batch no.): 867 -- acc: 0.97, loss 0.34 -- iter 51750/55000, training for: 425.19s\n",
            "Epoch 4, step (batch no.): 870 -- acc: 0.97, loss 0.26 -- iter 52500/55000, training for: 426.61s\n",
            "Epoch 4, step (batch no.): 873 -- acc: 0.98, loss 0.19 -- iter 53250/55000, training for: 428.07s\n",
            "Epoch 4, step (batch no.): 876 -- acc: 0.98, loss 0.15 -- iter 54000/55000, training for: 429.46s\n",
            "Epoch 4, step (batch no.): 879 -- acc: 0.98, loss 0.12 -- iter 54750/55000, training for: 430.86s\n",
            "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.11350\u001b[0m\u001b[0m | time: 108.762s\n",
            "| Adam | epoch: 004 | loss: 0.11350 - acc: 0.9823 | val_loss: 0.06832 - val_acc: 0.9851 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 4, step (batch no.): 880 -- acc: 0.98, loss 0.11 -- iter 55000/55000, training for: 436.18s\n",
            "Epoch 5, step (batch no.): 883 -- acc: 0.99, loss 0.09 -- iter 00750/55000, training for: 437.62s\n",
            "Epoch 5, step (batch no.): 885 -- acc: 0.99, loss 0.07 -- iter 01250/55000, training for: 438.62s\n",
            "Epoch 5, step (batch no.): 888 -- acc: 0.99, loss 0.08 -- iter 02000/55000, training for: 440.04s\n",
            "Epoch 5, step (batch no.): 891 -- acc: 0.99, loss 0.07 -- iter 02750/55000, training for: 441.44s\n",
            "Epoch 5, step (batch no.): 894 -- acc: 0.99, loss 0.06 -- iter 03500/55000, training for: 442.83s\n",
            "Epoch 5, step (batch no.): 897 -- acc: 0.99, loss 0.06 -- iter 04250/55000, training for: 444.30s\n",
            "Epoch 5, step (batch no.): 900 -- acc: 0.98, loss 0.06 -- iter 05000/55000, training for: 445.71s\n",
            "Epoch 5, step (batch no.): 903 -- acc: 0.98, loss 0.06 -- iter 05750/55000, training for: 447.12s\n",
            "Epoch 5, step (batch no.): 906 -- acc: 0.98, loss 0.06 -- iter 06500/55000, training for: 448.58s\n",
            "Epoch 5, step (batch no.): 909 -- acc: 0.98, loss 0.06 -- iter 07250/55000, training for: 449.99s\n",
            "Epoch 5, step (batch no.): 912 -- acc: 0.98, loss 0.05 -- iter 08000/55000, training for: 451.43s\n",
            "Epoch 5, step (batch no.): 915 -- acc: 0.98, loss 0.05 -- iter 08750/55000, training for: 452.82s\n",
            "Epoch 5, step (batch no.): 918 -- acc: 0.98, loss 0.06 -- iter 09500/55000, training for: 454.24s\n",
            "Epoch 5, step (batch no.): 921 -- acc: 0.98, loss 0.06 -- iter 10250/55000, training for: 455.65s\n",
            "Epoch 5, step (batch no.): 924 -- acc: 0.98, loss 0.05 -- iter 11000/55000, training for: 457.05s\n",
            "Epoch 5, step (batch no.): 927 -- acc: 0.99, loss 0.05 -- iter 11750/55000, training for: 458.48s\n",
            "Epoch 5, step (batch no.): 930 -- acc: 0.98, loss 0.05 -- iter 12500/55000, training for: 459.94s\n",
            "Epoch 5, step (batch no.): 933 -- acc: 0.99, loss 0.05 -- iter 13250/55000, training for: 461.38s\n",
            "Epoch 5, step (batch no.): 936 -- acc: 0.99, loss 0.04 -- iter 14000/55000, training for: 462.79s\n",
            "Epoch 5, step (batch no.): 939 -- acc: 0.99, loss 0.04 -- iter 14750/55000, training for: 464.22s\n",
            "Epoch 5, step (batch no.): 942 -- acc: 0.99, loss 0.04 -- iter 15500/55000, training for: 465.64s\n",
            "Epoch 5, step (batch no.): 945 -- acc: 0.99, loss 0.03 -- iter 16250/55000, training for: 467.10s\n",
            "Epoch 5, step (batch no.): 948 -- acc: 0.99, loss 0.04 -- iter 17000/55000, training for: 468.55s\n",
            "Epoch 5, step (batch no.): 951 -- acc: 0.99, loss 0.04 -- iter 17750/55000, training for: 469.98s\n",
            "Epoch 5, step (batch no.): 954 -- acc: 0.99, loss 0.04 -- iter 18500/55000, training for: 471.41s\n",
            "Epoch 5, step (batch no.): 957 -- acc: 0.99, loss 0.05 -- iter 19250/55000, training for: 472.87s\n",
            "Epoch 5, step (batch no.): 960 -- acc: 0.99, loss 0.04 -- iter 20000/55000, training for: 474.25s\n",
            "Epoch 5, step (batch no.): 963 -- acc: 0.99, loss 0.04 -- iter 20750/55000, training for: 475.67s\n",
            "Epoch 5, step (batch no.): 966 -- acc: 0.99, loss 0.04 -- iter 21500/55000, training for: 477.10s\n",
            "Epoch 5, step (batch no.): 969 -- acc: 0.99, loss 0.04 -- iter 22250/55000, training for: 478.51s\n",
            "Epoch 5, step (batch no.): 972 -- acc: 0.99, loss 0.03 -- iter 23000/55000, training for: 479.97s\n",
            "Epoch 5, step (batch no.): 975 -- acc: 0.99, loss 0.03 -- iter 23750/55000, training for: 481.38s\n",
            "Epoch 5, step (batch no.): 978 -- acc: 0.99, loss 0.03 -- iter 24500/55000, training for: 482.79s\n",
            "Epoch 5, step (batch no.): 981 -- acc: 0.99, loss 0.03 -- iter 25250/55000, training for: 484.26s\n",
            "Epoch 5, step (batch no.): 984 -- acc: 0.99, loss 0.03 -- iter 26000/55000, training for: 485.67s\n",
            "Epoch 5, step (batch no.): 987 -- acc: 0.99, loss 0.03 -- iter 26750/55000, training for: 487.08s\n",
            "Epoch 5, step (batch no.): 990 -- acc: 0.99, loss 0.02 -- iter 27500/55000, training for: 488.48s\n",
            "Epoch 5, step (batch no.): 993 -- acc: 0.99, loss 0.02 -- iter 28250/55000, training for: 489.91s\n",
            "Epoch 5, step (batch no.): 996 -- acc: 0.99, loss 0.02 -- iter 29000/55000, training for: 491.33s\n",
            "Epoch 5, step (batch no.): 999 -- acc: 0.99, loss 0.02 -- iter 29750/55000, training for: 492.76s\n",
            "Epoch 5, step (batch no.): 1002 -- acc: 0.99, loss 0.03 -- iter 30500/55000, training for: 494.17s\n",
            "Epoch 5, step (batch no.): 1005 -- acc: 0.99, loss 0.03 -- iter 31250/55000, training for: 495.56s\n",
            "Epoch 5, step (batch no.): 1008 -- acc: 0.99, loss 0.02 -- iter 32000/55000, training for: 496.95s\n",
            "Epoch 5, step (batch no.): 1011 -- acc: 0.99, loss 0.03 -- iter 32750/55000, training for: 498.33s\n",
            "Epoch 5, step (batch no.): 1014 -- acc: 0.99, loss 0.03 -- iter 33500/55000, training for: 499.75s\n",
            "Epoch 5, step (batch no.): 1017 -- acc: 0.99, loss 0.03 -- iter 34250/55000, training for: 501.18s\n",
            "Epoch 5, step (batch no.): 1020 -- acc: 0.99, loss 0.03 -- iter 35000/55000, training for: 502.59s\n",
            "Epoch 5, step (batch no.): 1023 -- acc: 0.99, loss 0.03 -- iter 35750/55000, training for: 504.04s\n",
            "Epoch 5, step (batch no.): 1026 -- acc: 0.99, loss 0.03 -- iter 36500/55000, training for: 505.50s\n",
            "Epoch 5, step (batch no.): 1029 -- acc: 0.99, loss 0.03 -- iter 37250/55000, training for: 506.93s\n",
            "Epoch 5, step (batch no.): 1032 -- acc: 0.99, loss 0.03 -- iter 38000/55000, training for: 508.36s\n",
            "Epoch 5, step (batch no.): 1035 -- acc: 0.99, loss 0.03 -- iter 38750/55000, training for: 509.76s\n",
            "Epoch 5, step (batch no.): 1038 -- acc: 0.99, loss 0.03 -- iter 39500/55000, training for: 511.18s\n",
            "Epoch 5, step (batch no.): 1041 -- acc: 0.99, loss 0.03 -- iter 40250/55000, training for: 512.59s\n",
            "Epoch 5, step (batch no.): 1044 -- acc: 0.99, loss 0.03 -- iter 41000/55000, training for: 514.01s\n",
            "Epoch 5, step (batch no.): 1047 -- acc: 0.99, loss 0.03 -- iter 41750/55000, training for: 515.45s\n",
            "Epoch 5, step (batch no.): 1050 -- acc: 0.99, loss 0.02 -- iter 42500/55000, training for: 516.90s\n",
            "Epoch 5, step (batch no.): 1053 -- acc: 0.99, loss 0.02 -- iter 43250/55000, training for: 518.35s\n",
            "Epoch 5, step (batch no.): 1056 -- acc: 0.99, loss 0.02 -- iter 44000/55000, training for: 519.77s\n",
            "Epoch 5, step (batch no.): 1059 -- acc: 0.99, loss 0.02 -- iter 44750/55000, training for: 521.20s\n",
            "Epoch 5, step (batch no.): 1062 -- acc: 0.99, loss 0.02 -- iter 45500/55000, training for: 522.65s\n",
            "Epoch 5, step (batch no.): 1065 -- acc: 0.99, loss 0.02 -- iter 46250/55000, training for: 524.08s\n",
            "Epoch 5, step (batch no.): 1068 -- acc: 0.99, loss 0.02 -- iter 47000/55000, training for: 525.53s\n",
            "Epoch 5, step (batch no.): 1071 -- acc: 0.99, loss 0.02 -- iter 47750/55000, training for: 526.93s\n",
            "Epoch 5, step (batch no.): 1074 -- acc: 0.99, loss 0.02 -- iter 48500/55000, training for: 528.36s\n",
            "Epoch 5, step (batch no.): 1077 -- acc: 0.99, loss 0.02 -- iter 49250/55000, training for: 529.78s\n",
            "Epoch 5, step (batch no.): 1080 -- acc: 0.99, loss 0.02 -- iter 50000/55000, training for: 531.19s\n",
            "Epoch 5, step (batch no.): 1083 -- acc: 0.99, loss 0.02 -- iter 50750/55000, training for: 532.63s\n",
            "Epoch 5, step (batch no.): 1086 -- acc: 0.99, loss 0.02 -- iter 51500/55000, training for: 534.03s\n",
            "Epoch 5, step (batch no.): 1089 -- acc: 0.99, loss 0.02 -- iter 52250/55000, training for: 535.46s\n",
            "Epoch 5, step (batch no.): 1092 -- acc: 0.99, loss 0.02 -- iter 53000/55000, training for: 536.87s\n",
            "Epoch 5, step (batch no.): 1095 -- acc: 0.99, loss 0.03 -- iter 53750/55000, training for: 538.38s\n",
            "Epoch 5, step (batch no.): 1098 -- acc: 0.99, loss 0.02 -- iter 54500/55000, training for: 539.80s\n",
            "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.02058\u001b[0m\u001b[0m | time: 109.431s\n",
            "| Adam | epoch: 005 | loss: 0.02058 - acc: 0.9943 | val_loss: 0.04620 - val_acc: 0.9867 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 5, step (batch no.): 1100 -- acc: 0.99, loss 0.02 -- iter 55000/55000, training for: 545.63s\n",
            "Epoch 6, step (batch no.): 1103 -- acc: 0.99, loss 0.02 -- iter 00750/55000, training for: 547.07s\n",
            "Epoch 6, step (batch no.): 1106 -- acc: 0.99, loss 0.03 -- iter 01500/55000, training for: 548.48s\n",
            "Epoch 6, step (batch no.): 1109 -- acc: 0.99, loss 0.03 -- iter 02250/55000, training for: 549.87s\n",
            "Epoch 6, step (batch no.): 1112 -- acc: 0.99, loss 0.03 -- iter 03000/55000, training for: 551.27s\n",
            "Epoch 6, step (batch no.): 1115 -- acc: 0.99, loss 0.03 -- iter 03750/55000, training for: 552.68s\n",
            "Epoch 6, step (batch no.): 1118 -- acc: 0.99, loss 0.03 -- iter 04500/55000, training for: 554.11s\n",
            "Epoch 6, step (batch no.): 1121 -- acc: 0.99, loss 0.03 -- iter 05250/55000, training for: 555.50s\n",
            "Epoch 6, step (batch no.): 1124 -- acc: 0.99, loss 0.03 -- iter 06000/55000, training for: 556.91s\n",
            "Epoch 6, step (batch no.): 1127 -- acc: 0.99, loss 0.03 -- iter 06750/55000, training for: 558.32s\n",
            "Epoch 6, step (batch no.): 1130 -- acc: 0.99, loss 0.03 -- iter 07500/55000, training for: 559.72s\n",
            "Epoch 6, step (batch no.): 1133 -- acc: 0.99, loss 0.03 -- iter 08250/55000, training for: 561.12s\n",
            "Epoch 6, step (batch no.): 1136 -- acc: 0.99, loss 0.02 -- iter 09000/55000, training for: 562.51s\n",
            "Epoch 6, step (batch no.): 1139 -- acc: 0.99, loss 0.02 -- iter 09750/55000, training for: 563.92s\n",
            "Epoch 6, step (batch no.): 1142 -- acc: 0.99, loss 0.02 -- iter 10500/55000, training for: 565.33s\n",
            "Epoch 6, step (batch no.): 1145 -- acc: 0.99, loss 0.02 -- iter 11250/55000, training for: 566.72s\n",
            "Epoch 6, step (batch no.): 1148 -- acc: 0.99, loss 0.02 -- iter 12000/55000, training for: 568.15s\n",
            "Epoch 6, step (batch no.): 1151 -- acc: 1.00, loss 0.02 -- iter 12750/55000, training for: 569.59s\n",
            "Epoch 6, step (batch no.): 1154 -- acc: 0.99, loss 0.02 -- iter 13500/55000, training for: 571.03s\n",
            "Epoch 6, step (batch no.): 1157 -- acc: 0.99, loss 0.02 -- iter 14250/55000, training for: 572.50s\n",
            "Epoch 6, step (batch no.): 1160 -- acc: 0.99, loss 0.02 -- iter 15000/55000, training for: 573.88s\n",
            "Epoch 6, step (batch no.): 1163 -- acc: 0.99, loss 0.02 -- iter 15750/55000, training for: 575.28s\n",
            "Epoch 6, step (batch no.): 1166 -- acc: 0.99, loss 0.02 -- iter 16500/55000, training for: 576.67s\n",
            "Epoch 6, step (batch no.): 1169 -- acc: 0.99, loss 0.02 -- iter 17250/55000, training for: 578.05s\n",
            "Epoch 6, step (batch no.): 1172 -- acc: 0.99, loss 0.02 -- iter 18000/55000, training for: 579.42s\n",
            "Epoch 6, step (batch no.): 1175 -- acc: 1.00, loss 0.02 -- iter 18750/55000, training for: 580.80s\n",
            "Epoch 6, step (batch no.): 1178 -- acc: 0.99, loss 0.02 -- iter 19500/55000, training for: 582.19s\n",
            "Epoch 6, step (batch no.): 1181 -- acc: 0.99, loss 0.02 -- iter 20250/55000, training for: 583.60s\n",
            "Epoch 6, step (batch no.): 1184 -- acc: 0.99, loss 0.02 -- iter 21000/55000, training for: 584.97s\n",
            "Epoch 6, step (batch no.): 1187 -- acc: 0.99, loss 0.02 -- iter 21750/55000, training for: 586.35s\n",
            "Epoch 6, step (batch no.): 1190 -- acc: 0.99, loss 0.02 -- iter 22500/55000, training for: 587.77s\n",
            "Epoch 6, step (batch no.): 1193 -- acc: 0.99, loss 0.02 -- iter 23250/55000, training for: 589.15s\n",
            "Epoch 6, step (batch no.): 1196 -- acc: 0.99, loss 0.02 -- iter 24000/55000, training for: 590.52s\n",
            "Epoch 6, step (batch no.): 1199 -- acc: 0.99, loss 0.02 -- iter 24750/55000, training for: 591.90s\n",
            "Epoch 6, step (batch no.): 1202 -- acc: 1.00, loss 0.02 -- iter 25500/55000, training for: 593.29s\n",
            "Epoch 6, step (batch no.): 1205 -- acc: 1.00, loss 0.02 -- iter 26250/55000, training for: 594.69s\n",
            "Epoch 6, step (batch no.): 1208 -- acc: 1.00, loss 0.01 -- iter 27000/55000, training for: 596.07s\n",
            "Epoch 6, step (batch no.): 1211 -- acc: 1.00, loss 0.02 -- iter 27750/55000, training for: 597.47s\n",
            "Epoch 6, step (batch no.): 1214 -- acc: 1.00, loss 0.01 -- iter 28500/55000, training for: 598.88s\n",
            "Epoch 6, step (batch no.): 1217 -- acc: 1.00, loss 0.02 -- iter 29250/55000, training for: 600.32s\n",
            "Epoch 6, step (batch no.): 1220 -- acc: 1.00, loss 0.02 -- iter 30000/55000, training for: 601.74s\n",
            "Epoch 6, step (batch no.): 1223 -- acc: 0.99, loss 0.03 -- iter 30750/55000, training for: 603.15s\n",
            "Epoch 6, step (batch no.): 1226 -- acc: 0.99, loss 0.02 -- iter 31500/55000, training for: 604.58s\n",
            "Epoch 6, step (batch no.): 1229 -- acc: 0.99, loss 0.02 -- iter 32250/55000, training for: 606.02s\n",
            "Epoch 6, step (batch no.): 1232 -- acc: 0.99, loss 0.02 -- iter 33000/55000, training for: 607.39s\n",
            "Epoch 6, step (batch no.): 1235 -- acc: 0.99, loss 0.03 -- iter 33750/55000, training for: 608.80s\n",
            "Epoch 6, step (batch no.): 1238 -- acc: 0.99, loss 0.02 -- iter 34500/55000, training for: 610.22s\n",
            "Epoch 6, step (batch no.): 1241 -- acc: 0.99, loss 0.02 -- iter 35250/55000, training for: 611.58s\n",
            "Epoch 6, step (batch no.): 1244 -- acc: 0.99, loss 0.02 -- iter 36000/55000, training for: 612.95s\n",
            "Epoch 6, step (batch no.): 1247 -- acc: 0.99, loss 0.02 -- iter 36750/55000, training for: 614.36s\n",
            "Epoch 6, step (batch no.): 1250 -- acc: 0.99, loss 0.02 -- iter 37500/55000, training for: 615.74s\n",
            "Epoch 6, step (batch no.): 1253 -- acc: 0.99, loss 0.02 -- iter 38250/55000, training for: 617.11s\n",
            "Epoch 6, step (batch no.): 1256 -- acc: 0.99, loss 0.02 -- iter 39000/55000, training for: 618.49s\n",
            "Epoch 6, step (batch no.): 1259 -- acc: 0.99, loss 0.02 -- iter 39750/55000, training for: 619.89s\n",
            "Epoch 6, step (batch no.): 1262 -- acc: 0.99, loss 0.02 -- iter 40500/55000, training for: 621.29s\n",
            "Epoch 6, step (batch no.): 1265 -- acc: 0.99, loss 0.02 -- iter 41250/55000, training for: 622.68s\n",
            "Epoch 6, step (batch no.): 1268 -- acc: 0.99, loss 0.02 -- iter 42000/55000, training for: 624.10s\n",
            "Epoch 6, step (batch no.): 1271 -- acc: 0.99, loss 0.02 -- iter 42750/55000, training for: 625.46s\n",
            "Epoch 6, step (batch no.): 1274 -- acc: 0.99, loss 0.02 -- iter 43500/55000, training for: 626.84s\n",
            "Epoch 6, step (batch no.): 1277 -- acc: 0.99, loss 0.02 -- iter 44250/55000, training for: 628.23s\n",
            "Epoch 6, step (batch no.): 1280 -- acc: 0.99, loss 0.02 -- iter 45000/55000, training for: 629.62s\n",
            "Epoch 6, step (batch no.): 1283 -- acc: 0.99, loss 0.03 -- iter 45750/55000, training for: 631.00s\n",
            "Epoch 6, step (batch no.): 1286 -- acc: 0.99, loss 0.03 -- iter 46500/55000, training for: 632.41s\n",
            "Epoch 6, step (batch no.): 1289 -- acc: 0.99, loss 0.03 -- iter 47250/55000, training for: 633.80s\n",
            "Epoch 6, step (batch no.): 1292 -- acc: 0.99, loss 0.03 -- iter 48000/55000, training for: 635.20s\n",
            "Epoch 6, step (batch no.): 1295 -- acc: 0.91, loss 1.40 -- iter 48750/55000, training for: 636.59s\n",
            "Epoch 6, step (batch no.): 1298 -- acc: 0.93, loss 1.08 -- iter 49500/55000, training for: 638.02s\n",
            "Epoch 6, step (batch no.): 1301 -- acc: 0.95, loss 0.82 -- iter 50250/55000, training for: 639.40s\n",
            "Epoch 6, step (batch no.): 1304 -- acc: 0.96, loss 0.60 -- iter 51000/55000, training for: 640.78s\n",
            "Epoch 6, step (batch no.): 1307 -- acc: 0.97, loss 0.45 -- iter 51750/55000, training for: 642.14s\n",
            "Epoch 6, step (batch no.): 1310 -- acc: 0.97, loss 0.33 -- iter 52500/55000, training for: 643.50s\n",
            "Epoch 6, step (batch no.): 1313 -- acc: 0.98, loss 0.25 -- iter 53250/55000, training for: 644.89s\n",
            "Epoch 6, step (batch no.): 1316 -- acc: 0.98, loss 0.19 -- iter 54000/55000, training for: 646.29s\n",
            "Epoch 6, step (batch no.): 1319 -- acc: 0.98, loss 0.15 -- iter 54750/55000, training for: 647.69s\n",
            "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.13543\u001b[0m\u001b[0m | time: 107.204s\n",
            "| Adam | epoch: 006 | loss: 0.13543 - acc: 0.9843 | val_loss: 0.08633 - val_acc: 0.9822 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 6, step (batch no.): 1320 -- acc: 0.98, loss 0.14 -- iter 55000/55000, training for: 652.88s\n",
            "Epoch 7, step (batch no.): 1323 -- acc: 0.99, loss 0.11 -- iter 00750/55000, training for: 654.26s\n",
            "Epoch 7, step (batch no.): 1326 -- acc: 0.99, loss 0.09 -- iter 01500/55000, training for: 655.64s\n",
            "Epoch 7, step (batch no.): 1329 -- acc: 0.99, loss 0.08 -- iter 02250/55000, training for: 657.01s\n",
            "Epoch 7, step (batch no.): 1332 -- acc: 0.99, loss 0.07 -- iter 03000/55000, training for: 658.39s\n",
            "Epoch 7, step (batch no.): 1335 -- acc: 0.99, loss 0.06 -- iter 03750/55000, training for: 659.80s\n",
            "Epoch 7, step (batch no.): 1338 -- acc: 0.99, loss 0.05 -- iter 04500/55000, training for: 661.20s\n",
            "Epoch 7, step (batch no.): 1341 -- acc: 0.99, loss 0.05 -- iter 05250/55000, training for: 662.58s\n",
            "Epoch 7, step (batch no.): 1343 -- acc: 0.99, loss 0.04 -- iter 05750/55000, training for: 663.60s\n",
            "Epoch 7, step (batch no.): 1346 -- acc: 0.99, loss 0.04 -- iter 06500/55000, training for: 665.07s\n",
            "Epoch 7, step (batch no.): 1349 -- acc: 0.99, loss 0.04 -- iter 07250/55000, training for: 666.45s\n",
            "Epoch 7, step (batch no.): 1352 -- acc: 0.99, loss 0.04 -- iter 08000/55000, training for: 667.87s\n",
            "Epoch 7, step (batch no.): 1355 -- acc: 0.99, loss 0.04 -- iter 08750/55000, training for: 669.28s\n",
            "Epoch 7, step (batch no.): 1358 -- acc: 0.99, loss 0.04 -- iter 09500/55000, training for: 670.69s\n",
            "Epoch 7, step (batch no.): 1361 -- acc: 0.99, loss 0.04 -- iter 10250/55000, training for: 672.13s\n",
            "Epoch 7, step (batch no.): 1364 -- acc: 0.99, loss 0.03 -- iter 11000/55000, training for: 673.51s\n",
            "Epoch 7, step (batch no.): 1367 -- acc: 0.99, loss 0.03 -- iter 11750/55000, training for: 674.87s\n",
            "Epoch 7, step (batch no.): 1370 -- acc: 0.99, loss 0.02 -- iter 12500/55000, training for: 676.26s\n",
            "Epoch 7, step (batch no.): 1373 -- acc: 0.99, loss 0.03 -- iter 13250/55000, training for: 677.64s\n",
            "Epoch 7, step (batch no.): 1376 -- acc: 0.99, loss 0.02 -- iter 14000/55000, training for: 679.04s\n",
            "Epoch 7, step (batch no.): 1379 -- acc: 0.99, loss 0.02 -- iter 14750/55000, training for: 680.44s\n",
            "Epoch 7, step (batch no.): 1382 -- acc: 0.99, loss 0.02 -- iter 15500/55000, training for: 681.81s\n",
            "Epoch 7, step (batch no.): 1385 -- acc: 0.99, loss 0.03 -- iter 16250/55000, training for: 683.22s\n",
            "Epoch 7, step (batch no.): 1388 -- acc: 0.99, loss 0.03 -- iter 17000/55000, training for: 684.60s\n",
            "Epoch 7, step (batch no.): 1391 -- acc: 0.99, loss 0.03 -- iter 17750/55000, training for: 685.99s\n",
            "Epoch 7, step (batch no.): 1394 -- acc: 0.99, loss 0.03 -- iter 18500/55000, training for: 687.34s\n",
            "Epoch 7, step (batch no.): 1397 -- acc: 0.99, loss 0.04 -- iter 19250/55000, training for: 688.71s\n",
            "Epoch 7, step (batch no.): 1400 -- acc: 0.99, loss 0.03 -- iter 20000/55000, training for: 690.15s\n",
            "Epoch 7, step (batch no.): 1403 -- acc: 0.99, loss 0.03 -- iter 20750/55000, training for: 691.53s\n",
            "Epoch 7, step (batch no.): 1406 -- acc: 0.99, loss 0.03 -- iter 21500/55000, training for: 692.94s\n",
            "Epoch 7, step (batch no.): 1409 -- acc: 0.99, loss 0.03 -- iter 22250/55000, training for: 694.31s\n",
            "Epoch 7, step (batch no.): 1412 -- acc: 0.99, loss 0.03 -- iter 23000/55000, training for: 695.70s\n",
            "Epoch 7, step (batch no.): 1415 -- acc: 0.99, loss 0.02 -- iter 23750/55000, training for: 697.10s\n",
            "Epoch 7, step (batch no.): 1418 -- acc: 0.99, loss 0.02 -- iter 24500/55000, training for: 698.52s\n",
            "Epoch 7, step (batch no.): 1421 -- acc: 0.99, loss 0.02 -- iter 25250/55000, training for: 699.92s\n",
            "Epoch 7, step (batch no.): 1424 -- acc: 0.99, loss 0.02 -- iter 26000/55000, training for: 701.32s\n",
            "Epoch 7, step (batch no.): 1427 -- acc: 0.99, loss 0.02 -- iter 26750/55000, training for: 702.72s\n",
            "Epoch 7, step (batch no.): 1430 -- acc: 1.00, loss 0.02 -- iter 27500/55000, training for: 704.12s\n",
            "Epoch 7, step (batch no.): 1433 -- acc: 1.00, loss 0.02 -- iter 28250/55000, training for: 705.51s\n",
            "Epoch 7, step (batch no.): 1436 -- acc: 0.99, loss 0.02 -- iter 29000/55000, training for: 706.95s\n",
            "Epoch 7, step (batch no.): 1439 -- acc: 0.99, loss 0.02 -- iter 29750/55000, training for: 708.33s\n",
            "Epoch 7, step (batch no.): 1442 -- acc: 1.00, loss 0.01 -- iter 30500/55000, training for: 709.72s\n",
            "Epoch 7, step (batch no.): 1445 -- acc: 0.99, loss 0.02 -- iter 31250/55000, training for: 711.13s\n",
            "Epoch 7, step (batch no.): 1448 -- acc: 0.99, loss 0.02 -- iter 32000/55000, training for: 712.52s\n",
            "Epoch 7, step (batch no.): 1451 -- acc: 0.99, loss 0.02 -- iter 32750/55000, training for: 713.90s\n",
            "Epoch 7, step (batch no.): 1454 -- acc: 0.99, loss 0.03 -- iter 33500/55000, training for: 715.24s\n",
            "Epoch 7, step (batch no.): 1457 -- acc: 0.99, loss 0.03 -- iter 34250/55000, training for: 716.65s\n",
            "Epoch 7, step (batch no.): 1460 -- acc: 0.99, loss 0.03 -- iter 35000/55000, training for: 718.02s\n",
            "Epoch 7, step (batch no.): 1463 -- acc: 0.99, loss 0.03 -- iter 35750/55000, training for: 719.40s\n",
            "Epoch 7, step (batch no.): 1466 -- acc: 0.99, loss 0.02 -- iter 36500/55000, training for: 720.86s\n",
            "Epoch 7, step (batch no.): 1469 -- acc: 0.99, loss 0.02 -- iter 37250/55000, training for: 722.27s\n",
            "Epoch 7, step (batch no.): 1472 -- acc: 0.99, loss 0.02 -- iter 38000/55000, training for: 723.67s\n",
            "Epoch 7, step (batch no.): 1475 -- acc: 0.99, loss 0.02 -- iter 38750/55000, training for: 725.06s\n",
            "Epoch 7, step (batch no.): 1478 -- acc: 0.99, loss 0.03 -- iter 39500/55000, training for: 726.43s\n",
            "Epoch 7, step (batch no.): 1481 -- acc: 0.99, loss 0.03 -- iter 40250/55000, training for: 727.85s\n",
            "Epoch 7, step (batch no.): 1484 -- acc: 0.99, loss 0.03 -- iter 41000/55000, training for: 729.23s\n",
            "Epoch 7, step (batch no.): 1487 -- acc: 0.99, loss 0.02 -- iter 41750/55000, training for: 730.62s\n",
            "Epoch 7, step (batch no.): 1490 -- acc: 0.99, loss 0.03 -- iter 42500/55000, training for: 732.02s\n",
            "Epoch 7, step (batch no.): 1493 -- acc: 0.99, loss 0.03 -- iter 43250/55000, training for: 733.39s\n",
            "Epoch 7, step (batch no.): 1496 -- acc: 0.99, loss 0.03 -- iter 44000/55000, training for: 734.76s\n",
            "Epoch 7, step (batch no.): 1499 -- acc: 0.99, loss 0.03 -- iter 44750/55000, training for: 736.18s\n",
            "Epoch 7, step (batch no.): 1502 -- acc: 0.99, loss 0.03 -- iter 45500/55000, training for: 737.59s\n",
            "Epoch 7, step (batch no.): 1505 -- acc: 0.99, loss 0.03 -- iter 46250/55000, training for: 738.99s\n",
            "Epoch 7, step (batch no.): 1508 -- acc: 0.99, loss 0.03 -- iter 47000/55000, training for: 740.35s\n",
            "Epoch 7, step (batch no.): 1511 -- acc: 0.99, loss 0.03 -- iter 47750/55000, training for: 741.71s\n",
            "Epoch 7, step (batch no.): 1514 -- acc: 0.99, loss 0.03 -- iter 48500/55000, training for: 743.07s\n",
            "Epoch 7, step (batch no.): 1517 -- acc: 0.92, loss 1.22 -- iter 49250/55000, training for: 744.45s\n",
            "Epoch 7, step (batch no.): 1520 -- acc: 0.93, loss 0.94 -- iter 50000/55000, training for: 745.82s\n",
            "Epoch 7, step (batch no.): 1523 -- acc: 0.95, loss 0.71 -- iter 50750/55000, training for: 747.21s\n",
            "Epoch 7, step (batch no.): 1526 -- acc: 0.96, loss 0.53 -- iter 51500/55000, training for: 748.62s\n",
            "Epoch 7, step (batch no.): 1529 -- acc: 0.97, loss 0.39 -- iter 52250/55000, training for: 750.03s\n",
            "Epoch 7, step (batch no.): 1532 -- acc: 0.98, loss 0.29 -- iter 53000/55000, training for: 751.38s\n",
            "Epoch 7, step (batch no.): 1535 -- acc: 0.98, loss 0.22 -- iter 53750/55000, training for: 752.75s\n",
            "Epoch 7, step (batch no.): 1538 -- acc: 0.98, loss 0.17 -- iter 54500/55000, training for: 754.14s\n",
            "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.14362\u001b[0m\u001b[0m | time: 106.859s\n",
            "| Adam | epoch: 007 | loss: 0.14362 - acc: 0.9839 | val_loss: 0.07079 - val_acc: 0.9841 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 7, step (batch no.): 1540 -- acc: 0.98, loss 0.14 -- iter 55000/55000, training for: 759.76s\n",
            "Epoch 8, step (batch no.): 1543 -- acc: 0.99, loss 0.11 -- iter 00750/55000, training for: 761.20s\n",
            "Epoch 8, step (batch no.): 1546 -- acc: 0.99, loss 0.09 -- iter 01500/55000, training for: 762.59s\n",
            "Epoch 8, step (batch no.): 1549 -- acc: 0.99, loss 0.07 -- iter 02250/55000, training for: 764.00s\n",
            "Epoch 8, step (batch no.): 1552 -- acc: 0.99, loss 0.06 -- iter 03000/55000, training for: 765.42s\n",
            "Epoch 8, step (batch no.): 1555 -- acc: 0.99, loss 0.06 -- iter 03750/55000, training for: 766.83s\n",
            "Epoch 8, step (batch no.): 1558 -- acc: 0.99, loss 0.05 -- iter 04500/55000, training for: 768.23s\n",
            "Epoch 8, step (batch no.): 1561 -- acc: 0.99, loss 0.04 -- iter 05250/55000, training for: 769.68s\n",
            "Epoch 8, step (batch no.): 1564 -- acc: 0.99, loss 0.04 -- iter 06000/55000, training for: 771.10s\n",
            "Epoch 8, step (batch no.): 1567 -- acc: 0.99, loss 0.04 -- iter 06750/55000, training for: 772.51s\n",
            "Epoch 8, step (batch no.): 1570 -- acc: 0.99, loss 0.04 -- iter 07500/55000, training for: 773.88s\n",
            "Epoch 8, step (batch no.): 1573 -- acc: 0.99, loss 0.04 -- iter 08250/55000, training for: 775.27s\n",
            "Epoch 8, step (batch no.): 1576 -- acc: 0.99, loss 0.04 -- iter 09000/55000, training for: 776.65s\n",
            "Epoch 8, step (batch no.): 1579 -- acc: 0.99, loss 0.04 -- iter 09750/55000, training for: 778.03s\n",
            "Epoch 8, step (batch no.): 1582 -- acc: 0.99, loss 0.03 -- iter 10500/55000, training for: 779.48s\n",
            "Epoch 8, step (batch no.): 1585 -- acc: 0.99, loss 0.03 -- iter 11250/55000, training for: 780.89s\n",
            "Epoch 8, step (batch no.): 1588 -- acc: 0.99, loss 0.04 -- iter 12000/55000, training for: 782.29s\n",
            "Epoch 8, step (batch no.): 1591 -- acc: 0.99, loss 0.04 -- iter 12750/55000, training for: 783.71s\n",
            "Epoch 8, step (batch no.): 1594 -- acc: 0.99, loss 0.03 -- iter 13500/55000, training for: 785.13s\n",
            "Epoch 8, step (batch no.): 1597 -- acc: 0.99, loss 0.03 -- iter 14250/55000, training for: 786.54s\n",
            "Epoch 8, step (batch no.): 1600 -- acc: 0.99, loss 0.03 -- iter 15000/55000, training for: 787.94s\n",
            "Epoch 8, step (batch no.): 1603 -- acc: 0.99, loss 0.03 -- iter 15750/55000, training for: 789.34s\n",
            "Epoch 8, step (batch no.): 1606 -- acc: 0.99, loss 0.03 -- iter 16500/55000, training for: 790.72s\n",
            "Epoch 8, step (batch no.): 1609 -- acc: 0.99, loss 0.02 -- iter 17250/55000, training for: 792.10s\n",
            "Epoch 8, step (batch no.): 1612 -- acc: 0.99, loss 0.02 -- iter 18000/55000, training for: 793.48s\n",
            "Epoch 8, step (batch no.): 1615 -- acc: 0.99, loss 0.02 -- iter 18750/55000, training for: 794.86s\n",
            "Epoch 8, step (batch no.): 1618 -- acc: 0.99, loss 0.02 -- iter 19500/55000, training for: 796.24s\n",
            "Epoch 8, step (batch no.): 1621 -- acc: 0.99, loss 0.02 -- iter 20250/55000, training for: 797.63s\n",
            "Epoch 8, step (batch no.): 1624 -- acc: 0.99, loss 0.02 -- iter 21000/55000, training for: 799.02s\n",
            "Epoch 8, step (batch no.): 1627 -- acc: 0.99, loss 0.02 -- iter 21750/55000, training for: 800.40s\n",
            "Epoch 8, step (batch no.): 1630 -- acc: 0.99, loss 0.02 -- iter 22500/55000, training for: 801.77s\n",
            "Epoch 8, step (batch no.): 1633 -- acc: 0.99, loss 0.02 -- iter 23250/55000, training for: 803.15s\n",
            "Epoch 8, step (batch no.): 1636 -- acc: 0.99, loss 0.02 -- iter 24000/55000, training for: 804.54s\n",
            "Epoch 8, step (batch no.): 1639 -- acc: 0.99, loss 0.02 -- iter 24750/55000, training for: 805.92s\n",
            "Epoch 8, step (batch no.): 1642 -- acc: 1.00, loss 0.02 -- iter 25500/55000, training for: 807.28s\n",
            "Epoch 8, step (batch no.): 1645 -- acc: 1.00, loss 0.02 -- iter 26250/55000, training for: 808.64s\n",
            "Epoch 8, step (batch no.): 1648 -- acc: 1.00, loss 0.02 -- iter 27000/55000, training for: 810.01s\n",
            "Epoch 8, step (batch no.): 1651 -- acc: 0.99, loss 0.02 -- iter 27750/55000, training for: 811.41s\n",
            "Epoch 8, step (batch no.): 1654 -- acc: 0.99, loss 0.03 -- iter 28500/55000, training for: 812.81s\n",
            "Epoch 8, step (batch no.): 1657 -- acc: 0.99, loss 0.02 -- iter 29250/55000, training for: 814.16s\n",
            "Epoch 8, step (batch no.): 1660 -- acc: 0.99, loss 0.03 -- iter 30000/55000, training for: 815.54s\n",
            "Epoch 8, step (batch no.): 1663 -- acc: 0.99, loss 0.03 -- iter 30750/55000, training for: 816.92s\n",
            "Epoch 8, step (batch no.): 1666 -- acc: 0.99, loss 0.02 -- iter 31500/55000, training for: 818.31s\n",
            "Epoch 8, step (batch no.): 1669 -- acc: 0.99, loss 0.02 -- iter 32250/55000, training for: 819.69s\n",
            "Epoch 8, step (batch no.): 1672 -- acc: 0.99, loss 0.02 -- iter 33000/55000, training for: 821.08s\n",
            "Epoch 8, step (batch no.): 1675 -- acc: 0.99, loss 0.03 -- iter 33750/55000, training for: 822.45s\n",
            "Epoch 8, step (batch no.): 1677 -- acc: 0.99, loss 0.02 -- iter 34250/55000, training for: 823.47s\n",
            "Epoch 8, step (batch no.): 1680 -- acc: 0.99, loss 0.03 -- iter 35000/55000, training for: 824.85s\n",
            "Epoch 8, step (batch no.): 1683 -- acc: 0.99, loss 0.03 -- iter 35750/55000, training for: 826.23s\n",
            "Epoch 8, step (batch no.): 1686 -- acc: 0.99, loss 0.03 -- iter 36500/55000, training for: 827.60s\n",
            "Epoch 8, step (batch no.): 1689 -- acc: 0.99, loss 0.03 -- iter 37250/55000, training for: 829.01s\n",
            "Epoch 8, step (batch no.): 1692 -- acc: 0.99, loss 0.03 -- iter 38000/55000, training for: 830.42s\n",
            "Epoch 8, step (batch no.): 1695 -- acc: 0.99, loss 0.03 -- iter 38750/55000, training for: 831.81s\n",
            "Epoch 8, step (batch no.): 1698 -- acc: 0.99, loss 0.03 -- iter 39500/55000, training for: 833.18s\n",
            "Epoch 8, step (batch no.): 1701 -- acc: 0.99, loss 0.03 -- iter 40250/55000, training for: 834.55s\n",
            "Epoch 8, step (batch no.): 1704 -- acc: 0.99, loss 0.02 -- iter 41000/55000, training for: 835.98s\n",
            "Epoch 8, step (batch no.): 1707 -- acc: 0.99, loss 0.02 -- iter 41750/55000, training for: 837.34s\n",
            "Epoch 8, step (batch no.): 1710 -- acc: 0.99, loss 0.02 -- iter 42500/55000, training for: 838.70s\n",
            "Epoch 8, step (batch no.): 1713 -- acc: 0.99, loss 0.02 -- iter 43250/55000, training for: 840.08s\n",
            "Epoch 8, step (batch no.): 1716 -- acc: 0.99, loss 0.02 -- iter 44000/55000, training for: 841.46s\n",
            "Epoch 8, step (batch no.): 1719 -- acc: 0.99, loss 0.02 -- iter 44750/55000, training for: 842.82s\n",
            "Epoch 8, step (batch no.): 1722 -- acc: 0.99, loss 0.02 -- iter 45500/55000, training for: 844.18s\n",
            "Epoch 8, step (batch no.): 1725 -- acc: 0.99, loss 0.02 -- iter 46250/55000, training for: 845.54s\n",
            "Epoch 8, step (batch no.): 1728 -- acc: 0.99, loss 0.02 -- iter 47000/55000, training for: 846.89s\n",
            "Epoch 8, step (batch no.): 1731 -- acc: 1.00, loss 0.02 -- iter 47750/55000, training for: 848.25s\n",
            "Epoch 8, step (batch no.): 1734 -- acc: 0.99, loss 0.02 -- iter 48500/55000, training for: 849.62s\n",
            "Epoch 8, step (batch no.): 1737 -- acc: 0.99, loss 0.02 -- iter 49250/55000, training for: 851.00s\n",
            "Epoch 8, step (batch no.): 1740 -- acc: 0.99, loss 0.02 -- iter 50000/55000, training for: 852.40s\n",
            "Epoch 8, step (batch no.): 1743 -- acc: 0.99, loss 0.02 -- iter 50750/55000, training for: 853.76s\n",
            "Epoch 8, step (batch no.): 1746 -- acc: 0.99, loss 0.02 -- iter 51500/55000, training for: 855.10s\n",
            "Epoch 8, step (batch no.): 1749 -- acc: 0.99, loss 0.02 -- iter 52250/55000, training for: 856.49s\n",
            "Epoch 8, step (batch no.): 1752 -- acc: 0.99, loss 0.02 -- iter 53000/55000, training for: 857.86s\n",
            "Epoch 8, step (batch no.): 1755 -- acc: 0.99, loss 0.02 -- iter 53750/55000, training for: 859.19s\n",
            "Epoch 8, step (batch no.): 1758 -- acc: 0.99, loss 0.01 -- iter 54500/55000, training for: 860.54s\n",
            "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.01490\u001b[0m\u001b[0m | time: 106.408s\n",
            "| Adam | epoch: 008 | loss: 0.01490 - acc: 0.9946 | val_loss: 0.05104 - val_acc: 0.9870 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 8, step (batch no.): 1760 -- acc: 0.99, loss 0.01 -- iter 55000/55000, training for: 866.20s\n",
            "Epoch 9, step (batch no.): 1763 -- acc: 0.99, loss 0.01 -- iter 00750/55000, training for: 867.59s\n",
            "Epoch 9, step (batch no.): 1766 -- acc: 0.99, loss 0.01 -- iter 01500/55000, training for: 869.01s\n",
            "Epoch 9, step (batch no.): 1769 -- acc: 1.00, loss 0.01 -- iter 02250/55000, training for: 870.37s\n",
            "Epoch 9, step (batch no.): 1772 -- acc: 1.00, loss 0.01 -- iter 03000/55000, training for: 871.75s\n",
            "Epoch 9, step (batch no.): 1775 -- acc: 0.99, loss 0.02 -- iter 03750/55000, training for: 873.13s\n",
            "Epoch 9, step (batch no.): 1778 -- acc: 1.00, loss 0.02 -- iter 04500/55000, training for: 874.49s\n",
            "Epoch 9, step (batch no.): 1781 -- acc: 0.99, loss 0.02 -- iter 05250/55000, training for: 875.86s\n",
            "Epoch 9, step (batch no.): 1784 -- acc: 1.00, loss 0.01 -- iter 06000/55000, training for: 877.20s\n",
            "Epoch 9, step (batch no.): 1787 -- acc: 1.00, loss 0.01 -- iter 06750/55000, training for: 878.52s\n",
            "Epoch 9, step (batch no.): 1790 -- acc: 0.99, loss 0.02 -- iter 07500/55000, training for: 879.85s\n",
            "Epoch 9, step (batch no.): 1793 -- acc: 1.00, loss 0.01 -- iter 08250/55000, training for: 881.20s\n",
            "Epoch 9, step (batch no.): 1796 -- acc: 1.00, loss 0.02 -- iter 09000/55000, training for: 882.55s\n",
            "Epoch 9, step (batch no.): 1799 -- acc: 1.00, loss 0.02 -- iter 09750/55000, training for: 883.89s\n",
            "Epoch 9, step (batch no.): 1802 -- acc: 0.99, loss 0.02 -- iter 10500/55000, training for: 885.22s\n",
            "Epoch 9, step (batch no.): 1805 -- acc: 1.00, loss 0.01 -- iter 11250/55000, training for: 886.57s\n",
            "Epoch 9, step (batch no.): 1808 -- acc: 1.00, loss 0.02 -- iter 12000/55000, training for: 887.93s\n",
            "Epoch 9, step (batch no.): 1811 -- acc: 1.00, loss 0.01 -- iter 12750/55000, training for: 889.27s\n",
            "Epoch 9, step (batch no.): 1814 -- acc: 1.00, loss 0.01 -- iter 13500/55000, training for: 890.63s\n",
            "Epoch 9, step (batch no.): 1817 -- acc: 1.00, loss 0.01 -- iter 14250/55000, training for: 892.00s\n",
            "Epoch 9, step (batch no.): 1820 -- acc: 1.00, loss 0.01 -- iter 15000/55000, training for: 893.36s\n",
            "Epoch 9, step (batch no.): 1823 -- acc: 1.00, loss 0.01 -- iter 15750/55000, training for: 894.71s\n",
            "Epoch 9, step (batch no.): 1826 -- acc: 1.00, loss 0.01 -- iter 16500/55000, training for: 896.12s\n",
            "Epoch 9, step (batch no.): 1829 -- acc: 1.00, loss 0.01 -- iter 17250/55000, training for: 897.49s\n",
            "Epoch 9, step (batch no.): 1832 -- acc: 0.99, loss 0.02 -- iter 18000/55000, training for: 898.87s\n",
            "Epoch 9, step (batch no.): 1835 -- acc: 0.99, loss 0.01 -- iter 18750/55000, training for: 900.31s\n",
            "Epoch 9, step (batch no.): 1838 -- acc: 1.00, loss 0.01 -- iter 19500/55000, training for: 901.72s\n",
            "Epoch 9, step (batch no.): 1841 -- acc: 1.00, loss 0.01 -- iter 20250/55000, training for: 903.14s\n",
            "Epoch 9, step (batch no.): 1844 -- acc: 1.00, loss 0.01 -- iter 21000/55000, training for: 904.54s\n",
            "Epoch 9, step (batch no.): 1847 -- acc: 0.99, loss 0.01 -- iter 21750/55000, training for: 905.91s\n",
            "Epoch 9, step (batch no.): 1850 -- acc: 0.99, loss 0.02 -- iter 22500/55000, training for: 907.27s\n",
            "Epoch 9, step (batch no.): 1853 -- acc: 0.99, loss 0.02 -- iter 23250/55000, training for: 908.63s\n",
            "Epoch 9, step (batch no.): 1856 -- acc: 0.99, loss 0.02 -- iter 24000/55000, training for: 909.99s\n",
            "Epoch 9, step (batch no.): 1859 -- acc: 0.99, loss 0.02 -- iter 24750/55000, training for: 911.34s\n",
            "Epoch 9, step (batch no.): 1862 -- acc: 0.99, loss 0.02 -- iter 25500/55000, training for: 912.71s\n",
            "Epoch 9, step (batch no.): 1865 -- acc: 0.99, loss 0.02 -- iter 26250/55000, training for: 914.07s\n",
            "Epoch 9, step (batch no.): 1868 -- acc: 0.99, loss 0.02 -- iter 27000/55000, training for: 915.42s\n",
            "Epoch 9, step (batch no.): 1871 -- acc: 0.99, loss 0.02 -- iter 27750/55000, training for: 916.77s\n",
            "Epoch 9, step (batch no.): 1874 -- acc: 0.99, loss 0.02 -- iter 28500/55000, training for: 918.13s\n",
            "Epoch 9, step (batch no.): 1877 -- acc: 1.00, loss 0.02 -- iter 29250/55000, training for: 919.50s\n",
            "Epoch 9, step (batch no.): 1880 -- acc: 0.99, loss 0.02 -- iter 30000/55000, training for: 920.87s\n",
            "Epoch 9, step (batch no.): 1883 -- acc: 1.00, loss 0.01 -- iter 30750/55000, training for: 922.22s\n",
            "Epoch 9, step (batch no.): 1886 -- acc: 1.00, loss 0.01 -- iter 31500/55000, training for: 923.57s\n",
            "Epoch 9, step (batch no.): 1889 -- acc: 1.00, loss 0.01 -- iter 32250/55000, training for: 924.94s\n",
            "Epoch 9, step (batch no.): 1892 -- acc: 0.99, loss 0.02 -- iter 33000/55000, training for: 926.31s\n",
            "Epoch 9, step (batch no.): 1895 -- acc: 0.99, loss 0.02 -- iter 33750/55000, training for: 927.68s\n",
            "Epoch 9, step (batch no.): 1898 -- acc: 1.00, loss 0.02 -- iter 34500/55000, training for: 929.07s\n",
            "Epoch 9, step (batch no.): 1901 -- acc: 1.00, loss 0.02 -- iter 35250/55000, training for: 930.42s\n",
            "Epoch 9, step (batch no.): 1904 -- acc: 1.00, loss 0.01 -- iter 36000/55000, training for: 931.79s\n",
            "Epoch 9, step (batch no.): 1907 -- acc: 1.00, loss 0.02 -- iter 36750/55000, training for: 933.23s\n",
            "Epoch 9, step (batch no.): 1910 -- acc: 1.00, loss 0.02 -- iter 37500/55000, training for: 934.65s\n",
            "Epoch 9, step (batch no.): 1913 -- acc: 0.99, loss 0.02 -- iter 38250/55000, training for: 936.03s\n",
            "Epoch 9, step (batch no.): 1916 -- acc: 0.99, loss 0.02 -- iter 39000/55000, training for: 937.38s\n",
            "Epoch 9, step (batch no.): 1919 -- acc: 0.99, loss 0.02 -- iter 39750/55000, training for: 938.74s\n",
            "Epoch 9, step (batch no.): 1922 -- acc: 0.99, loss 0.02 -- iter 40500/55000, training for: 940.11s\n",
            "Epoch 9, step (batch no.): 1925 -- acc: 0.99, loss 0.03 -- iter 41250/55000, training for: 941.46s\n",
            "Epoch 9, step (batch no.): 1928 -- acc: 0.99, loss 0.03 -- iter 42000/55000, training for: 942.81s\n",
            "Epoch 9, step (batch no.): 1931 -- acc: 0.99, loss 0.03 -- iter 42750/55000, training for: 944.20s\n",
            "Epoch 9, step (batch no.): 1934 -- acc: 0.99, loss 0.03 -- iter 43500/55000, training for: 945.57s\n",
            "Epoch 9, step (batch no.): 1937 -- acc: 0.99, loss 0.03 -- iter 44250/55000, training for: 946.94s\n",
            "Epoch 9, step (batch no.): 1940 -- acc: 0.99, loss 0.03 -- iter 45000/55000, training for: 948.32s\n",
            "Epoch 9, step (batch no.): 1943 -- acc: 0.99, loss 0.03 -- iter 45750/55000, training for: 949.69s\n",
            "Epoch 9, step (batch no.): 1946 -- acc: 0.99, loss 0.02 -- iter 46500/55000, training for: 951.09s\n",
            "Epoch 9, step (batch no.): 1949 -- acc: 0.99, loss 0.02 -- iter 47250/55000, training for: 952.46s\n",
            "Epoch 9, step (batch no.): 1952 -- acc: 0.99, loss 0.02 -- iter 48000/55000, training for: 953.83s\n",
            "Epoch 9, step (batch no.): 1955 -- acc: 1.00, loss 0.02 -- iter 48750/55000, training for: 955.20s\n",
            "Epoch 9, step (batch no.): 1958 -- acc: 0.92, loss 1.27 -- iter 49500/55000, training for: 956.55s\n",
            "Epoch 9, step (batch no.): 1961 -- acc: 0.94, loss 0.97 -- iter 50250/55000, training for: 957.91s\n",
            "Epoch 9, step (batch no.): 1964 -- acc: 0.95, loss 0.75 -- iter 51000/55000, training for: 959.27s\n",
            "Epoch 9, step (batch no.): 1967 -- acc: 0.96, loss 0.56 -- iter 51750/55000, training for: 960.63s\n",
            "Epoch 9, step (batch no.): 1970 -- acc: 0.97, loss 0.42 -- iter 52500/55000, training for: 962.03s\n",
            "Epoch 9, step (batch no.): 1973 -- acc: 0.98, loss 0.31 -- iter 53250/55000, training for: 963.40s\n",
            "Epoch 9, step (batch no.): 1976 -- acc: 0.98, loss 0.23 -- iter 54000/55000, training for: 964.82s\n",
            "Epoch 9, step (batch no.): 1979 -- acc: 0.98, loss 0.17 -- iter 54750/55000, training for: 966.18s\n",
            "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.15711\u001b[0m\u001b[0m | time: 105.146s\n",
            "| Adam | epoch: 009 | loss: 0.15711 - acc: 0.9858 | val_loss: 0.05976 - val_acc: 0.9858 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 9, step (batch no.): 1980 -- acc: 0.99, loss 0.16 -- iter 55000/55000, training for: 971.38s\n",
            "Epoch 10, step (batch no.): 1983 -- acc: 0.99, loss 0.12 -- iter 00750/55000, training for: 972.74s\n",
            "Epoch 10, step (batch no.): 1986 -- acc: 0.99, loss 0.09 -- iter 01500/55000, training for: 974.10s\n",
            "Epoch 10, step (batch no.): 1989 -- acc: 0.99, loss 0.07 -- iter 02250/55000, training for: 975.49s\n",
            "Epoch 10, step (batch no.): 1992 -- acc: 0.99, loss 0.06 -- iter 03000/55000, training for: 976.84s\n",
            "Epoch 10, step (batch no.): 1995 -- acc: 0.99, loss 0.04 -- iter 03750/55000, training for: 978.20s\n",
            "Epoch 10, step (batch no.): 1998 -- acc: 0.99, loss 0.03 -- iter 04500/55000, training for: 979.54s\n",
            "Epoch 10, step (batch no.): 2001 -- acc: 0.99, loss 0.03 -- iter 05250/55000, training for: 980.90s\n",
            "Epoch 10, step (batch no.): 2004 -- acc: 1.00, loss 0.02 -- iter 06000/55000, training for: 982.25s\n",
            "Epoch 10, step (batch no.): 2007 -- acc: 0.99, loss 0.03 -- iter 06750/55000, training for: 983.62s\n",
            "Epoch 10, step (batch no.): 2010 -- acc: 0.99, loss 0.02 -- iter 07500/55000, training for: 985.00s\n",
            "Epoch 10, step (batch no.): 2013 -- acc: 0.99, loss 0.02 -- iter 08250/55000, training for: 986.36s\n",
            "Epoch 10, step (batch no.): 2016 -- acc: 0.99, loss 0.02 -- iter 09000/55000, training for: 987.72s\n",
            "Epoch 10, step (batch no.): 2019 -- acc: 0.99, loss 0.03 -- iter 09750/55000, training for: 989.09s\n",
            "Epoch 10, step (batch no.): 2022 -- acc: 0.99, loss 0.02 -- iter 10500/55000, training for: 990.43s\n",
            "Epoch 10, step (batch no.): 2025 -- acc: 0.99, loss 0.02 -- iter 11250/55000, training for: 991.77s\n",
            "Epoch 10, step (batch no.): 2028 -- acc: 0.99, loss 0.03 -- iter 12000/55000, training for: 993.19s\n",
            "Epoch 10, step (batch no.): 2031 -- acc: 0.99, loss 0.02 -- iter 12750/55000, training for: 994.56s\n",
            "Epoch 10, step (batch no.): 2034 -- acc: 0.99, loss 0.03 -- iter 13500/55000, training for: 995.96s\n",
            "Epoch 10, step (batch no.): 2037 -- acc: 0.99, loss 0.02 -- iter 14250/55000, training for: 997.33s\n",
            "Epoch 10, step (batch no.): 2040 -- acc: 0.99, loss 0.03 -- iter 15000/55000, training for: 998.69s\n",
            "Epoch 10, step (batch no.): 2043 -- acc: 0.99, loss 0.03 -- iter 15750/55000, training for: 1000.06s\n",
            "Epoch 10, step (batch no.): 2046 -- acc: 0.99, loss 0.03 -- iter 16500/55000, training for: 1001.49s\n",
            "Epoch 10, step (batch no.): 2049 -- acc: 0.99, loss 0.03 -- iter 17250/55000, training for: 1002.88s\n",
            "Epoch 10, step (batch no.): 2052 -- acc: 0.99, loss 0.03 -- iter 18000/55000, training for: 1004.25s\n",
            "Epoch 10, step (batch no.): 2055 -- acc: 0.99, loss 0.02 -- iter 18750/55000, training for: 1005.60s\n",
            "Epoch 10, step (batch no.): 2058 -- acc: 0.99, loss 0.03 -- iter 19500/55000, training for: 1006.95s\n",
            "Epoch 10, step (batch no.): 2061 -- acc: 0.99, loss 0.03 -- iter 20250/55000, training for: 1008.31s\n",
            "Epoch 10, step (batch no.): 2064 -- acc: 0.99, loss 0.03 -- iter 21000/55000, training for: 1009.66s\n",
            "Epoch 10, step (batch no.): 2067 -- acc: 0.99, loss 0.03 -- iter 21750/55000, training for: 1011.04s\n",
            "Epoch 10, step (batch no.): 2070 -- acc: 0.99, loss 0.02 -- iter 22500/55000, training for: 1012.41s\n",
            "Epoch 10, step (batch no.): 2073 -- acc: 0.99, loss 0.02 -- iter 23250/55000, training for: 1013.79s\n",
            "Epoch 10, step (batch no.): 2076 -- acc: 0.99, loss 0.03 -- iter 24000/55000, training for: 1015.16s\n",
            "Epoch 10, step (batch no.): 2079 -- acc: 0.99, loss 0.02 -- iter 24750/55000, training for: 1016.52s\n",
            "Epoch 10, step (batch no.): 2082 -- acc: 0.99, loss 0.02 -- iter 25500/55000, training for: 1017.87s\n",
            "Epoch 10, step (batch no.): 2085 -- acc: 0.99, loss 0.02 -- iter 26250/55000, training for: 1019.26s\n",
            "Epoch 10, step (batch no.): 2088 -- acc: 0.99, loss 0.02 -- iter 27000/55000, training for: 1020.68s\n",
            "Epoch 10, step (batch no.): 2091 -- acc: 0.99, loss 0.02 -- iter 27750/55000, training for: 1022.03s\n",
            "Epoch 10, step (batch no.): 2094 -- acc: 0.99, loss 0.02 -- iter 28500/55000, training for: 1023.44s\n",
            "Epoch 10, step (batch no.): 2097 -- acc: 0.99, loss 0.02 -- iter 29250/55000, training for: 1024.84s\n",
            "Epoch 10, step (batch no.): 2100 -- acc: 0.99, loss 0.02 -- iter 30000/55000, training for: 1026.25s\n",
            "Epoch 10, step (batch no.): 2103 -- acc: 0.99, loss 0.02 -- iter 30750/55000, training for: 1027.65s\n",
            "Epoch 10, step (batch no.): 2106 -- acc: 0.99, loss 0.02 -- iter 31500/55000, training for: 1029.02s\n",
            "Epoch 10, step (batch no.): 2109 -- acc: 0.99, loss 0.02 -- iter 32250/55000, training for: 1030.41s\n",
            "Epoch 10, step (batch no.): 2112 -- acc: 0.99, loss 0.02 -- iter 33000/55000, training for: 1031.78s\n",
            "Epoch 10, step (batch no.): 2115 -- acc: 0.99, loss 0.02 -- iter 33750/55000, training for: 1033.16s\n",
            "Epoch 10, step (batch no.): 2118 -- acc: 0.99, loss 0.03 -- iter 34500/55000, training for: 1034.58s\n",
            "Epoch 10, step (batch no.): 2121 -- acc: 0.99, loss 0.02 -- iter 35250/55000, training for: 1035.94s\n",
            "Epoch 10, step (batch no.): 2124 -- acc: 0.99, loss 0.03 -- iter 36000/55000, training for: 1037.30s\n",
            "Epoch 10, step (batch no.): 2127 -- acc: 0.99, loss 0.03 -- iter 36750/55000, training for: 1038.65s\n",
            "Epoch 10, step (batch no.): 2130 -- acc: 0.99, loss 0.03 -- iter 37500/55000, training for: 1039.99s\n",
            "Epoch 10, step (batch no.): 2133 -- acc: 0.99, loss 0.03 -- iter 38250/55000, training for: 1041.35s\n",
            "Epoch 10, step (batch no.): 2136 -- acc: 0.99, loss 0.02 -- iter 39000/55000, training for: 1042.72s\n",
            "Epoch 10, step (batch no.): 2139 -- acc: 0.99, loss 0.02 -- iter 39750/55000, training for: 1044.06s\n",
            "Epoch 10, step (batch no.): 2142 -- acc: 0.99, loss 0.03 -- iter 40500/55000, training for: 1045.40s\n",
            "Epoch 10, step (batch no.): 2145 -- acc: 0.99, loss 0.02 -- iter 41250/55000, training for: 1046.77s\n",
            "Epoch 10, step (batch no.): 2148 -- acc: 0.99, loss 0.02 -- iter 42000/55000, training for: 1048.10s\n",
            "Epoch 10, step (batch no.): 2151 -- acc: 0.99, loss 0.02 -- iter 42750/55000, training for: 1049.45s\n",
            "Epoch 10, step (batch no.): 2154 -- acc: 0.99, loss 0.02 -- iter 43500/55000, training for: 1050.81s\n",
            "Epoch 10, step (batch no.): 2157 -- acc: 0.99, loss 0.02 -- iter 44250/55000, training for: 1052.15s\n",
            "Epoch 10, step (batch no.): 2160 -- acc: 0.99, loss 0.02 -- iter 45000/55000, training for: 1053.51s\n",
            "Epoch 10, step (batch no.): 2163 -- acc: 0.99, loss 0.02 -- iter 45750/55000, training for: 1054.86s\n",
            "Epoch 10, step (batch no.): 2166 -- acc: 0.99, loss 0.02 -- iter 46500/55000, training for: 1056.20s\n",
            "Epoch 10, step (batch no.): 2169 -- acc: 0.99, loss 0.03 -- iter 47250/55000, training for: 1057.56s\n",
            "Epoch 10, step (batch no.): 2172 -- acc: 0.99, loss 0.02 -- iter 48000/55000, training for: 1058.89s\n",
            "Epoch 10, step (batch no.): 2175 -- acc: 0.99, loss 0.03 -- iter 48750/55000, training for: 1060.24s\n",
            "Epoch 10, step (batch no.): 2178 -- acc: 0.90, loss 1.62 -- iter 49500/55000, training for: 1061.61s\n",
            "Epoch 10, step (batch no.): 2181 -- acc: 0.92, loss 1.22 -- iter 50250/55000, training for: 1062.96s\n",
            "Epoch 10, step (batch no.): 2184 -- acc: 0.93, loss 0.93 -- iter 51000/55000, training for: 1064.31s\n",
            "Epoch 10, step (batch no.): 2187 -- acc: 0.95, loss 0.71 -- iter 51750/55000, training for: 1065.67s\n",
            "Epoch 10, step (batch no.): 2190 -- acc: 0.96, loss 0.53 -- iter 52500/55000, training for: 1067.07s\n",
            "Epoch 10, step (batch no.): 2193 -- acc: 0.97, loss 0.40 -- iter 53250/55000, training for: 1068.42s\n",
            "Epoch 10, step (batch no.): 2196 -- acc: 0.97, loss 0.30 -- iter 54000/55000, training for: 1069.75s\n",
            "Epoch 10, step (batch no.): 2199 -- acc: 0.98, loss 0.23 -- iter 54750/55000, training for: 1071.12s\n",
            "Training Step: 2200  | total loss: \u001b[1m\u001b[32m0.20726\u001b[0m\u001b[0m | time: 104.789s\n",
            "| Adam | epoch: 010 | loss: 0.20726 - acc: 0.9789 | val_loss: 0.06921 - val_acc: 0.9796 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 10, step (batch no.): 2200 -- acc: 0.98, loss 0.21 -- iter 55000/55000, training for: 1076.20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "LOw4rTcSetRD",
        "outputId": "f5305795-bee4-4775-e49b-b61bc9415fa1"
      },
      "source": [
        "#Validation accuracy\n",
        "vall_acc_for_epochs =[metrics['val_acc'] for metrics in scores.epoch_data]\n",
        "\n",
        "data = pd.DataFrame(vall_acc_for_epochs)\n",
        "data.plot(label='accuracy')\n",
        "plt.legend([\"validation_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f12428db4d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0N1KQYcjrpy"
      },
      "source": [
        "Wariant 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsHYupAbetTj"
      },
      "source": [
        "\n",
        "# !!! UWAGA - przy każdych nowych obliczeniach należy zresetować graf sieci\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Wyłączamy warningi z tensorflow\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "# Warstwa wejściowa - musi mieć takie same wymiary jak dane\n",
        "network = input_data(shape=[None, 28, 28, 1], name='input') # None oznacza, że ta \n",
        "# wartość będzie uzupełniona automatyzcnie i jest to liczba próbek we wsadzie (batch)\n",
        "\n",
        "### MODYFIKUJEMY OD TĄD\n",
        "# ---\n",
        "# Pierwsza i druga warstwa konwolucyjna - 3`2 filtry o rozmiarach 3x3\n",
        "\n",
        "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
        "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
        "\n",
        "\n",
        "network = max_pool_2d(network, 2) # teraz obrazki są [14x14]\n",
        "\n",
        "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
        "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
        "\n",
        "\n",
        "network = max_pool_2d(network, 2) # teraz obrazki są [14x14]\n",
        "\n",
        "# druga warstwa konwolucyjna - 32 filtry o rozmiarach 3x3 z podpróbkowaniem\n",
        "network = conv_2d(network, 32, 3, activation='relu', regularizer=\"L2\")\n",
        "# network = max_pool_2d(network, 2) # teraz obrazki są [7x7]\n",
        "\n",
        "# warstwa pełna - tu już zaczyna się \"normalna\" sieć neuronowa\n",
        "network = fully_connected(network, 128, activation='relu') # 128 neuronów, aktywacja \"relu\", przyjmuje wejście [7x7] do [128] neuronów\n",
        "network = fully_connected(network, 256, activation='relu') # 256 neuronów, aktywacja \"relu\"\n",
        "# ---\n",
        "### DO TĄD\n",
        "\n",
        "# warstwa wyjściowa - używamy aktywacji softmax, żeby dostać prawdopodobieństwa dla każdej klasy (cyfry)\n",
        "network = fully_connected(network, 10, activation='softmax')\n",
        "\n",
        "# tu definiujemy w jaki sposób optymalizować sieć (regression nie oznacza, że robimy regresję)\n",
        "# nie będziemy modyfikować tych argumentów; stosujemy optymizator Adam\n",
        "network = regression(network, optimizer='adam', learning_rate=0.01,\n",
        "                     batch_size=250,\n",
        "                     loss='categorical_crossentropy', name='target')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sGJZnnCqj0T_",
        "outputId": "1dd49f7f-6e5a-470f-ac47-73d53d0aee65"
      },
      "source": [
        "# uruchamiamy trenowanie naszej sieci\n",
        "# n_epoch - liczba epok, czyli przejść przez cały zbiór danych treningowych\n",
        "scores = Stats(examples=len(X))\n",
        "model = tflearn.DNN(network, tensorboard_verbose=0)\n",
        "model.fit({'input': X}, {'target': Y},\n",
        "          n_epoch=EPOCHS,  # <-- DO ZMIANY\n",
        "          validation_set=({'input': testX}, {'target': testY}), show_metric=True, callbacks=[scores])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: VX4G66\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "---------------------------------\n",
            "Training samples: 55000\n",
            "Validation samples: 10000\n",
            "--\n",
            "Epoch 1, step (batch no.): 1 -- acc: 0.00, loss 0.00 -- iter 00250/55000, training for: 0.73s\n",
            "Epoch 1, step (batch no.): 3 -- acc: 0.09, loss 2.27 -- iter 00750/55000, training for: 1.83s\n",
            "Epoch 1, step (batch no.): 5 -- acc: 0.23, loss 2.26 -- iter 01250/55000, training for: 2.91s\n",
            "Epoch 1, step (batch no.): 7 -- acc: 0.27, loss 2.22 -- iter 01750/55000, training for: 3.99s\n",
            "Epoch 1, step (batch no.): 9 -- acc: 0.25, loss 2.16 -- iter 02250/55000, training for: 5.07s\n",
            "Epoch 1, step (batch no.): 11 -- acc: 0.22, loss 2.12 -- iter 02750/55000, training for: 6.14s\n",
            "Epoch 1, step (batch no.): 13 -- acc: 0.33, loss 1.80 -- iter 03250/55000, training for: 7.23s\n",
            "Epoch 1, step (batch no.): 15 -- acc: 0.32, loss 1.63 -- iter 03750/55000, training for: 8.29s\n",
            "Epoch 1, step (batch no.): 17 -- acc: 0.42, loss 1.44 -- iter 04250/55000, training for: 9.35s\n",
            "Epoch 1, step (batch no.): 19 -- acc: 0.51, loss 1.25 -- iter 04750/55000, training for: 10.43s\n",
            "Epoch 1, step (batch no.): 21 -- acc: 0.58, loss 1.15 -- iter 05250/55000, training for: 11.51s\n",
            "Epoch 1, step (batch no.): 23 -- acc: 0.62, loss 1.07 -- iter 05750/55000, training for: 12.57s\n",
            "Epoch 1, step (batch no.): 25 -- acc: 0.64, loss 1.00 -- iter 06250/55000, training for: 13.64s\n",
            "Epoch 1, step (batch no.): 27 -- acc: 0.69, loss 0.87 -- iter 06750/55000, training for: 14.70s\n",
            "Epoch 1, step (batch no.): 29 -- acc: 0.74, loss 0.76 -- iter 07250/55000, training for: 15.76s\n",
            "Epoch 1, step (batch no.): 31 -- acc: 0.75, loss 0.73 -- iter 07750/55000, training for: 16.87s\n",
            "Epoch 1, step (batch no.): 33 -- acc: 0.77, loss 0.68 -- iter 08250/55000, training for: 17.98s\n",
            "Epoch 1, step (batch no.): 35 -- acc: 0.79, loss 0.64 -- iter 08750/55000, training for: 19.05s\n",
            "Epoch 1, step (batch no.): 37 -- acc: 0.81, loss 0.57 -- iter 09250/55000, training for: 20.13s\n",
            "Epoch 1, step (batch no.): 39 -- acc: 0.83, loss 0.51 -- iter 09750/55000, training for: 21.22s\n",
            "Epoch 1, step (batch no.): 41 -- acc: 0.85, loss 0.43 -- iter 10250/55000, training for: 22.31s\n",
            "Epoch 1, step (batch no.): 43 -- acc: 0.87, loss 0.38 -- iter 10750/55000, training for: 23.45s\n",
            "Epoch 1, step (batch no.): 45 -- acc: 0.88, loss 0.35 -- iter 11250/55000, training for: 24.53s\n",
            "Epoch 1, step (batch no.): 47 -- acc: 0.89, loss 0.32 -- iter 11750/55000, training for: 25.60s\n",
            "Epoch 1, step (batch no.): 49 -- acc: 0.90, loss 0.30 -- iter 12250/55000, training for: 26.67s\n",
            "Epoch 1, step (batch no.): 51 -- acc: 0.91, loss 0.29 -- iter 12750/55000, training for: 27.75s\n",
            "Epoch 1, step (batch no.): 53 -- acc: 0.91, loss 0.27 -- iter 13250/55000, training for: 28.82s\n",
            "Epoch 1, step (batch no.): 55 -- acc: 0.92, loss 0.27 -- iter 13750/55000, training for: 29.89s\n",
            "Epoch 1, step (batch no.): 57 -- acc: 0.92, loss 0.24 -- iter 14250/55000, training for: 30.96s\n",
            "Epoch 1, step (batch no.): 59 -- acc: 0.92, loss 0.25 -- iter 14750/55000, training for: 32.08s\n",
            "Epoch 1, step (batch no.): 61 -- acc: 0.92, loss 0.25 -- iter 15250/55000, training for: 33.16s\n",
            "Epoch 1, step (batch no.): 63 -- acc: 0.92, loss 0.24 -- iter 15750/55000, training for: 34.24s\n",
            "Epoch 1, step (batch no.): 65 -- acc: 0.93, loss 0.23 -- iter 16250/55000, training for: 35.33s\n",
            "Epoch 1, step (batch no.): 67 -- acc: 0.94, loss 0.20 -- iter 16750/55000, training for: 36.39s\n",
            "Epoch 1, step (batch no.): 69 -- acc: 0.93, loss 0.22 -- iter 17250/55000, training for: 37.47s\n",
            "Epoch 1, step (batch no.): 71 -- acc: 0.94, loss 0.21 -- iter 17750/55000, training for: 38.57s\n",
            "Epoch 1, step (batch no.): 73 -- acc: 0.93, loss 0.23 -- iter 18250/55000, training for: 39.62s\n",
            "Epoch 1, step (batch no.): 75 -- acc: 0.93, loss 0.24 -- iter 18750/55000, training for: 40.69s\n",
            "Epoch 1, step (batch no.): 77 -- acc: 0.93, loss 0.23 -- iter 19250/55000, training for: 41.77s\n",
            "Epoch 1, step (batch no.): 79 -- acc: 0.93, loss 0.22 -- iter 19750/55000, training for: 42.82s\n",
            "Epoch 1, step (batch no.): 81 -- acc: 0.93, loss 0.21 -- iter 20250/55000, training for: 43.88s\n",
            "Epoch 1, step (batch no.): 83 -- acc: 0.94, loss 0.20 -- iter 20750/55000, training for: 44.93s\n",
            "Epoch 1, step (batch no.): 85 -- acc: 0.94, loss 0.19 -- iter 21250/55000, training for: 46.05s\n",
            "Epoch 1, step (batch no.): 87 -- acc: 0.95, loss 0.19 -- iter 21750/55000, training for: 47.12s\n",
            "Epoch 1, step (batch no.): 89 -- acc: 0.95, loss 0.18 -- iter 22250/55000, training for: 48.18s\n",
            "Epoch 1, step (batch no.): 91 -- acc: 0.95, loss 0.17 -- iter 22750/55000, training for: 49.24s\n",
            "Epoch 1, step (batch no.): 93 -- acc: 0.95, loss 0.16 -- iter 23250/55000, training for: 50.32s\n",
            "Epoch 1, step (batch no.): 95 -- acc: 0.95, loss 0.16 -- iter 23750/55000, training for: 51.41s\n",
            "Epoch 1, step (batch no.): 97 -- acc: 0.95, loss 0.16 -- iter 24250/55000, training for: 52.51s\n",
            "Epoch 1, step (batch no.): 99 -- acc: 0.96, loss 0.14 -- iter 24750/55000, training for: 53.59s\n",
            "Epoch 1, step (batch no.): 101 -- acc: 0.96, loss 0.14 -- iter 25250/55000, training for: 54.67s\n",
            "Epoch 1, step (batch no.): 103 -- acc: 0.96, loss 0.14 -- iter 25750/55000, training for: 55.80s\n",
            "Epoch 1, step (batch no.): 105 -- acc: 0.96, loss 0.12 -- iter 26250/55000, training for: 56.88s\n",
            "Epoch 1, step (batch no.): 107 -- acc: 0.96, loss 0.12 -- iter 26750/55000, training for: 57.94s\n",
            "Epoch 1, step (batch no.): 109 -- acc: 0.96, loss 0.12 -- iter 27250/55000, training for: 59.00s\n",
            "Epoch 1, step (batch no.): 111 -- acc: 0.96, loss 0.12 -- iter 27750/55000, training for: 60.10s\n",
            "Epoch 1, step (batch no.): 113 -- acc: 0.96, loss 0.12 -- iter 28250/55000, training for: 61.21s\n",
            "Epoch 1, step (batch no.): 115 -- acc: 0.96, loss 0.12 -- iter 28750/55000, training for: 62.29s\n",
            "Epoch 1, step (batch no.): 117 -- acc: 0.97, loss 0.11 -- iter 29250/55000, training for: 63.40s\n",
            "Epoch 1, step (batch no.): 119 -- acc: 0.96, loss 0.12 -- iter 29750/55000, training for: 64.46s\n",
            "Epoch 1, step (batch no.): 121 -- acc: 0.97, loss 0.11 -- iter 30250/55000, training for: 65.52s\n",
            "Epoch 1, step (batch no.): 123 -- acc: 0.97, loss 0.11 -- iter 30750/55000, training for: 66.59s\n",
            "Epoch 1, step (batch no.): 125 -- acc: 0.97, loss 0.10 -- iter 31250/55000, training for: 67.66s\n",
            "Epoch 1, step (batch no.): 127 -- acc: 0.97, loss 0.11 -- iter 31750/55000, training for: 68.74s\n",
            "Epoch 1, step (batch no.): 129 -- acc: 0.97, loss 0.11 -- iter 32250/55000, training for: 69.79s\n",
            "Epoch 1, step (batch no.): 131 -- acc: 0.96, loss 0.11 -- iter 32750/55000, training for: 70.85s\n",
            "Epoch 1, step (batch no.): 133 -- acc: 0.97, loss 0.11 -- iter 33250/55000, training for: 71.91s\n",
            "Epoch 1, step (batch no.): 135 -- acc: 0.97, loss 0.11 -- iter 33750/55000, training for: 72.99s\n",
            "Epoch 1, step (batch no.): 137 -- acc: 0.97, loss 0.11 -- iter 34250/55000, training for: 74.05s\n",
            "Epoch 1, step (batch no.): 139 -- acc: 0.97, loss 0.10 -- iter 34750/55000, training for: 75.12s\n",
            "Epoch 1, step (batch no.): 141 -- acc: 0.97, loss 0.09 -- iter 35250/55000, training for: 76.19s\n",
            "Epoch 1, step (batch no.): 143 -- acc: 0.97, loss 0.09 -- iter 35750/55000, training for: 77.24s\n",
            "Epoch 1, step (batch no.): 145 -- acc: 0.97, loss 0.10 -- iter 36250/55000, training for: 78.32s\n",
            "Epoch 1, step (batch no.): 147 -- acc: 0.97, loss 0.09 -- iter 36750/55000, training for: 79.38s\n",
            "Epoch 1, step (batch no.): 149 -- acc: 0.97, loss 0.10 -- iter 37250/55000, training for: 80.44s\n",
            "Epoch 1, step (batch no.): 151 -- acc: 0.97, loss 0.10 -- iter 37750/55000, training for: 81.51s\n",
            "Epoch 1, step (batch no.): 153 -- acc: 0.97, loss 0.10 -- iter 38250/55000, training for: 82.58s\n",
            "Epoch 1, step (batch no.): 155 -- acc: 0.97, loss 0.10 -- iter 38750/55000, training for: 83.67s\n",
            "Epoch 1, step (batch no.): 157 -- acc: 0.97, loss 0.10 -- iter 39250/55000, training for: 84.74s\n",
            "Epoch 1, step (batch no.): 159 -- acc: 0.97, loss 0.10 -- iter 39750/55000, training for: 85.81s\n",
            "Epoch 1, step (batch no.): 161 -- acc: 0.97, loss 0.10 -- iter 40250/55000, training for: 86.88s\n",
            "Epoch 1, step (batch no.): 163 -- acc: 0.97, loss 0.09 -- iter 40750/55000, training for: 87.96s\n",
            "Epoch 1, step (batch no.): 165 -- acc: 0.97, loss 0.09 -- iter 41250/55000, training for: 89.07s\n",
            "Epoch 1, step (batch no.): 167 -- acc: 0.97, loss 0.09 -- iter 41750/55000, training for: 90.12s\n",
            "Epoch 1, step (batch no.): 169 -- acc: 0.98, loss 0.09 -- iter 42250/55000, training for: 91.18s\n",
            "Epoch 1, step (batch no.): 171 -- acc: 0.97, loss 0.10 -- iter 42750/55000, training for: 92.25s\n",
            "Epoch 1, step (batch no.): 173 -- acc: 0.97, loss 0.10 -- iter 43250/55000, training for: 93.33s\n",
            "Epoch 1, step (batch no.): 175 -- acc: 0.97, loss 0.09 -- iter 43750/55000, training for: 94.38s\n",
            "Epoch 1, step (batch no.): 177 -- acc: 0.97, loss 0.09 -- iter 44250/55000, training for: 95.47s\n",
            "Epoch 1, step (batch no.): 179 -- acc: 0.97, loss 0.09 -- iter 44750/55000, training for: 96.53s\n",
            "Epoch 1, step (batch no.): 181 -- acc: 0.97, loss 0.09 -- iter 45250/55000, training for: 97.61s\n",
            "Epoch 1, step (batch no.): 183 -- acc: 0.98, loss 0.08 -- iter 45750/55000, training for: 98.74s\n",
            "Epoch 1, step (batch no.): 185 -- acc: 0.97, loss 0.09 -- iter 46250/55000, training for: 99.80s\n",
            "Epoch 1, step (batch no.): 187 -- acc: 0.97, loss 0.09 -- iter 46750/55000, training for: 100.87s\n",
            "Epoch 1, step (batch no.): 189 -- acc: 0.89, loss 1.31 -- iter 47250/55000, training for: 101.93s\n",
            "Epoch 1, step (batch no.): 191 -- acc: 0.90, loss 1.14 -- iter 47750/55000, training for: 103.00s\n",
            "Epoch 1, step (batch no.): 193 -- acc: 0.91, loss 0.99 -- iter 48250/55000, training for: 104.07s\n",
            "Epoch 1, step (batch no.): 195 -- acc: 0.92, loss 0.83 -- iter 48750/55000, training for: 105.13s\n",
            "Epoch 1, step (batch no.): 197 -- acc: 0.93, loss 0.69 -- iter 49250/55000, training for: 106.18s\n",
            "Epoch 1, step (batch no.): 199 -- acc: 0.93, loss 0.62 -- iter 49750/55000, training for: 107.26s\n",
            "Epoch 1, step (batch no.): 201 -- acc: 0.94, loss 0.53 -- iter 50250/55000, training for: 108.33s\n",
            "Epoch 1, step (batch no.): 203 -- acc: 0.93, loss 0.50 -- iter 50750/55000, training for: 109.42s\n",
            "Epoch 1, step (batch no.): 205 -- acc: 0.94, loss 0.43 -- iter 51250/55000, training for: 110.50s\n",
            "Epoch 1, step (batch no.): 207 -- acc: 0.94, loss 0.40 -- iter 51750/55000, training for: 111.57s\n",
            "Epoch 1, step (batch no.): 209 -- acc: 0.94, loss 0.36 -- iter 52250/55000, training for: 112.66s\n",
            "Epoch 1, step (batch no.): 211 -- acc: 0.94, loss 0.33 -- iter 52750/55000, training for: 113.75s\n",
            "Epoch 1, step (batch no.): 213 -- acc: 0.94, loss 0.29 -- iter 53250/55000, training for: 114.84s\n",
            "Epoch 1, step (batch no.): 215 -- acc: 0.95, loss 0.25 -- iter 53750/55000, training for: 115.96s\n",
            "Epoch 1, step (batch no.): 217 -- acc: 0.95, loss 0.23 -- iter 54250/55000, training for: 117.06s\n",
            "Epoch 1, step (batch no.): 219 -- acc: 0.95, loss 0.21 -- iter 54750/55000, training for: 118.15s\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.19114\u001b[0m\u001b[0m | time: 124.305s\n",
            "| Adam | epoch: 001 | loss: 0.19114 - acc: 0.9573 | val_loss: 0.08683 - val_acc: 0.9745 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 1, step (batch no.): 220 -- acc: 0.96, loss 0.19 -- iter 55000/55000, training for: 124.35s\n",
            "Epoch 2, step (batch no.): 222 -- acc: 0.96, loss 0.17 -- iter 00500/55000, training for: 125.43s\n",
            "Epoch 2, step (batch no.): 224 -- acc: 0.96, loss 0.15 -- iter 01000/55000, training for: 126.50s\n",
            "Epoch 2, step (batch no.): 226 -- acc: 0.96, loss 0.15 -- iter 01500/55000, training for: 127.57s\n",
            "Epoch 2, step (batch no.): 228 -- acc: 0.96, loss 0.15 -- iter 02000/55000, training for: 128.65s\n",
            "Epoch 2, step (batch no.): 230 -- acc: 0.96, loss 0.14 -- iter 02500/55000, training for: 129.72s\n",
            "Epoch 2, step (batch no.): 232 -- acc: 0.96, loss 0.14 -- iter 03000/55000, training for: 130.80s\n",
            "Epoch 2, step (batch no.): 234 -- acc: 0.96, loss 0.14 -- iter 03500/55000, training for: 131.86s\n",
            "Epoch 2, step (batch no.): 236 -- acc: 0.96, loss 0.13 -- iter 04000/55000, training for: 132.93s\n",
            "Epoch 2, step (batch no.): 238 -- acc: 0.96, loss 0.12 -- iter 04500/55000, training for: 133.99s\n",
            "Epoch 2, step (batch no.): 240 -- acc: 0.97, loss 0.12 -- iter 05000/55000, training for: 135.08s\n",
            "Epoch 2, step (batch no.): 242 -- acc: 0.97, loss 0.10 -- iter 05500/55000, training for: 136.14s\n",
            "Epoch 2, step (batch no.): 244 -- acc: 0.97, loss 0.10 -- iter 06000/55000, training for: 137.21s\n",
            "Epoch 2, step (batch no.): 246 -- acc: 0.97, loss 0.10 -- iter 06500/55000, training for: 138.29s\n",
            "Epoch 2, step (batch no.): 248 -- acc: 0.97, loss 0.11 -- iter 07000/55000, training for: 139.36s\n",
            "Epoch 2, step (batch no.): 250 -- acc: 0.97, loss 0.10 -- iter 07500/55000, training for: 140.43s\n",
            "Epoch 2, step (batch no.): 252 -- acc: 0.97, loss 0.10 -- iter 08000/55000, training for: 141.50s\n",
            "Epoch 2, step (batch no.): 254 -- acc: 0.97, loss 0.10 -- iter 08500/55000, training for: 142.56s\n",
            "Epoch 2, step (batch no.): 256 -- acc: 0.97, loss 0.11 -- iter 09000/55000, training for: 143.62s\n",
            "Epoch 2, step (batch no.): 258 -- acc: 0.97, loss 0.10 -- iter 09500/55000, training for: 144.69s\n",
            "Epoch 2, step (batch no.): 260 -- acc: 0.97, loss 0.10 -- iter 10000/55000, training for: 145.76s\n",
            "Epoch 2, step (batch no.): 262 -- acc: 0.97, loss 0.10 -- iter 10500/55000, training for: 146.83s\n",
            "Epoch 2, step (batch no.): 264 -- acc: 0.97, loss 0.09 -- iter 11000/55000, training for: 147.90s\n",
            "Epoch 2, step (batch no.): 266 -- acc: 0.97, loss 0.08 -- iter 11500/55000, training for: 149.02s\n",
            "Epoch 2, step (batch no.): 268 -- acc: 0.98, loss 0.07 -- iter 12000/55000, training for: 150.12s\n",
            "Epoch 2, step (batch no.): 270 -- acc: 0.98, loss 0.08 -- iter 12500/55000, training for: 151.20s\n",
            "Epoch 2, step (batch no.): 272 -- acc: 0.98, loss 0.08 -- iter 13000/55000, training for: 152.30s\n",
            "Epoch 2, step (batch no.): 274 -- acc: 0.98, loss 0.07 -- iter 13500/55000, training for: 153.38s\n",
            "Epoch 2, step (batch no.): 276 -- acc: 0.98, loss 0.08 -- iter 14000/55000, training for: 154.50s\n",
            "Epoch 2, step (batch no.): 278 -- acc: 0.98, loss 0.07 -- iter 14500/55000, training for: 155.62s\n",
            "Epoch 2, step (batch no.): 280 -- acc: 0.98, loss 0.07 -- iter 15000/55000, training for: 156.70s\n",
            "Epoch 2, step (batch no.): 282 -- acc: 0.98, loss 0.06 -- iter 15500/55000, training for: 157.77s\n",
            "Epoch 2, step (batch no.): 284 -- acc: 0.98, loss 0.06 -- iter 16000/55000, training for: 158.85s\n",
            "Epoch 2, step (batch no.): 286 -- acc: 0.98, loss 0.05 -- iter 16500/55000, training for: 159.91s\n",
            "Epoch 2, step (batch no.): 288 -- acc: 0.98, loss 0.06 -- iter 17000/55000, training for: 161.00s\n",
            "Epoch 2, step (batch no.): 290 -- acc: 0.98, loss 0.07 -- iter 17500/55000, training for: 162.05s\n",
            "Epoch 2, step (batch no.): 292 -- acc: 0.98, loss 0.07 -- iter 18000/55000, training for: 163.13s\n",
            "Epoch 2, step (batch no.): 294 -- acc: 0.98, loss 0.07 -- iter 18500/55000, training for: 164.20s\n",
            "Epoch 2, step (batch no.): 296 -- acc: 0.98, loss 0.06 -- iter 19000/55000, training for: 165.30s\n",
            "Epoch 2, step (batch no.): 298 -- acc: 0.98, loss 0.07 -- iter 19500/55000, training for: 166.40s\n",
            "Epoch 2, step (batch no.): 300 -- acc: 0.98, loss 0.06 -- iter 20000/55000, training for: 167.47s\n",
            "Epoch 2, step (batch no.): 302 -- acc: 0.98, loss 0.06 -- iter 20500/55000, training for: 168.54s\n",
            "Epoch 2, step (batch no.): 304 -- acc: 0.98, loss 0.05 -- iter 21000/55000, training for: 169.61s\n",
            "Epoch 2, step (batch no.): 306 -- acc: 0.98, loss 0.05 -- iter 21500/55000, training for: 170.68s\n",
            "Epoch 2, step (batch no.): 308 -- acc: 0.98, loss 0.05 -- iter 22000/55000, training for: 171.75s\n",
            "Epoch 2, step (batch no.): 310 -- acc: 0.98, loss 0.06 -- iter 22500/55000, training for: 172.81s\n",
            "Epoch 2, step (batch no.): 312 -- acc: 0.98, loss 0.05 -- iter 23000/55000, training for: 173.90s\n",
            "Epoch 2, step (batch no.): 314 -- acc: 0.98, loss 0.05 -- iter 23500/55000, training for: 174.96s\n",
            "Epoch 2, step (batch no.): 316 -- acc: 0.98, loss 0.06 -- iter 24000/55000, training for: 176.05s\n",
            "Epoch 2, step (batch no.): 318 -- acc: 0.98, loss 0.06 -- iter 24500/55000, training for: 177.14s\n",
            "Epoch 2, step (batch no.): 320 -- acc: 0.98, loss 0.06 -- iter 25000/55000, training for: 178.21s\n",
            "Epoch 2, step (batch no.): 322 -- acc: 0.98, loss 0.06 -- iter 25500/55000, training for: 179.32s\n",
            "Epoch 2, step (batch no.): 324 -- acc: 0.98, loss 0.06 -- iter 26000/55000, training for: 180.42s\n",
            "Epoch 2, step (batch no.): 326 -- acc: 0.98, loss 0.05 -- iter 26500/55000, training for: 181.52s\n",
            "Epoch 2, step (batch no.): 328 -- acc: 0.98, loss 0.06 -- iter 27000/55000, training for: 182.64s\n",
            "Epoch 2, step (batch no.): 330 -- acc: 0.98, loss 0.06 -- iter 27500/55000, training for: 183.76s\n",
            "Epoch 2, step (batch no.): 332 -- acc: 0.98, loss 0.05 -- iter 28000/55000, training for: 184.86s\n",
            "Epoch 2, step (batch no.): 334 -- acc: 0.98, loss 0.05 -- iter 28500/55000, training for: 185.98s\n",
            "Epoch 2, step (batch no.): 336 -- acc: 0.98, loss 0.06 -- iter 29000/55000, training for: 187.08s\n",
            "Epoch 2, step (batch no.): 338 -- acc: 0.98, loss 0.06 -- iter 29500/55000, training for: 188.18s\n",
            "Epoch 2, step (batch no.): 340 -- acc: 0.98, loss 0.06 -- iter 30000/55000, training for: 189.32s\n",
            "Epoch 2, step (batch no.): 342 -- acc: 0.98, loss 0.06 -- iter 30500/55000, training for: 190.40s\n",
            "Epoch 2, step (batch no.): 344 -- acc: 0.98, loss 0.07 -- iter 31000/55000, training for: 191.49s\n",
            "Epoch 2, step (batch no.): 346 -- acc: 0.98, loss 0.07 -- iter 31500/55000, training for: 192.58s\n",
            "Epoch 2, step (batch no.): 348 -- acc: 0.98, loss 0.08 -- iter 32000/55000, training for: 193.66s\n",
            "Epoch 2, step (batch no.): 350 -- acc: 0.98, loss 0.07 -- iter 32500/55000, training for: 194.74s\n",
            "Epoch 2, step (batch no.): 352 -- acc: 0.98, loss 0.07 -- iter 33000/55000, training for: 195.81s\n",
            "Epoch 2, step (batch no.): 354 -- acc: 0.98, loss 0.07 -- iter 33500/55000, training for: 196.91s\n",
            "Epoch 2, step (batch no.): 356 -- acc: 0.98, loss 0.07 -- iter 34000/55000, training for: 197.99s\n",
            "Epoch 2, step (batch no.): 358 -- acc: 0.98, loss 0.07 -- iter 34500/55000, training for: 199.07s\n",
            "Epoch 2, step (batch no.): 360 -- acc: 0.98, loss 0.06 -- iter 35000/55000, training for: 200.16s\n",
            "Epoch 2, step (batch no.): 362 -- acc: 0.98, loss 0.07 -- iter 35500/55000, training for: 201.23s\n",
            "Epoch 2, step (batch no.): 364 -- acc: 0.98, loss 0.06 -- iter 36000/55000, training for: 202.31s\n",
            "Epoch 2, step (batch no.): 366 -- acc: 0.98, loss 0.06 -- iter 36500/55000, training for: 203.36s\n",
            "Epoch 2, step (batch no.): 368 -- acc: 0.98, loss 0.06 -- iter 37000/55000, training for: 204.44s\n",
            "Epoch 2, step (batch no.): 370 -- acc: 0.98, loss 0.05 -- iter 37500/55000, training for: 205.50s\n",
            "Epoch 2, step (batch no.): 372 -- acc: 0.98, loss 0.06 -- iter 38000/55000, training for: 206.56s\n",
            "Epoch 2, step (batch no.): 374 -- acc: 0.98, loss 0.08 -- iter 38500/55000, training for: 207.62s\n",
            "Epoch 2, step (batch no.): 376 -- acc: 0.98, loss 0.08 -- iter 39000/55000, training for: 208.70s\n",
            "Epoch 2, step (batch no.): 378 -- acc: 0.98, loss 0.08 -- iter 39500/55000, training for: 209.75s\n",
            "Epoch 2, step (batch no.): 380 -- acc: 0.98, loss 0.08 -- iter 40000/55000, training for: 210.83s\n",
            "Epoch 2, step (batch no.): 382 -- acc: 0.98, loss 0.08 -- iter 40500/55000, training for: 211.91s\n",
            "Epoch 2, step (batch no.): 384 -- acc: 0.98, loss 0.07 -- iter 41000/55000, training for: 212.98s\n",
            "Epoch 2, step (batch no.): 386 -- acc: 0.98, loss 0.07 -- iter 41500/55000, training for: 214.06s\n",
            "Epoch 2, step (batch no.): 388 -- acc: 0.98, loss 0.06 -- iter 42000/55000, training for: 215.17s\n",
            "Epoch 2, step (batch no.): 390 -- acc: 0.98, loss 0.06 -- iter 42500/55000, training for: 216.30s\n",
            "Epoch 2, step (batch no.): 392 -- acc: 0.98, loss 0.08 -- iter 43000/55000, training for: 217.38s\n",
            "Epoch 2, step (batch no.): 394 -- acc: 0.98, loss 0.07 -- iter 43500/55000, training for: 218.45s\n",
            "Epoch 2, step (batch no.): 396 -- acc: 0.98, loss 0.07 -- iter 44000/55000, training for: 219.52s\n",
            "Epoch 2, step (batch no.): 398 -- acc: 0.98, loss 0.07 -- iter 44500/55000, training for: 220.61s\n",
            "Epoch 2, step (batch no.): 400 -- acc: 0.98, loss 0.07 -- iter 45000/55000, training for: 221.75s\n",
            "Epoch 2, step (batch no.): 402 -- acc: 0.98, loss 0.08 -- iter 45500/55000, training for: 222.83s\n",
            "Epoch 2, step (batch no.): 404 -- acc: 0.98, loss 0.08 -- iter 46000/55000, training for: 223.90s\n",
            "Epoch 2, step (batch no.): 406 -- acc: 0.98, loss 0.08 -- iter 46500/55000, training for: 224.97s\n",
            "Epoch 2, step (batch no.): 408 -- acc: 0.98, loss 0.08 -- iter 47000/55000, training for: 226.02s\n",
            "Epoch 2, step (batch no.): 410 -- acc: 0.98, loss 0.08 -- iter 47500/55000, training for: 227.09s\n",
            "Epoch 2, step (batch no.): 412 -- acc: 0.98, loss 0.07 -- iter 48000/55000, training for: 228.17s\n",
            "Epoch 2, step (batch no.): 414 -- acc: 0.98, loss 0.07 -- iter 48500/55000, training for: 229.25s\n",
            "Epoch 2, step (batch no.): 416 -- acc: 0.98, loss 0.07 -- iter 49000/55000, training for: 230.32s\n",
            "Epoch 2, step (batch no.): 418 -- acc: 0.98, loss 0.06 -- iter 49500/55000, training for: 231.39s\n",
            "Epoch 2, step (batch no.): 420 -- acc: 0.98, loss 0.06 -- iter 50000/55000, training for: 232.44s\n",
            "Epoch 2, step (batch no.): 422 -- acc: 0.98, loss 0.05 -- iter 50500/55000, training for: 233.50s\n",
            "Epoch 2, step (batch no.): 424 -- acc: 0.99, loss 0.05 -- iter 51000/55000, training for: 234.57s\n",
            "Epoch 2, step (batch no.): 426 -- acc: 0.98, loss 0.06 -- iter 51500/55000, training for: 235.63s\n",
            "Epoch 2, step (batch no.): 428 -- acc: 0.98, loss 0.06 -- iter 52000/55000, training for: 236.70s\n",
            "Epoch 2, step (batch no.): 430 -- acc: 0.98, loss 0.06 -- iter 52500/55000, training for: 237.75s\n",
            "Epoch 2, step (batch no.): 432 -- acc: 0.98, loss 0.06 -- iter 53000/55000, training for: 238.83s\n",
            "Epoch 2, step (batch no.): 434 -- acc: 0.98, loss 0.05 -- iter 53500/55000, training for: 239.88s\n",
            "Epoch 2, step (batch no.): 436 -- acc: 0.98, loss 0.06 -- iter 54000/55000, training for: 240.95s\n",
            "Epoch 2, step (batch no.): 438 -- acc: 0.98, loss 0.05 -- iter 54500/55000, training for: 242.02s\n",
            "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.06114\u001b[0m\u001b[0m | time: 124.129s\n",
            "| Adam | epoch: 002 | loss: 0.06114 - acc: 0.9824 | val_loss: 0.05607 - val_acc: 0.9839 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 2, step (batch no.): 440 -- acc: 0.98, loss 0.06 -- iter 55000/55000, training for: 248.52s\n",
            "Epoch 3, step (batch no.): 442 -- acc: 0.98, loss 0.05 -- iter 00500/55000, training for: 249.61s\n",
            "Epoch 3, step (batch no.): 444 -- acc: 0.99, loss 0.05 -- iter 01000/55000, training for: 250.69s\n",
            "Epoch 3, step (batch no.): 446 -- acc: 0.99, loss 0.05 -- iter 01500/55000, training for: 251.77s\n",
            "Epoch 3, step (batch no.): 448 -- acc: 0.99, loss 0.05 -- iter 02000/55000, training for: 252.86s\n",
            "Epoch 3, step (batch no.): 450 -- acc: 0.99, loss 0.05 -- iter 02500/55000, training for: 253.96s\n",
            "Epoch 3, step (batch no.): 452 -- acc: 0.98, loss 0.05 -- iter 03000/55000, training for: 255.03s\n",
            "Epoch 3, step (batch no.): 454 -- acc: 0.98, loss 0.05 -- iter 03500/55000, training for: 256.09s\n",
            "Epoch 3, step (batch no.): 456 -- acc: 0.98, loss 0.05 -- iter 04000/55000, training for: 257.16s\n",
            "Epoch 3, step (batch no.): 458 -- acc: 0.98, loss 0.05 -- iter 04500/55000, training for: 258.22s\n",
            "Epoch 3, step (batch no.): 460 -- acc: 0.98, loss 0.05 -- iter 05000/55000, training for: 259.30s\n",
            "Epoch 3, step (batch no.): 462 -- acc: 0.98, loss 0.05 -- iter 05500/55000, training for: 260.41s\n",
            "Epoch 3, step (batch no.): 464 -- acc: 0.98, loss 0.06 -- iter 06000/55000, training for: 261.50s\n",
            "Epoch 3, step (batch no.): 466 -- acc: 0.98, loss 0.07 -- iter 06500/55000, training for: 262.55s\n",
            "Epoch 3, step (batch no.): 468 -- acc: 0.98, loss 0.07 -- iter 07000/55000, training for: 263.62s\n",
            "Epoch 3, step (batch no.): 470 -- acc: 0.98, loss 0.08 -- iter 07500/55000, training for: 264.71s\n",
            "Epoch 3, step (batch no.): 472 -- acc: 0.98, loss 0.08 -- iter 08000/55000, training for: 265.77s\n",
            "Epoch 3, step (batch no.): 474 -- acc: 0.98, loss 0.08 -- iter 08500/55000, training for: 266.84s\n",
            "Epoch 3, step (batch no.): 476 -- acc: 0.98, loss 0.07 -- iter 09000/55000, training for: 267.91s\n",
            "Epoch 3, step (batch no.): 478 -- acc: 0.98, loss 0.07 -- iter 09500/55000, training for: 268.97s\n",
            "Epoch 3, step (batch no.): 480 -- acc: 0.98, loss 0.07 -- iter 10000/55000, training for: 270.04s\n",
            "Epoch 3, step (batch no.): 482 -- acc: 0.98, loss 0.06 -- iter 10500/55000, training for: 271.14s\n",
            "Epoch 3, step (batch no.): 484 -- acc: 0.98, loss 0.06 -- iter 11000/55000, training for: 272.23s\n",
            "Epoch 3, step (batch no.): 486 -- acc: 0.98, loss 0.06 -- iter 11500/55000, training for: 273.33s\n",
            "Epoch 3, step (batch no.): 488 -- acc: 0.98, loss 0.06 -- iter 12000/55000, training for: 274.40s\n",
            "Epoch 3, step (batch no.): 490 -- acc: 0.98, loss 0.07 -- iter 12500/55000, training for: 275.50s\n",
            "Epoch 3, step (batch no.): 492 -- acc: 0.98, loss 0.06 -- iter 13000/55000, training for: 276.57s\n",
            "Epoch 3, step (batch no.): 494 -- acc: 0.98, loss 0.06 -- iter 13500/55000, training for: 277.67s\n",
            "Epoch 3, step (batch no.): 496 -- acc: 0.98, loss 0.06 -- iter 14000/55000, training for: 278.76s\n",
            "Epoch 3, step (batch no.): 498 -- acc: 0.98, loss 0.07 -- iter 14500/55000, training for: 279.84s\n",
            "Epoch 3, step (batch no.): 500 -- acc: 0.98, loss 0.06 -- iter 15000/55000, training for: 280.95s\n",
            "Epoch 3, step (batch no.): 502 -- acc: 0.98, loss 0.05 -- iter 15500/55000, training for: 282.02s\n",
            "Epoch 3, step (batch no.): 504 -- acc: 0.98, loss 0.05 -- iter 16000/55000, training for: 283.10s\n",
            "Epoch 3, step (batch no.): 506 -- acc: 0.98, loss 0.05 -- iter 16500/55000, training for: 284.20s\n",
            "Epoch 3, step (batch no.): 508 -- acc: 0.98, loss 0.06 -- iter 17000/55000, training for: 285.28s\n",
            "Epoch 3, step (batch no.): 510 -- acc: 0.98, loss 0.05 -- iter 17500/55000, training for: 286.36s\n",
            "Epoch 3, step (batch no.): 512 -- acc: 0.98, loss 0.05 -- iter 18000/55000, training for: 287.49s\n",
            "Epoch 3, step (batch no.): 514 -- acc: 0.98, loss 0.05 -- iter 18500/55000, training for: 288.56s\n",
            "Epoch 3, step (batch no.): 516 -- acc: 0.98, loss 0.05 -- iter 19000/55000, training for: 289.62s\n",
            "Epoch 3, step (batch no.): 518 -- acc: 0.98, loss 0.06 -- iter 19500/55000, training for: 290.72s\n",
            "Epoch 3, step (batch no.): 520 -- acc: 0.98, loss 0.07 -- iter 20000/55000, training for: 291.81s\n",
            "Epoch 3, step (batch no.): 522 -- acc: 0.98, loss 0.07 -- iter 20500/55000, training for: 292.92s\n",
            "Epoch 3, step (batch no.): 524 -- acc: 0.98, loss 0.07 -- iter 21000/55000, training for: 294.05s\n",
            "Epoch 3, step (batch no.): 526 -- acc: 0.98, loss 0.07 -- iter 21500/55000, training for: 295.17s\n",
            "Epoch 3, step (batch no.): 528 -- acc: 0.98, loss 0.07 -- iter 22000/55000, training for: 296.27s\n",
            "Epoch 3, step (batch no.): 530 -- acc: 0.98, loss 0.07 -- iter 22500/55000, training for: 297.38s\n",
            "Epoch 3, step (batch no.): 532 -- acc: 0.98, loss 0.06 -- iter 23000/55000, training for: 298.46s\n",
            "Epoch 3, step (batch no.): 534 -- acc: 0.98, loss 0.06 -- iter 23500/55000, training for: 299.55s\n",
            "Epoch 3, step (batch no.): 536 -- acc: 0.98, loss 0.05 -- iter 24000/55000, training for: 300.65s\n",
            "Epoch 3, step (batch no.): 538 -- acc: 0.98, loss 0.05 -- iter 24500/55000, training for: 301.72s\n",
            "Epoch 3, step (batch no.): 540 -- acc: 0.98, loss 0.05 -- iter 25000/55000, training for: 302.83s\n",
            "Epoch 3, step (batch no.): 542 -- acc: 0.98, loss 0.05 -- iter 25500/55000, training for: 303.96s\n",
            "Epoch 3, step (batch no.): 544 -- acc: 0.98, loss 0.05 -- iter 26000/55000, training for: 305.05s\n",
            "Epoch 3, step (batch no.): 546 -- acc: 0.98, loss 0.05 -- iter 26500/55000, training for: 306.14s\n",
            "Epoch 3, step (batch no.): 548 -- acc: 0.98, loss 0.06 -- iter 27000/55000, training for: 307.23s\n",
            "Epoch 3, step (batch no.): 550 -- acc: 0.98, loss 0.06 -- iter 27500/55000, training for: 308.35s\n",
            "Epoch 3, step (batch no.): 552 -- acc: 0.98, loss 0.06 -- iter 28000/55000, training for: 309.44s\n",
            "Epoch 3, step (batch no.): 554 -- acc: 0.98, loss 0.05 -- iter 28500/55000, training for: 310.59s\n",
            "Epoch 3, step (batch no.): 556 -- acc: 0.98, loss 0.06 -- iter 29000/55000, training for: 311.71s\n",
            "Epoch 3, step (batch no.): 558 -- acc: 0.98, loss 0.06 -- iter 29500/55000, training for: 312.82s\n",
            "Epoch 3, step (batch no.): 560 -- acc: 0.98, loss 0.06 -- iter 30000/55000, training for: 313.90s\n",
            "Epoch 3, step (batch no.): 562 -- acc: 0.98, loss 0.06 -- iter 30500/55000, training for: 314.99s\n",
            "Epoch 3, step (batch no.): 564 -- acc: 0.98, loss 0.06 -- iter 31000/55000, training for: 316.09s\n",
            "Epoch 3, step (batch no.): 566 -- acc: 0.98, loss 0.07 -- iter 31500/55000, training for: 317.18s\n",
            "Epoch 3, step (batch no.): 568 -- acc: 0.98, loss 0.07 -- iter 32000/55000, training for: 318.28s\n",
            "Epoch 3, step (batch no.): 570 -- acc: 0.98, loss 0.06 -- iter 32500/55000, training for: 319.37s\n",
            "Epoch 3, step (batch no.): 572 -- acc: 0.98, loss 0.06 -- iter 33000/55000, training for: 320.48s\n",
            "Epoch 3, step (batch no.): 574 -- acc: 0.98, loss 0.06 -- iter 33500/55000, training for: 321.64s\n",
            "Epoch 3, step (batch no.): 576 -- acc: 0.98, loss 0.06 -- iter 34000/55000, training for: 322.73s\n",
            "Epoch 3, step (batch no.): 578 -- acc: 0.98, loss 0.06 -- iter 34500/55000, training for: 323.81s\n",
            "Epoch 3, step (batch no.): 580 -- acc: 0.98, loss 0.07 -- iter 35000/55000, training for: 324.86s\n",
            "Epoch 3, step (batch no.): 582 -- acc: 0.98, loss 0.07 -- iter 35500/55000, training for: 325.93s\n",
            "Epoch 3, step (batch no.): 584 -- acc: 0.98, loss 0.07 -- iter 36000/55000, training for: 326.99s\n",
            "Epoch 3, step (batch no.): 586 -- acc: 0.98, loss 0.07 -- iter 36500/55000, training for: 328.07s\n",
            "Epoch 3, step (batch no.): 588 -- acc: 0.98, loss 0.06 -- iter 37000/55000, training for: 329.14s\n",
            "Epoch 3, step (batch no.): 590 -- acc: 0.98, loss 0.06 -- iter 37500/55000, training for: 330.20s\n",
            "Epoch 3, step (batch no.): 592 -- acc: 0.98, loss 0.07 -- iter 38000/55000, training for: 331.29s\n",
            "Epoch 3, step (batch no.): 594 -- acc: 0.98, loss 0.07 -- iter 38500/55000, training for: 332.34s\n",
            "Epoch 3, step (batch no.): 596 -- acc: 0.98, loss 0.07 -- iter 39000/55000, training for: 333.42s\n",
            "Epoch 3, step (batch no.): 598 -- acc: 0.98, loss 0.06 -- iter 39500/55000, training for: 334.56s\n",
            "Epoch 3, step (batch no.): 600 -- acc: 0.98, loss 0.05 -- iter 40000/55000, training for: 335.64s\n",
            "Epoch 3, step (batch no.): 602 -- acc: 0.98, loss 0.05 -- iter 40500/55000, training for: 336.71s\n",
            "Epoch 3, step (batch no.): 604 -- acc: 0.98, loss 0.06 -- iter 41000/55000, training for: 337.79s\n",
            "Epoch 3, step (batch no.): 606 -- acc: 0.98, loss 0.05 -- iter 41500/55000, training for: 338.87s\n",
            "Epoch 3, step (batch no.): 608 -- acc: 0.98, loss 0.05 -- iter 42000/55000, training for: 339.94s\n",
            "Epoch 3, step (batch no.): 610 -- acc: 0.98, loss 0.05 -- iter 42500/55000, training for: 341.05s\n",
            "Epoch 3, step (batch no.): 612 -- acc: 0.98, loss 0.05 -- iter 43000/55000, training for: 342.15s\n",
            "Epoch 3, step (batch no.): 614 -- acc: 0.98, loss 0.06 -- iter 43500/55000, training for: 343.22s\n",
            "Epoch 3, step (batch no.): 616 -- acc: 0.98, loss 0.07 -- iter 44000/55000, training for: 344.30s\n",
            "Epoch 3, step (batch no.): 618 -- acc: 0.98, loss 0.06 -- iter 44500/55000, training for: 345.38s\n",
            "Epoch 3, step (batch no.): 620 -- acc: 0.98, loss 0.06 -- iter 45000/55000, training for: 346.49s\n",
            "Epoch 3, step (batch no.): 622 -- acc: 0.98, loss 0.06 -- iter 45500/55000, training for: 347.63s\n",
            "Epoch 3, step (batch no.): 624 -- acc: 0.99, loss 0.05 -- iter 46000/55000, training for: 348.72s\n",
            "Epoch 3, step (batch no.): 626 -- acc: 0.99, loss 0.05 -- iter 46500/55000, training for: 349.82s\n",
            "Epoch 3, step (batch no.): 628 -- acc: 0.99, loss 0.04 -- iter 47000/55000, training for: 350.91s\n",
            "Epoch 3, step (batch no.): 630 -- acc: 0.99, loss 0.05 -- iter 47500/55000, training for: 352.02s\n",
            "Epoch 3, step (batch no.): 632 -- acc: 0.99, loss 0.05 -- iter 48000/55000, training for: 353.13s\n",
            "Epoch 3, step (batch no.): 634 -- acc: 0.99, loss 0.04 -- iter 48500/55000, training for: 354.27s\n",
            "Epoch 3, step (batch no.): 636 -- acc: 0.99, loss 0.04 -- iter 49000/55000, training for: 355.36s\n",
            "Epoch 3, step (batch no.): 638 -- acc: 0.99, loss 0.04 -- iter 49500/55000, training for: 356.45s\n",
            "Epoch 3, step (batch no.): 640 -- acc: 0.99, loss 0.04 -- iter 50000/55000, training for: 357.54s\n",
            "Epoch 3, step (batch no.): 642 -- acc: 0.99, loss 0.04 -- iter 50500/55000, training for: 358.63s\n",
            "Epoch 3, step (batch no.): 644 -- acc: 0.99, loss 0.03 -- iter 51000/55000, training for: 359.71s\n",
            "Epoch 3, step (batch no.): 646 -- acc: 0.99, loss 0.04 -- iter 51500/55000, training for: 360.79s\n",
            "Epoch 3, step (batch no.): 648 -- acc: 0.99, loss 0.04 -- iter 52000/55000, training for: 361.90s\n",
            "Epoch 3, step (batch no.): 650 -- acc: 0.99, loss 0.04 -- iter 52500/55000, training for: 363.03s\n",
            "Epoch 3, step (batch no.): 652 -- acc: 0.99, loss 0.04 -- iter 53000/55000, training for: 364.19s\n",
            "Epoch 3, step (batch no.): 654 -- acc: 0.98, loss 0.06 -- iter 53500/55000, training for: 365.26s\n",
            "Epoch 3, step (batch no.): 656 -- acc: 0.99, loss 0.05 -- iter 54000/55000, training for: 366.37s\n",
            "Epoch 3, step (batch no.): 658 -- acc: 0.98, loss 0.07 -- iter 54500/55000, training for: 367.58s\n",
            "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.06231\u001b[0m\u001b[0m | time: 125.543s\n",
            "| Adam | epoch: 003 | loss: 0.06231 - acc: 0.9836 | val_loss: 0.05135 - val_acc: 0.9839 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 3, step (batch no.): 660 -- acc: 0.98, loss 0.06 -- iter 55000/55000, training for: 374.11s\n",
            "Epoch 4, step (batch no.): 662 -- acc: 0.98, loss 0.06 -- iter 00500/55000, training for: 375.18s\n",
            "Epoch 4, step (batch no.): 664 -- acc: 0.98, loss 0.06 -- iter 01000/55000, training for: 376.32s\n",
            "Epoch 4, step (batch no.): 666 -- acc: 0.98, loss 0.06 -- iter 01500/55000, training for: 377.39s\n",
            "Epoch 4, step (batch no.): 668 -- acc: 0.98, loss 0.06 -- iter 02000/55000, training for: 378.48s\n",
            "Epoch 4, step (batch no.): 670 -- acc: 0.98, loss 0.05 -- iter 02500/55000, training for: 379.57s\n",
            "Epoch 4, step (batch no.): 672 -- acc: 0.99, loss 0.05 -- iter 03000/55000, training for: 380.67s\n",
            "Epoch 4, step (batch no.): 674 -- acc: 0.99, loss 0.05 -- iter 03500/55000, training for: 381.74s\n",
            "Epoch 4, step (batch no.): 676 -- acc: 0.99, loss 0.05 -- iter 04000/55000, training for: 382.85s\n",
            "Epoch 4, step (batch no.): 678 -- acc: 0.99, loss 0.05 -- iter 04500/55000, training for: 383.91s\n",
            "Epoch 4, step (batch no.): 680 -- acc: 0.99, loss 0.04 -- iter 05000/55000, training for: 384.98s\n",
            "Epoch 4, step (batch no.): 682 -- acc: 0.99, loss 0.04 -- iter 05500/55000, training for: 386.10s\n",
            "Epoch 4, step (batch no.): 684 -- acc: 0.99, loss 0.04 -- iter 06000/55000, training for: 387.17s\n",
            "Epoch 4, step (batch no.): 686 -- acc: 0.99, loss 0.04 -- iter 06500/55000, training for: 388.24s\n",
            "Epoch 4, step (batch no.): 688 -- acc: 0.99, loss 0.04 -- iter 07000/55000, training for: 389.30s\n",
            "Epoch 4, step (batch no.): 690 -- acc: 0.99, loss 0.04 -- iter 07500/55000, training for: 390.37s\n",
            "Epoch 4, step (batch no.): 692 -- acc: 0.98, loss 0.04 -- iter 08000/55000, training for: 391.43s\n",
            "Epoch 4, step (batch no.): 694 -- acc: 0.98, loss 0.04 -- iter 08500/55000, training for: 392.50s\n",
            "Epoch 4, step (batch no.): 696 -- acc: 0.98, loss 0.05 -- iter 09000/55000, training for: 393.60s\n",
            "Epoch 4, step (batch no.): 698 -- acc: 0.98, loss 0.05 -- iter 09500/55000, training for: 394.67s\n",
            "Epoch 4, step (batch no.): 700 -- acc: 0.99, loss 0.04 -- iter 10000/55000, training for: 395.74s\n",
            "Epoch 4, step (batch no.): 702 -- acc: 0.99, loss 0.04 -- iter 10500/55000, training for: 396.81s\n",
            "Epoch 4, step (batch no.): 704 -- acc: 0.99, loss 0.04 -- iter 11000/55000, training for: 397.87s\n",
            "Epoch 4, step (batch no.): 706 -- acc: 0.99, loss 0.05 -- iter 11500/55000, training for: 398.93s\n",
            "Epoch 4, step (batch no.): 708 -- acc: 0.99, loss 0.06 -- iter 12000/55000, training for: 400.01s\n",
            "Epoch 4, step (batch no.): 710 -- acc: 0.98, loss 0.06 -- iter 12500/55000, training for: 401.08s\n",
            "Epoch 4, step (batch no.): 712 -- acc: 0.98, loss 0.06 -- iter 13000/55000, training for: 402.13s\n",
            "Epoch 4, step (batch no.): 714 -- acc: 0.98, loss 0.06 -- iter 13500/55000, training for: 403.22s\n",
            "Epoch 4, step (batch no.): 716 -- acc: 0.98, loss 0.05 -- iter 14000/55000, training for: 404.33s\n",
            "Epoch 4, step (batch no.): 718 -- acc: 0.98, loss 0.05 -- iter 14500/55000, training for: 405.42s\n",
            "Epoch 4, step (batch no.): 720 -- acc: 0.98, loss 0.05 -- iter 15000/55000, training for: 406.49s\n",
            "Epoch 4, step (batch no.): 722 -- acc: 0.98, loss 0.05 -- iter 15500/55000, training for: 407.55s\n",
            "Epoch 4, step (batch no.): 724 -- acc: 0.98, loss 0.05 -- iter 16000/55000, training for: 408.61s\n",
            "Epoch 4, step (batch no.): 726 -- acc: 0.99, loss 0.04 -- iter 16500/55000, training for: 409.67s\n",
            "Epoch 4, step (batch no.): 728 -- acc: 0.99, loss 0.04 -- iter 17000/55000, training for: 410.74s\n",
            "Epoch 4, step (batch no.): 730 -- acc: 0.99, loss 0.05 -- iter 17500/55000, training for: 411.80s\n",
            "Epoch 4, step (batch no.): 732 -- acc: 0.98, loss 0.06 -- iter 18000/55000, training for: 412.89s\n",
            "Epoch 4, step (batch no.): 734 -- acc: 0.98, loss 0.06 -- iter 18500/55000, training for: 414.00s\n",
            "Epoch 4, step (batch no.): 736 -- acc: 0.98, loss 0.06 -- iter 19000/55000, training for: 415.09s\n",
            "Epoch 4, step (batch no.): 738 -- acc: 0.98, loss 0.06 -- iter 19500/55000, training for: 416.19s\n",
            "Epoch 4, step (batch no.): 740 -- acc: 0.98, loss 0.06 -- iter 20000/55000, training for: 417.26s\n",
            "Epoch 4, step (batch no.): 742 -- acc: 0.98, loss 0.06 -- iter 20500/55000, training for: 418.33s\n",
            "Epoch 4, step (batch no.): 744 -- acc: 0.98, loss 0.06 -- iter 21000/55000, training for: 419.47s\n",
            "Epoch 4, step (batch no.): 746 -- acc: 0.98, loss 0.06 -- iter 21500/55000, training for: 420.53s\n",
            "Epoch 4, step (batch no.): 748 -- acc: 0.98, loss 0.06 -- iter 22000/55000, training for: 421.61s\n",
            "Epoch 4, step (batch no.): 750 -- acc: 0.98, loss 0.06 -- iter 22500/55000, training for: 422.71s\n",
            "Epoch 4, step (batch no.): 752 -- acc: 0.98, loss 0.06 -- iter 23000/55000, training for: 423.80s\n",
            "Epoch 4, step (batch no.): 754 -- acc: 0.98, loss 0.06 -- iter 23500/55000, training for: 424.88s\n",
            "Epoch 4, step (batch no.): 756 -- acc: 0.98, loss 0.06 -- iter 24000/55000, training for: 425.95s\n",
            "Epoch 4, step (batch no.): 758 -- acc: 0.98, loss 0.05 -- iter 24500/55000, training for: 427.04s\n",
            "Epoch 4, step (batch no.): 760 -- acc: 0.99, loss 0.05 -- iter 25000/55000, training for: 428.11s\n",
            "Epoch 4, step (batch no.): 762 -- acc: 0.99, loss 0.05 -- iter 25500/55000, training for: 429.20s\n",
            "Epoch 4, step (batch no.): 764 -- acc: 0.99, loss 0.05 -- iter 26000/55000, training for: 430.27s\n",
            "Epoch 4, step (batch no.): 766 -- acc: 0.98, loss 0.06 -- iter 26500/55000, training for: 431.34s\n",
            "Epoch 4, step (batch no.): 768 -- acc: 0.99, loss 0.05 -- iter 27000/55000, training for: 432.41s\n",
            "Epoch 4, step (batch no.): 770 -- acc: 0.99, loss 0.05 -- iter 27500/55000, training for: 433.49s\n",
            "Epoch 4, step (batch no.): 772 -- acc: 0.99, loss 0.05 -- iter 28000/55000, training for: 434.58s\n",
            "Epoch 4, step (batch no.): 774 -- acc: 0.99, loss 0.05 -- iter 28500/55000, training for: 435.64s\n",
            "Epoch 4, step (batch no.): 776 -- acc: 0.99, loss 0.05 -- iter 29000/55000, training for: 436.71s\n",
            "Epoch 4, step (batch no.): 778 -- acc: 0.99, loss 0.05 -- iter 29500/55000, training for: 437.81s\n",
            "Epoch 4, step (batch no.): 780 -- acc: 0.98, loss 0.05 -- iter 30000/55000, training for: 438.89s\n",
            "Epoch 4, step (batch no.): 782 -- acc: 0.98, loss 0.05 -- iter 30500/55000, training for: 439.95s\n",
            "Epoch 4, step (batch no.): 784 -- acc: 0.98, loss 0.06 -- iter 31000/55000, training for: 441.03s\n",
            "Epoch 4, step (batch no.): 786 -- acc: 0.99, loss 0.05 -- iter 31500/55000, training for: 442.11s\n",
            "Epoch 4, step (batch no.): 788 -- acc: 0.99, loss 0.05 -- iter 32000/55000, training for: 443.20s\n",
            "Epoch 4, step (batch no.): 790 -- acc: 0.98, loss 0.05 -- iter 32500/55000, training for: 444.25s\n",
            "Epoch 4, step (batch no.): 792 -- acc: 0.98, loss 0.06 -- iter 33000/55000, training for: 445.34s\n",
            "Epoch 4, step (batch no.): 794 -- acc: 0.98, loss 0.05 -- iter 33500/55000, training for: 446.43s\n",
            "Epoch 4, step (batch no.): 796 -- acc: 0.98, loss 0.05 -- iter 34000/55000, training for: 447.52s\n",
            "Epoch 4, step (batch no.): 798 -- acc: 0.98, loss 0.05 -- iter 34500/55000, training for: 448.59s\n",
            "Epoch 4, step (batch no.): 800 -- acc: 0.98, loss 0.05 -- iter 35000/55000, training for: 449.65s\n",
            "Epoch 4, step (batch no.): 802 -- acc: 0.98, loss 0.05 -- iter 35500/55000, training for: 450.72s\n",
            "Epoch 4, step (batch no.): 804 -- acc: 0.99, loss 0.05 -- iter 36000/55000, training for: 451.84s\n",
            "Epoch 4, step (batch no.): 806 -- acc: 0.98, loss 0.05 -- iter 36500/55000, training for: 452.91s\n",
            "Epoch 4, step (batch no.): 808 -- acc: 0.98, loss 0.05 -- iter 37000/55000, training for: 453.97s\n",
            "Epoch 4, step (batch no.): 810 -- acc: 0.98, loss 0.05 -- iter 37500/55000, training for: 455.05s\n",
            "Epoch 4, step (batch no.): 812 -- acc: 0.98, loss 0.06 -- iter 38000/55000, training for: 456.14s\n",
            "Epoch 4, step (batch no.): 814 -- acc: 0.98, loss 0.07 -- iter 38500/55000, training for: 457.19s\n",
            "Epoch 4, step (batch no.): 816 -- acc: 0.98, loss 0.07 -- iter 39000/55000, training for: 458.25s\n",
            "Epoch 4, step (batch no.): 818 -- acc: 0.98, loss 0.07 -- iter 39500/55000, training for: 459.31s\n",
            "Epoch 4, step (batch no.): 820 -- acc: 0.98, loss 0.07 -- iter 40000/55000, training for: 460.37s\n",
            "Epoch 4, step (batch no.): 822 -- acc: 0.98, loss 0.07 -- iter 40500/55000, training for: 461.45s\n",
            "Epoch 4, step (batch no.): 824 -- acc: 0.98, loss 0.06 -- iter 41000/55000, training for: 462.50s\n",
            "Epoch 4, step (batch no.): 826 -- acc: 0.98, loss 0.06 -- iter 41500/55000, training for: 463.57s\n",
            "Epoch 4, step (batch no.): 828 -- acc: 0.98, loss 0.05 -- iter 42000/55000, training for: 464.63s\n",
            "Epoch 4, step (batch no.): 830 -- acc: 0.98, loss 0.05 -- iter 42500/55000, training for: 465.71s\n",
            "Epoch 4, step (batch no.): 832 -- acc: 0.98, loss 0.05 -- iter 43000/55000, training for: 466.78s\n",
            "Epoch 4, step (batch no.): 834 -- acc: 0.98, loss 0.05 -- iter 43500/55000, training for: 467.87s\n",
            "Epoch 4, step (batch no.): 836 -- acc: 0.98, loss 0.07 -- iter 44000/55000, training for: 468.94s\n",
            "Epoch 4, step (batch no.): 838 -- acc: 0.98, loss 0.06 -- iter 44500/55000, training for: 470.00s\n",
            "Epoch 4, step (batch no.): 840 -- acc: 0.98, loss 0.07 -- iter 45000/55000, training for: 471.06s\n",
            "Epoch 4, step (batch no.): 842 -- acc: 0.98, loss 0.07 -- iter 45500/55000, training for: 472.12s\n",
            "Epoch 4, step (batch no.): 844 -- acc: 0.98, loss 0.07 -- iter 46000/55000, training for: 473.18s\n",
            "Epoch 4, step (batch no.): 846 -- acc: 0.98, loss 0.07 -- iter 46500/55000, training for: 474.26s\n",
            "Epoch 4, step (batch no.): 848 -- acc: 0.98, loss 0.07 -- iter 47000/55000, training for: 475.33s\n",
            "Epoch 4, step (batch no.): 850 -- acc: 0.98, loss 0.08 -- iter 47500/55000, training for: 476.41s\n",
            "Epoch 4, step (batch no.): 852 -- acc: 0.89, loss 1.29 -- iter 48000/55000, training for: 477.50s\n",
            "Epoch 4, step (batch no.): 854 -- acc: 0.90, loss 1.13 -- iter 48500/55000, training for: 478.57s\n",
            "Epoch 4, step (batch no.): 856 -- acc: 0.91, loss 0.98 -- iter 49000/55000, training for: 479.65s\n",
            "Epoch 4, step (batch no.): 858 -- acc: 0.92, loss 0.82 -- iter 49500/55000, training for: 480.71s\n",
            "Epoch 4, step (batch no.): 860 -- acc: 0.93, loss 0.68 -- iter 50000/55000, training for: 481.77s\n",
            "Epoch 4, step (batch no.): 862 -- acc: 0.94, loss 0.57 -- iter 50500/55000, training for: 482.84s\n",
            "Epoch 4, step (batch no.): 864 -- acc: 0.95, loss 0.47 -- iter 51000/55000, training for: 483.97s\n",
            "Epoch 4, step (batch no.): 866 -- acc: 0.95, loss 0.43 -- iter 51500/55000, training for: 485.09s\n",
            "Epoch 4, step (batch no.): 868 -- acc: 0.96, loss 0.38 -- iter 52000/55000, training for: 486.18s\n",
            "Epoch 4, step (batch no.): 870 -- acc: 0.96, loss 0.33 -- iter 52500/55000, training for: 487.25s\n",
            "Epoch 4, step (batch no.): 872 -- acc: 0.96, loss 0.29 -- iter 53000/55000, training for: 488.33s\n",
            "Epoch 4, step (batch no.): 874 -- acc: 0.97, loss 0.25 -- iter 53500/55000, training for: 489.40s\n",
            "Epoch 4, step (batch no.): 876 -- acc: 0.97, loss 0.22 -- iter 54000/55000, training for: 490.45s\n",
            "Epoch 4, step (batch no.): 878 -- acc: 0.97, loss 0.21 -- iter 54500/55000, training for: 491.51s\n",
            "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.17693\u001b[0m\u001b[0m | time: 123.834s\n",
            "| Adam | epoch: 004 | loss: 0.17693 - acc: 0.9691 | val_loss: 0.08922 - val_acc: 0.9736 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 4, step (batch no.): 880 -- acc: 0.97, loss 0.18 -- iter 55000/55000, training for: 497.98s\n",
            "Epoch 5, step (batch no.): 882 -- acc: 0.97, loss 0.17 -- iter 00500/55000, training for: 499.07s\n",
            "Epoch 5, step (batch no.): 884 -- acc: 0.97, loss 0.15 -- iter 01000/55000, training for: 500.13s\n",
            "Epoch 5, step (batch no.): 886 -- acc: 0.97, loss 0.13 -- iter 01500/55000, training for: 501.18s\n",
            "Epoch 5, step (batch no.): 888 -- acc: 0.97, loss 0.12 -- iter 02000/55000, training for: 502.24s\n",
            "Epoch 5, step (batch no.): 890 -- acc: 0.98, loss 0.10 -- iter 02500/55000, training for: 503.35s\n",
            "Epoch 5, step (batch no.): 892 -- acc: 0.98, loss 0.10 -- iter 03000/55000, training for: 504.41s\n",
            "Epoch 5, step (batch no.): 894 -- acc: 0.98, loss 0.09 -- iter 03500/55000, training for: 505.46s\n",
            "Epoch 5, step (batch no.): 896 -- acc: 0.98, loss 0.08 -- iter 04000/55000, training for: 506.53s\n",
            "Epoch 5, step (batch no.): 898 -- acc: 0.98, loss 0.08 -- iter 04500/55000, training for: 507.59s\n",
            "Epoch 5, step (batch no.): 900 -- acc: 0.98, loss 0.07 -- iter 05000/55000, training for: 508.65s\n",
            "Epoch 5, step (batch no.): 902 -- acc: 0.98, loss 0.07 -- iter 05500/55000, training for: 509.72s\n",
            "Epoch 5, step (batch no.): 904 -- acc: 0.98, loss 0.07 -- iter 06000/55000, training for: 510.78s\n",
            "Epoch 5, step (batch no.): 906 -- acc: 0.98, loss 0.07 -- iter 06500/55000, training for: 511.87s\n",
            "Epoch 5, step (batch no.): 908 -- acc: 0.98, loss 0.07 -- iter 07000/55000, training for: 512.94s\n",
            "Epoch 5, step (batch no.): 910 -- acc: 0.98, loss 0.06 -- iter 07500/55000, training for: 514.03s\n",
            "Epoch 5, step (batch no.): 912 -- acc: 0.98, loss 0.06 -- iter 08000/55000, training for: 515.13s\n",
            "Epoch 5, step (batch no.): 914 -- acc: 0.98, loss 0.06 -- iter 08500/55000, training for: 516.27s\n",
            "Epoch 5, step (batch no.): 916 -- acc: 0.98, loss 0.06 -- iter 09000/55000, training for: 517.38s\n",
            "Epoch 5, step (batch no.): 918 -- acc: 0.98, loss 0.06 -- iter 09500/55000, training for: 518.47s\n",
            "Epoch 5, step (batch no.): 920 -- acc: 0.98, loss 0.05 -- iter 10000/55000, training for: 519.55s\n",
            "Epoch 5, step (batch no.): 922 -- acc: 0.98, loss 0.05 -- iter 10500/55000, training for: 520.64s\n",
            "Epoch 5, step (batch no.): 924 -- acc: 0.99, loss 0.05 -- iter 11000/55000, training for: 521.72s\n",
            "Epoch 5, step (batch no.): 926 -- acc: 0.98, loss 0.06 -- iter 11500/55000, training for: 522.79s\n",
            "Epoch 5, step (batch no.): 928 -- acc: 0.99, loss 0.05 -- iter 12000/55000, training for: 523.87s\n",
            "Epoch 5, step (batch no.): 930 -- acc: 0.99, loss 0.05 -- iter 12500/55000, training for: 524.92s\n",
            "Epoch 5, step (batch no.): 932 -- acc: 0.98, loss 0.05 -- iter 13000/55000, training for: 525.97s\n",
            "Epoch 5, step (batch no.): 934 -- acc: 0.98, loss 0.05 -- iter 13500/55000, training for: 527.03s\n",
            "Epoch 5, step (batch no.): 936 -- acc: 0.99, loss 0.05 -- iter 14000/55000, training for: 528.11s\n",
            "Epoch 5, step (batch no.): 938 -- acc: 0.99, loss 0.05 -- iter 14500/55000, training for: 529.16s\n",
            "Epoch 5, step (batch no.): 940 -- acc: 0.98, loss 0.05 -- iter 15000/55000, training for: 530.23s\n",
            "Epoch 5, step (batch no.): 942 -- acc: 0.98, loss 0.05 -- iter 15500/55000, training for: 531.31s\n",
            "Epoch 5, step (batch no.): 944 -- acc: 0.98, loss 0.05 -- iter 16000/55000, training for: 532.38s\n",
            "Epoch 5, step (batch no.): 946 -- acc: 0.99, loss 0.05 -- iter 16500/55000, training for: 533.45s\n",
            "Epoch 5, step (batch no.): 948 -- acc: 0.98, loss 0.05 -- iter 17000/55000, training for: 534.50s\n",
            "Epoch 5, step (batch no.): 950 -- acc: 0.98, loss 0.05 -- iter 17500/55000, training for: 535.55s\n",
            "Epoch 5, step (batch no.): 952 -- acc: 0.98, loss 0.05 -- iter 18000/55000, training for: 536.62s\n",
            "Epoch 5, step (batch no.): 954 -- acc: 0.98, loss 0.06 -- iter 18500/55000, training for: 537.71s\n",
            "Epoch 5, step (batch no.): 956 -- acc: 0.98, loss 0.06 -- iter 19000/55000, training for: 538.77s\n",
            "Epoch 5, step (batch no.): 958 -- acc: 0.98, loss 0.06 -- iter 19500/55000, training for: 539.82s\n",
            "Epoch 5, step (batch no.): 960 -- acc: 0.98, loss 0.05 -- iter 20000/55000, training for: 540.88s\n",
            "Epoch 5, step (batch no.): 962 -- acc: 0.98, loss 0.05 -- iter 20500/55000, training for: 541.95s\n",
            "Epoch 5, step (batch no.): 964 -- acc: 0.99, loss 0.05 -- iter 21000/55000, training for: 543.01s\n",
            "Epoch 5, step (batch no.): 966 -- acc: 0.99, loss 0.05 -- iter 21500/55000, training for: 544.12s\n",
            "Epoch 5, step (batch no.): 968 -- acc: 0.99, loss 0.05 -- iter 22000/55000, training for: 545.20s\n",
            "Epoch 5, step (batch no.): 970 -- acc: 0.98, loss 0.05 -- iter 22500/55000, training for: 546.29s\n",
            "Epoch 5, step (batch no.): 972 -- acc: 0.98, loss 0.06 -- iter 23000/55000, training for: 547.38s\n",
            "Epoch 5, step (batch no.): 974 -- acc: 0.98, loss 0.06 -- iter 23500/55000, training for: 548.46s\n",
            "Epoch 5, step (batch no.): 976 -- acc: 0.98, loss 0.06 -- iter 24000/55000, training for: 549.55s\n",
            "Epoch 5, step (batch no.): 978 -- acc: 0.98, loss 0.05 -- iter 24500/55000, training for: 550.63s\n",
            "Epoch 5, step (batch no.): 980 -- acc: 0.98, loss 0.05 -- iter 25000/55000, training for: 551.79s\n",
            "Epoch 5, step (batch no.): 982 -- acc: 0.98, loss 0.05 -- iter 25500/55000, training for: 552.86s\n",
            "Epoch 5, step (batch no.): 984 -- acc: 0.98, loss 0.05 -- iter 26000/55000, training for: 553.91s\n",
            "Epoch 5, step (batch no.): 986 -- acc: 0.99, loss 0.05 -- iter 26500/55000, training for: 554.97s\n",
            "Epoch 5, step (batch no.): 988 -- acc: 0.99, loss 0.04 -- iter 27000/55000, training for: 556.03s\n",
            "Epoch 5, step (batch no.): 990 -- acc: 0.99, loss 0.04 -- iter 27500/55000, training for: 557.15s\n",
            "Epoch 5, step (batch no.): 992 -- acc: 0.99, loss 0.04 -- iter 28000/55000, training for: 558.27s\n",
            "Epoch 5, step (batch no.): 994 -- acc: 0.99, loss 0.05 -- iter 28500/55000, training for: 559.37s\n",
            "Epoch 5, step (batch no.): 996 -- acc: 0.99, loss 0.05 -- iter 29000/55000, training for: 560.44s\n",
            "Epoch 5, step (batch no.): 998 -- acc: 0.99, loss 0.04 -- iter 29500/55000, training for: 561.51s\n",
            "Epoch 5, step (batch no.): 1000 -- acc: 0.99, loss 0.04 -- iter 30000/55000, training for: 562.59s\n",
            "Epoch 5, step (batch no.): 1002 -- acc: 0.99, loss 0.04 -- iter 30500/55000, training for: 563.66s\n",
            "Epoch 5, step (batch no.): 1004 -- acc: 0.99, loss 0.04 -- iter 31000/55000, training for: 564.71s\n",
            "Epoch 5, step (batch no.): 1006 -- acc: 0.99, loss 0.04 -- iter 31500/55000, training for: 565.79s\n",
            "Epoch 5, step (batch no.): 1008 -- acc: 0.99, loss 0.04 -- iter 32000/55000, training for: 566.86s\n",
            "Epoch 5, step (batch no.): 1010 -- acc: 0.99, loss 0.05 -- iter 32500/55000, training for: 567.91s\n",
            "Epoch 5, step (batch no.): 1012 -- acc: 0.99, loss 0.04 -- iter 33000/55000, training for: 568.98s\n",
            "Epoch 5, step (batch no.): 1014 -- acc: 0.99, loss 0.05 -- iter 33500/55000, training for: 570.03s\n",
            "Epoch 5, step (batch no.): 1016 -- acc: 0.99, loss 0.05 -- iter 34000/55000, training for: 571.09s\n",
            "Epoch 5, step (batch no.): 1018 -- acc: 0.99, loss 0.04 -- iter 34500/55000, training for: 572.15s\n",
            "Epoch 5, step (batch no.): 1020 -- acc: 0.99, loss 0.05 -- iter 35000/55000, training for: 573.22s\n",
            "Epoch 5, step (batch no.): 1022 -- acc: 0.99, loss 0.05 -- iter 35500/55000, training for: 574.30s\n",
            "Epoch 5, step (batch no.): 1024 -- acc: 0.99, loss 0.05 -- iter 36000/55000, training for: 575.37s\n",
            "Epoch 5, step (batch no.): 1026 -- acc: 0.99, loss 0.05 -- iter 36500/55000, training for: 576.44s\n",
            "Epoch 5, step (batch no.): 1028 -- acc: 0.99, loss 0.05 -- iter 37000/55000, training for: 577.51s\n",
            "Epoch 5, step (batch no.): 1030 -- acc: 0.99, loss 0.04 -- iter 37500/55000, training for: 578.59s\n",
            "Epoch 5, step (batch no.): 1032 -- acc: 0.99, loss 0.04 -- iter 38000/55000, training for: 579.67s\n",
            "Epoch 5, step (batch no.): 1034 -- acc: 0.99, loss 0.04 -- iter 38500/55000, training for: 580.75s\n",
            "Epoch 5, step (batch no.): 1036 -- acc: 0.99, loss 0.04 -- iter 39000/55000, training for: 581.84s\n",
            "Epoch 5, step (batch no.): 1038 -- acc: 0.99, loss 0.04 -- iter 39500/55000, training for: 582.93s\n",
            "Epoch 5, step (batch no.): 1040 -- acc: 0.99, loss 0.04 -- iter 40000/55000, training for: 584.07s\n",
            "Epoch 5, step (batch no.): 1042 -- acc: 0.98, loss 0.05 -- iter 40500/55000, training for: 585.13s\n",
            "Epoch 5, step (batch no.): 1044 -- acc: 0.98, loss 0.05 -- iter 41000/55000, training for: 586.19s\n",
            "Epoch 5, step (batch no.): 1046 -- acc: 0.98, loss 0.05 -- iter 41500/55000, training for: 587.25s\n",
            "Epoch 5, step (batch no.): 1048 -- acc: 0.98, loss 0.05 -- iter 42000/55000, training for: 588.30s\n",
            "Epoch 5, step (batch no.): 1050 -- acc: 0.99, loss 0.05 -- iter 42500/55000, training for: 589.38s\n",
            "Epoch 5, step (batch no.): 1052 -- acc: 0.98, loss 0.05 -- iter 43000/55000, training for: 590.43s\n",
            "Epoch 5, step (batch no.): 1054 -- acc: 0.98, loss 0.05 -- iter 43500/55000, training for: 591.49s\n",
            "Epoch 5, step (batch no.): 1056 -- acc: 0.98, loss 0.06 -- iter 44000/55000, training for: 592.54s\n",
            "Epoch 5, step (batch no.): 1058 -- acc: 0.98, loss 0.05 -- iter 44500/55000, training for: 593.61s\n",
            "Epoch 5, step (batch no.): 1060 -- acc: 0.98, loss 0.05 -- iter 45000/55000, training for: 594.69s\n",
            "Epoch 5, step (batch no.): 1062 -- acc: 0.98, loss 0.05 -- iter 45500/55000, training for: 595.77s\n",
            "Epoch 5, step (batch no.): 1064 -- acc: 0.98, loss 0.05 -- iter 46000/55000, training for: 596.84s\n",
            "Epoch 5, step (batch no.): 1066 -- acc: 0.98, loss 0.05 -- iter 46500/55000, training for: 597.91s\n",
            "Epoch 5, step (batch no.): 1068 -- acc: 0.99, loss 0.05 -- iter 47000/55000, training for: 598.99s\n",
            "Epoch 5, step (batch no.): 1070 -- acc: 0.98, loss 0.05 -- iter 47500/55000, training for: 600.09s\n",
            "Epoch 5, step (batch no.): 1072 -- acc: 0.99, loss 0.05 -- iter 48000/55000, training for: 601.17s\n",
            "Epoch 5, step (batch no.): 1074 -- acc: 0.89, loss 1.17 -- iter 48500/55000, training for: 602.24s\n",
            "Epoch 5, step (batch no.): 1076 -- acc: 0.90, loss 1.02 -- iter 49000/55000, training for: 603.33s\n",
            "Epoch 5, step (batch no.): 1078 -- acc: 0.92, loss 0.85 -- iter 49500/55000, training for: 604.43s\n",
            "Epoch 5, step (batch no.): 1080 -- acc: 0.93, loss 0.71 -- iter 50000/55000, training for: 605.48s\n",
            "Epoch 5, step (batch no.): 1082 -- acc: 0.94, loss 0.59 -- iter 50500/55000, training for: 606.57s\n",
            "Epoch 5, step (batch no.): 1084 -- acc: 0.94, loss 0.49 -- iter 51000/55000, training for: 607.65s\n",
            "Epoch 5, step (batch no.): 1086 -- acc: 0.95, loss 0.41 -- iter 51500/55000, training for: 608.72s\n",
            "Epoch 5, step (batch no.): 1088 -- acc: 0.96, loss 0.34 -- iter 52000/55000, training for: 609.78s\n",
            "Epoch 5, step (batch no.): 1090 -- acc: 0.96, loss 0.30 -- iter 52500/55000, training for: 610.86s\n",
            "Epoch 5, step (batch no.): 1092 -- acc: 0.97, loss 0.25 -- iter 53000/55000, training for: 611.94s\n",
            "Epoch 5, step (batch no.): 1094 -- acc: 0.97, loss 0.21 -- iter 53500/55000, training for: 613.02s\n",
            "Epoch 5, step (batch no.): 1096 -- acc: 0.97, loss 0.18 -- iter 54000/55000, training for: 614.10s\n",
            "Epoch 5, step (batch no.): 1098 -- acc: 0.97, loss 0.17 -- iter 54500/55000, training for: 615.16s\n",
            "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.15126\u001b[0m\u001b[0m | time: 123.658s\n",
            "| Adam | epoch: 005 | loss: 0.15126 - acc: 0.9717 | val_loss: 0.08140 - val_acc: 0.9760 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 5, step (batch no.): 1100 -- acc: 0.97, loss 0.15 -- iter 55000/55000, training for: 621.69s\n",
            "Epoch 6, step (batch no.): 1102 -- acc: 0.97, loss 0.14 -- iter 00500/55000, training for: 622.75s\n",
            "Epoch 6, step (batch no.): 1104 -- acc: 0.97, loss 0.13 -- iter 01000/55000, training for: 623.80s\n",
            "Epoch 6, step (batch no.): 1106 -- acc: 0.97, loss 0.12 -- iter 01500/55000, training for: 624.87s\n",
            "Epoch 6, step (batch no.): 1108 -- acc: 0.98, loss 0.11 -- iter 02000/55000, training for: 625.92s\n",
            "Epoch 6, step (batch no.): 1110 -- acc: 0.98, loss 0.09 -- iter 02500/55000, training for: 626.98s\n",
            "Epoch 6, step (batch no.): 1112 -- acc: 0.98, loss 0.09 -- iter 03000/55000, training for: 628.03s\n",
            "Epoch 6, step (batch no.): 1114 -- acc: 0.98, loss 0.09 -- iter 03500/55000, training for: 629.09s\n",
            "Epoch 6, step (batch no.): 1116 -- acc: 0.98, loss 0.08 -- iter 04000/55000, training for: 630.14s\n",
            "Epoch 6, step (batch no.): 1118 -- acc: 0.98, loss 0.07 -- iter 04500/55000, training for: 631.22s\n",
            "Epoch 6, step (batch no.): 1120 -- acc: 0.98, loss 0.08 -- iter 05000/55000, training for: 632.29s\n",
            "Epoch 6, step (batch no.): 1122 -- acc: 0.98, loss 0.07 -- iter 05500/55000, training for: 633.36s\n",
            "Epoch 6, step (batch no.): 1124 -- acc: 0.98, loss 0.06 -- iter 06000/55000, training for: 634.41s\n",
            "Epoch 6, step (batch no.): 1126 -- acc: 0.98, loss 0.06 -- iter 06500/55000, training for: 635.49s\n",
            "Epoch 6, step (batch no.): 1128 -- acc: 0.98, loss 0.06 -- iter 07000/55000, training for: 636.58s\n",
            "Epoch 6, step (batch no.): 1130 -- acc: 0.98, loss 0.06 -- iter 07500/55000, training for: 637.65s\n",
            "Epoch 6, step (batch no.): 1132 -- acc: 0.98, loss 0.06 -- iter 08000/55000, training for: 638.72s\n",
            "Epoch 6, step (batch no.): 1134 -- acc: 0.98, loss 0.06 -- iter 08500/55000, training for: 639.79s\n",
            "Epoch 6, step (batch no.): 1136 -- acc: 0.98, loss 0.06 -- iter 09000/55000, training for: 640.88s\n",
            "Epoch 6, step (batch no.): 1138 -- acc: 0.98, loss 0.05 -- iter 09500/55000, training for: 641.94s\n",
            "Epoch 6, step (batch no.): 1140 -- acc: 0.98, loss 0.05 -- iter 10000/55000, training for: 643.02s\n",
            "Epoch 6, step (batch no.): 1142 -- acc: 0.98, loss 0.05 -- iter 10500/55000, training for: 644.11s\n",
            "Epoch 6, step (batch no.): 1144 -- acc: 0.99, loss 0.05 -- iter 11000/55000, training for: 645.18s\n",
            "Epoch 6, step (batch no.): 1146 -- acc: 0.99, loss 0.05 -- iter 11500/55000, training for: 646.30s\n",
            "Epoch 6, step (batch no.): 1148 -- acc: 0.99, loss 0.04 -- iter 12000/55000, training for: 647.37s\n",
            "Epoch 6, step (batch no.): 1150 -- acc: 0.99, loss 0.04 -- iter 12500/55000, training for: 648.44s\n",
            "Epoch 6, step (batch no.): 1152 -- acc: 0.99, loss 0.04 -- iter 13000/55000, training for: 649.51s\n",
            "Epoch 6, step (batch no.): 1154 -- acc: 0.99, loss 0.04 -- iter 13500/55000, training for: 650.62s\n",
            "Epoch 6, step (batch no.): 1156 -- acc: 0.99, loss 0.04 -- iter 14000/55000, training for: 651.70s\n",
            "Epoch 6, step (batch no.): 1158 -- acc: 0.99, loss 0.05 -- iter 14500/55000, training for: 652.77s\n",
            "Epoch 6, step (batch no.): 1160 -- acc: 0.99, loss 0.05 -- iter 15000/55000, training for: 653.84s\n",
            "Epoch 6, step (batch no.): 1162 -- acc: 0.99, loss 0.05 -- iter 15500/55000, training for: 654.90s\n",
            "Epoch 6, step (batch no.): 1164 -- acc: 0.99, loss 0.05 -- iter 16000/55000, training for: 655.95s\n",
            "Epoch 6, step (batch no.): 1166 -- acc: 0.99, loss 0.05 -- iter 16500/55000, training for: 657.04s\n",
            "Epoch 6, step (batch no.): 1168 -- acc: 0.99, loss 0.05 -- iter 17000/55000, training for: 658.11s\n",
            "Epoch 6, step (batch no.): 1170 -- acc: 0.99, loss 0.04 -- iter 17500/55000, training for: 659.17s\n",
            "Epoch 6, step (batch no.): 1172 -- acc: 0.99, loss 0.05 -- iter 18000/55000, training for: 660.23s\n",
            "Epoch 6, step (batch no.): 1174 -- acc: 0.99, loss 0.05 -- iter 18500/55000, training for: 661.33s\n",
            "Epoch 6, step (batch no.): 1176 -- acc: 0.99, loss 0.05 -- iter 19000/55000, training for: 662.40s\n",
            "Epoch 6, step (batch no.): 1178 -- acc: 0.99, loss 0.05 -- iter 19500/55000, training for: 663.47s\n",
            "Epoch 6, step (batch no.): 1180 -- acc: 0.99, loss 0.05 -- iter 20000/55000, training for: 664.60s\n",
            "Epoch 6, step (batch no.): 1182 -- acc: 0.99, loss 0.05 -- iter 20500/55000, training for: 665.69s\n",
            "Epoch 6, step (batch no.): 1184 -- acc: 0.99, loss 0.04 -- iter 21000/55000, training for: 666.76s\n",
            "Epoch 6, step (batch no.): 1186 -- acc: 0.99, loss 0.04 -- iter 21500/55000, training for: 667.86s\n",
            "Epoch 6, step (batch no.): 1188 -- acc: 0.99, loss 0.04 -- iter 22000/55000, training for: 668.93s\n",
            "Epoch 6, step (batch no.): 1190 -- acc: 0.99, loss 0.04 -- iter 22500/55000, training for: 670.00s\n",
            "Epoch 6, step (batch no.): 1192 -- acc: 0.99, loss 0.04 -- iter 23000/55000, training for: 671.07s\n",
            "Epoch 6, step (batch no.): 1194 -- acc: 0.99, loss 0.04 -- iter 23500/55000, training for: 672.15s\n",
            "Epoch 6, step (batch no.): 1196 -- acc: 0.99, loss 0.05 -- iter 24000/55000, training for: 673.22s\n",
            "Epoch 6, step (batch no.): 1198 -- acc: 0.99, loss 0.04 -- iter 24500/55000, training for: 674.29s\n",
            "Epoch 6, step (batch no.): 1200 -- acc: 0.99, loss 0.05 -- iter 25000/55000, training for: 675.38s\n",
            "Epoch 6, step (batch no.): 1202 -- acc: 0.99, loss 0.05 -- iter 25500/55000, training for: 676.45s\n",
            "Epoch 6, step (batch no.): 1204 -- acc: 0.99, loss 0.05 -- iter 26000/55000, training for: 677.56s\n",
            "Epoch 6, step (batch no.): 1206 -- acc: 0.99, loss 0.04 -- iter 26500/55000, training for: 678.64s\n",
            "Epoch 6, step (batch no.): 1208 -- acc: 0.99, loss 0.04 -- iter 27000/55000, training for: 679.74s\n",
            "Epoch 6, step (batch no.): 1210 -- acc: 0.99, loss 0.05 -- iter 27500/55000, training for: 680.83s\n",
            "Epoch 6, step (batch no.): 1212 -- acc: 0.99, loss 0.05 -- iter 28000/55000, training for: 681.94s\n",
            "Epoch 6, step (batch no.): 1214 -- acc: 0.99, loss 0.05 -- iter 28500/55000, training for: 683.10s\n",
            "Epoch 6, step (batch no.): 1216 -- acc: 0.99, loss 0.05 -- iter 29000/55000, training for: 684.17s\n",
            "Epoch 6, step (batch no.): 1218 -- acc: 0.99, loss 0.04 -- iter 29500/55000, training for: 685.27s\n",
            "Epoch 6, step (batch no.): 1220 -- acc: 0.99, loss 0.04 -- iter 30000/55000, training for: 686.34s\n",
            "Epoch 6, step (batch no.): 1222 -- acc: 0.99, loss 0.04 -- iter 30500/55000, training for: 687.41s\n",
            "Epoch 6, step (batch no.): 1224 -- acc: 0.99, loss 0.04 -- iter 31000/55000, training for: 688.47s\n",
            "Epoch 6, step (batch no.): 1226 -- acc: 0.99, loss 0.04 -- iter 31500/55000, training for: 689.54s\n",
            "Epoch 6, step (batch no.): 1228 -- acc: 0.99, loss 0.03 -- iter 32000/55000, training for: 690.60s\n",
            "Epoch 6, step (batch no.): 1230 -- acc: 0.99, loss 0.03 -- iter 32500/55000, training for: 691.66s\n",
            "Epoch 6, step (batch no.): 1232 -- acc: 0.99, loss 0.04 -- iter 33000/55000, training for: 692.74s\n",
            "Epoch 6, step (batch no.): 1234 -- acc: 0.99, loss 0.05 -- iter 33500/55000, training for: 693.81s\n",
            "Epoch 6, step (batch no.): 1236 -- acc: 0.99, loss 0.05 -- iter 34000/55000, training for: 694.87s\n",
            "Epoch 6, step (batch no.): 1238 -- acc: 0.99, loss 0.05 -- iter 34500/55000, training for: 695.93s\n",
            "Epoch 6, step (batch no.): 1240 -- acc: 0.99, loss 0.05 -- iter 35000/55000, training for: 697.00s\n",
            "Epoch 6, step (batch no.): 1242 -- acc: 0.98, loss 0.05 -- iter 35500/55000, training for: 698.06s\n",
            "Epoch 6, step (batch no.): 1244 -- acc: 0.98, loss 0.05 -- iter 36000/55000, training for: 699.12s\n",
            "Epoch 6, step (batch no.): 1246 -- acc: 0.98, loss 0.05 -- iter 36500/55000, training for: 700.19s\n",
            "Epoch 6, step (batch no.): 1248 -- acc: 0.99, loss 0.04 -- iter 37000/55000, training for: 701.26s\n",
            "Epoch 6, step (batch no.): 1250 -- acc: 0.99, loss 0.04 -- iter 37500/55000, training for: 702.33s\n",
            "Epoch 6, step (batch no.): 1252 -- acc: 0.99, loss 0.04 -- iter 38000/55000, training for: 703.42s\n",
            "Epoch 6, step (batch no.): 1254 -- acc: 0.99, loss 0.05 -- iter 38500/55000, training for: 704.49s\n",
            "Epoch 6, step (batch no.): 1256 -- acc: 0.99, loss 0.04 -- iter 39000/55000, training for: 705.55s\n",
            "Epoch 6, step (batch no.): 1258 -- acc: 0.99, loss 0.05 -- iter 39500/55000, training for: 706.61s\n",
            "Epoch 6, step (batch no.): 1260 -- acc: 0.99, loss 0.05 -- iter 40000/55000, training for: 707.67s\n",
            "Epoch 6, step (batch no.): 1262 -- acc: 0.99, loss 0.05 -- iter 40500/55000, training for: 708.74s\n",
            "Epoch 6, step (batch no.): 1264 -- acc: 0.99, loss 0.05 -- iter 41000/55000, training for: 709.82s\n",
            "Epoch 6, step (batch no.): 1266 -- acc: 0.98, loss 0.05 -- iter 41500/55000, training for: 710.89s\n",
            "Epoch 6, step (batch no.): 1268 -- acc: 0.98, loss 0.05 -- iter 42000/55000, training for: 711.95s\n",
            "Epoch 6, step (batch no.): 1270 -- acc: 0.98, loss 0.06 -- iter 42500/55000, training for: 713.04s\n",
            "Epoch 6, step (batch no.): 1272 -- acc: 0.98, loss 0.05 -- iter 43000/55000, training for: 714.10s\n",
            "Epoch 6, step (batch no.): 1274 -- acc: 0.99, loss 0.04 -- iter 43500/55000, training for: 715.15s\n",
            "Epoch 6, step (batch no.): 1276 -- acc: 0.99, loss 0.04 -- iter 44000/55000, training for: 716.29s\n",
            "Epoch 6, step (batch no.): 1278 -- acc: 0.99, loss 0.04 -- iter 44500/55000, training for: 717.36s\n",
            "Epoch 6, step (batch no.): 1280 -- acc: 0.99, loss 0.04 -- iter 45000/55000, training for: 718.44s\n",
            "Epoch 6, step (batch no.): 1282 -- acc: 0.99, loss 0.04 -- iter 45500/55000, training for: 719.50s\n",
            "Epoch 6, step (batch no.): 1284 -- acc: 0.99, loss 0.03 -- iter 46000/55000, training for: 720.57s\n",
            "Epoch 6, step (batch no.): 1286 -- acc: 0.99, loss 0.04 -- iter 46500/55000, training for: 721.63s\n",
            "Epoch 6, step (batch no.): 1288 -- acc: 0.99, loss 0.03 -- iter 47000/55000, training for: 722.70s\n",
            "Epoch 6, step (batch no.): 1290 -- acc: 0.99, loss 0.03 -- iter 47500/55000, training for: 723.82s\n",
            "Epoch 6, step (batch no.): 1292 -- acc: 0.99, loss 0.03 -- iter 48000/55000, training for: 724.89s\n",
            "Epoch 6, step (batch no.): 1294 -- acc: 0.90, loss 1.57 -- iter 48500/55000, training for: 725.95s\n",
            "Epoch 6, step (batch no.): 1296 -- acc: 0.91, loss 1.30 -- iter 49000/55000, training for: 727.04s\n",
            "Epoch 6, step (batch no.): 1298 -- acc: 0.92, loss 1.12 -- iter 49500/55000, training for: 728.10s\n",
            "Epoch 6, step (batch no.): 1300 -- acc: 0.93, loss 0.95 -- iter 50000/55000, training for: 729.17s\n",
            "Epoch 6, step (batch no.): 1302 -- acc: 0.94, loss 0.79 -- iter 50500/55000, training for: 730.23s\n",
            "Epoch 6, step (batch no.): 1304 -- acc: 0.95, loss 0.66 -- iter 51000/55000, training for: 731.31s\n",
            "Epoch 6, step (batch no.): 1306 -- acc: 0.96, loss 0.54 -- iter 51500/55000, training for: 732.41s\n",
            "Epoch 6, step (batch no.): 1308 -- acc: 0.96, loss 0.45 -- iter 52000/55000, training for: 733.49s\n",
            "Epoch 6, step (batch no.): 1310 -- acc: 0.96, loss 0.38 -- iter 52500/55000, training for: 734.56s\n",
            "Epoch 6, step (batch no.): 1312 -- acc: 0.97, loss 0.31 -- iter 53000/55000, training for: 735.61s\n",
            "Epoch 6, step (batch no.): 1314 -- acc: 0.97, loss 0.27 -- iter 53500/55000, training for: 736.69s\n",
            "Epoch 6, step (batch no.): 1316 -- acc: 0.97, loss 0.23 -- iter 54000/55000, training for: 737.75s\n",
            "Epoch 6, step (batch no.): 1318 -- acc: 0.98, loss 0.19 -- iter 54500/55000, training for: 738.85s\n",
            "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.17012\u001b[0m\u001b[0m | time: 123.665s\n",
            "| Adam | epoch: 006 | loss: 0.17012 - acc: 0.9763 | val_loss: 0.08526 - val_acc: 0.9802 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 6, step (batch no.): 1320 -- acc: 0.98, loss 0.17 -- iter 55000/55000, training for: 745.40s\n",
            "Epoch 7, step (batch no.): 1322 -- acc: 0.98, loss 0.15 -- iter 00500/55000, training for: 746.52s\n",
            "Epoch 7, step (batch no.): 1324 -- acc: 0.98, loss 0.13 -- iter 01000/55000, training for: 747.60s\n",
            "Epoch 7, step (batch no.): 1326 -- acc: 0.98, loss 0.13 -- iter 01500/55000, training for: 748.72s\n",
            "Epoch 7, step (batch no.): 1328 -- acc: 0.97, loss 0.13 -- iter 02000/55000, training for: 749.84s\n",
            "Epoch 7, step (batch no.): 1330 -- acc: 0.97, loss 0.11 -- iter 02500/55000, training for: 750.89s\n",
            "Epoch 7, step (batch no.): 1332 -- acc: 0.97, loss 0.12 -- iter 03000/55000, training for: 751.95s\n",
            "Epoch 7, step (batch no.): 1334 -- acc: 0.97, loss 0.11 -- iter 03500/55000, training for: 753.00s\n",
            "Epoch 7, step (batch no.): 1336 -- acc: 0.97, loss 0.11 -- iter 04000/55000, training for: 754.10s\n",
            "Epoch 7, step (batch no.): 1338 -- acc: 0.97, loss 0.10 -- iter 04500/55000, training for: 755.20s\n",
            "Epoch 7, step (batch no.): 1340 -- acc: 0.97, loss 0.10 -- iter 05000/55000, training for: 756.27s\n",
            "Epoch 7, step (batch no.): 1342 -- acc: 0.97, loss 0.09 -- iter 05500/55000, training for: 757.37s\n",
            "Epoch 7, step (batch no.): 1344 -- acc: 0.98, loss 0.08 -- iter 06000/55000, training for: 758.45s\n",
            "Epoch 7, step (batch no.): 1346 -- acc: 0.98, loss 0.09 -- iter 06500/55000, training for: 759.53s\n",
            "Epoch 7, step (batch no.): 1348 -- acc: 0.97, loss 0.09 -- iter 07000/55000, training for: 760.60s\n",
            "Epoch 7, step (batch no.): 1350 -- acc: 0.98, loss 0.08 -- iter 07500/55000, training for: 761.70s\n",
            "Epoch 7, step (batch no.): 1352 -- acc: 0.98, loss 0.08 -- iter 08000/55000, training for: 762.80s\n",
            "Epoch 7, step (batch no.): 1354 -- acc: 0.97, loss 0.08 -- iter 08500/55000, training for: 763.89s\n",
            "Epoch 7, step (batch no.): 1356 -- acc: 0.97, loss 0.08 -- iter 09000/55000, training for: 764.97s\n",
            "Epoch 7, step (batch no.): 1358 -- acc: 0.98, loss 0.08 -- iter 09500/55000, training for: 766.07s\n",
            "Epoch 7, step (batch no.): 1360 -- acc: 0.98, loss 0.07 -- iter 10000/55000, training for: 767.13s\n",
            "Epoch 7, step (batch no.): 1362 -- acc: 0.98, loss 0.08 -- iter 10500/55000, training for: 768.19s\n",
            "Epoch 7, step (batch no.): 1364 -- acc: 0.98, loss 0.07 -- iter 11000/55000, training for: 769.26s\n",
            "Epoch 7, step (batch no.): 1366 -- acc: 0.98, loss 0.07 -- iter 11500/55000, training for: 770.32s\n",
            "Epoch 7, step (batch no.): 1368 -- acc: 0.98, loss 0.07 -- iter 12000/55000, training for: 771.41s\n",
            "Epoch 7, step (batch no.): 1370 -- acc: 0.98, loss 0.06 -- iter 12500/55000, training for: 772.46s\n",
            "Epoch 7, step (batch no.): 1372 -- acc: 0.98, loss 0.06 -- iter 13000/55000, training for: 773.53s\n",
            "Epoch 7, step (batch no.): 1374 -- acc: 0.98, loss 0.06 -- iter 13500/55000, training for: 774.59s\n",
            "Epoch 7, step (batch no.): 1376 -- acc: 0.98, loss 0.06 -- iter 14000/55000, training for: 775.68s\n",
            "Epoch 7, step (batch no.): 1378 -- acc: 0.98, loss 0.06 -- iter 14500/55000, training for: 776.75s\n",
            "Epoch 7, step (batch no.): 1380 -- acc: 0.98, loss 0.06 -- iter 15000/55000, training for: 777.84s\n",
            "Epoch 7, step (batch no.): 1382 -- acc: 0.98, loss 0.06 -- iter 15500/55000, training for: 778.92s\n",
            "Epoch 7, step (batch no.): 1384 -- acc: 0.98, loss 0.06 -- iter 16000/55000, training for: 780.00s\n",
            "Epoch 7, step (batch no.): 1386 -- acc: 0.99, loss 0.05 -- iter 16500/55000, training for: 781.08s\n",
            "Epoch 7, step (batch no.): 1388 -- acc: 0.99, loss 0.05 -- iter 17000/55000, training for: 782.15s\n",
            "Epoch 7, step (batch no.): 1390 -- acc: 0.99, loss 0.05 -- iter 17500/55000, training for: 783.29s\n",
            "Epoch 7, step (batch no.): 1392 -- acc: 0.99, loss 0.05 -- iter 18000/55000, training for: 784.40s\n",
            "Epoch 7, step (batch no.): 1394 -- acc: 0.99, loss 0.04 -- iter 18500/55000, training for: 785.49s\n",
            "Epoch 7, step (batch no.): 1396 -- acc: 0.99, loss 0.05 -- iter 19000/55000, training for: 786.57s\n",
            "Epoch 7, step (batch no.): 1398 -- acc: 0.99, loss 0.05 -- iter 19500/55000, training for: 787.67s\n",
            "Epoch 7, step (batch no.): 1400 -- acc: 0.98, loss 0.05 -- iter 20000/55000, training for: 788.75s\n",
            "Epoch 7, step (batch no.): 1402 -- acc: 0.98, loss 0.05 -- iter 20500/55000, training for: 789.83s\n",
            "Epoch 7, step (batch no.): 1404 -- acc: 0.98, loss 0.05 -- iter 21000/55000, training for: 790.90s\n",
            "Epoch 7, step (batch no.): 1406 -- acc: 0.98, loss 0.05 -- iter 21500/55000, training for: 791.97s\n",
            "Epoch 7, step (batch no.): 1408 -- acc: 0.98, loss 0.05 -- iter 22000/55000, training for: 793.07s\n",
            "Epoch 7, step (batch no.): 1410 -- acc: 0.98, loss 0.05 -- iter 22500/55000, training for: 794.13s\n",
            "Epoch 7, step (batch no.): 1412 -- acc: 0.99, loss 0.05 -- iter 23000/55000, training for: 795.20s\n",
            "Epoch 7, step (batch no.): 1414 -- acc: 0.99, loss 0.04 -- iter 23500/55000, training for: 796.30s\n",
            "Epoch 7, step (batch no.): 1416 -- acc: 0.99, loss 0.04 -- iter 24000/55000, training for: 797.36s\n",
            "Epoch 7, step (batch no.): 1418 -- acc: 0.99, loss 0.04 -- iter 24500/55000, training for: 798.46s\n",
            "Epoch 7, step (batch no.): 1420 -- acc: 0.99, loss 0.03 -- iter 25000/55000, training for: 799.50s\n",
            "Epoch 7, step (batch no.): 1422 -- acc: 0.99, loss 0.04 -- iter 25500/55000, training for: 800.56s\n",
            "Epoch 7, step (batch no.): 1424 -- acc: 0.99, loss 0.04 -- iter 26000/55000, training for: 801.63s\n",
            "Epoch 7, step (batch no.): 1426 -- acc: 0.99, loss 0.04 -- iter 26500/55000, training for: 802.69s\n",
            "Epoch 7, step (batch no.): 1428 -- acc: 0.99, loss 0.05 -- iter 27000/55000, training for: 803.76s\n",
            "Epoch 7, step (batch no.): 1430 -- acc: 0.99, loss 0.05 -- iter 27500/55000, training for: 804.83s\n",
            "Epoch 7, step (batch no.): 1432 -- acc: 0.99, loss 0.04 -- iter 28000/55000, training for: 805.92s\n",
            "Epoch 7, step (batch no.): 1434 -- acc: 0.99, loss 0.04 -- iter 28500/55000, training for: 806.99s\n",
            "Epoch 7, step (batch no.): 1436 -- acc: 0.99, loss 0.04 -- iter 29000/55000, training for: 808.06s\n",
            "Epoch 7, step (batch no.): 1438 -- acc: 0.99, loss 0.04 -- iter 29500/55000, training for: 809.14s\n",
            "Epoch 7, step (batch no.): 1440 -- acc: 0.99, loss 0.04 -- iter 30000/55000, training for: 810.22s\n",
            "Epoch 7, step (batch no.): 1442 -- acc: 0.99, loss 0.04 -- iter 30500/55000, training for: 811.29s\n",
            "Epoch 7, step (batch no.): 1444 -- acc: 0.99, loss 0.04 -- iter 31000/55000, training for: 812.36s\n",
            "Epoch 7, step (batch no.): 1446 -- acc: 0.99, loss 0.04 -- iter 31500/55000, training for: 813.43s\n",
            "Epoch 7, step (batch no.): 1448 -- acc: 0.98, loss 0.05 -- iter 32000/55000, training for: 814.52s\n",
            "Epoch 7, step (batch no.): 1450 -- acc: 0.98, loss 0.05 -- iter 32500/55000, training for: 815.69s\n",
            "Epoch 7, step (batch no.): 1452 -- acc: 0.98, loss 0.05 -- iter 33000/55000, training for: 816.77s\n",
            "Epoch 7, step (batch no.): 1454 -- acc: 0.98, loss 0.05 -- iter 33500/55000, training for: 817.85s\n",
            "Epoch 7, step (batch no.): 1456 -- acc: 0.98, loss 0.05 -- iter 34000/55000, training for: 818.94s\n",
            "Epoch 7, step (batch no.): 1458 -- acc: 0.99, loss 0.04 -- iter 34500/55000, training for: 820.01s\n",
            "Epoch 7, step (batch no.): 1460 -- acc: 0.99, loss 0.04 -- iter 35000/55000, training for: 821.08s\n",
            "Epoch 7, step (batch no.): 1462 -- acc: 0.99, loss 0.04 -- iter 35500/55000, training for: 822.17s\n",
            "Epoch 7, step (batch no.): 1464 -- acc: 0.99, loss 0.04 -- iter 36000/55000, training for: 823.25s\n",
            "Epoch 7, step (batch no.): 1466 -- acc: 0.99, loss 0.04 -- iter 36500/55000, training for: 824.33s\n",
            "Epoch 7, step (batch no.): 1468 -- acc: 0.99, loss 0.04 -- iter 37000/55000, training for: 825.40s\n",
            "Epoch 7, step (batch no.): 1470 -- acc: 0.99, loss 0.04 -- iter 37500/55000, training for: 826.56s\n",
            "Epoch 7, step (batch no.): 1472 -- acc: 0.99, loss 0.04 -- iter 38000/55000, training for: 827.62s\n",
            "Epoch 7, step (batch no.): 1474 -- acc: 0.99, loss 0.04 -- iter 38500/55000, training for: 828.68s\n",
            "Epoch 7, step (batch no.): 1476 -- acc: 0.99, loss 0.04 -- iter 39000/55000, training for: 829.74s\n",
            "Epoch 7, step (batch no.): 1478 -- acc: 0.99, loss 0.04 -- iter 39500/55000, training for: 830.79s\n",
            "Epoch 7, step (batch no.): 1480 -- acc: 0.99, loss 0.04 -- iter 40000/55000, training for: 831.88s\n",
            "Epoch 7, step (batch no.): 1482 -- acc: 0.99, loss 0.04 -- iter 40500/55000, training for: 832.96s\n",
            "Epoch 7, step (batch no.): 1484 -- acc: 0.99, loss 0.04 -- iter 41000/55000, training for: 834.03s\n",
            "Epoch 7, step (batch no.): 1486 -- acc: 0.99, loss 0.04 -- iter 41500/55000, training for: 835.09s\n",
            "Epoch 7, step (batch no.): 1488 -- acc: 0.99, loss 0.04 -- iter 42000/55000, training for: 836.15s\n",
            "Epoch 7, step (batch no.): 1490 -- acc: 0.99, loss 0.05 -- iter 42500/55000, training for: 837.25s\n",
            "Epoch 7, step (batch no.): 1492 -- acc: 0.99, loss 0.04 -- iter 43000/55000, training for: 838.32s\n",
            "Epoch 7, step (batch no.): 1494 -- acc: 0.98, loss 0.05 -- iter 43500/55000, training for: 839.41s\n",
            "Epoch 7, step (batch no.): 1496 -- acc: 0.98, loss 0.05 -- iter 44000/55000, training for: 840.49s\n",
            "Epoch 7, step (batch no.): 1498 -- acc: 0.98, loss 0.05 -- iter 44500/55000, training for: 841.56s\n",
            "Epoch 7, step (batch no.): 1500 -- acc: 0.98, loss 0.05 -- iter 45000/55000, training for: 842.67s\n",
            "Epoch 7, step (batch no.): 1502 -- acc: 0.98, loss 0.06 -- iter 45500/55000, training for: 843.78s\n",
            "Epoch 7, step (batch no.): 1504 -- acc: 0.98, loss 0.05 -- iter 46000/55000, training for: 844.87s\n",
            "Epoch 7, step (batch no.): 1506 -- acc: 0.98, loss 0.06 -- iter 46500/55000, training for: 845.95s\n",
            "Epoch 7, step (batch no.): 1508 -- acc: 0.99, loss 0.05 -- iter 47000/55000, training for: 847.07s\n",
            "Epoch 7, step (batch no.): 1510 -- acc: 0.99, loss 0.04 -- iter 47500/55000, training for: 848.15s\n",
            "Epoch 7, step (batch no.): 1512 -- acc: 0.99, loss 0.04 -- iter 48000/55000, training for: 849.30s\n",
            "Epoch 7, step (batch no.): 1514 -- acc: 0.99, loss 0.04 -- iter 48500/55000, training for: 850.38s\n",
            "Epoch 7, step (batch no.): 1516 -- acc: 0.91, loss 1.18 -- iter 49000/55000, training for: 851.44s\n",
            "Epoch 7, step (batch no.): 1518 -- acc: 0.92, loss 1.01 -- iter 49500/55000, training for: 852.51s\n",
            "Epoch 7, step (batch no.): 1520 -- acc: 0.93, loss 0.88 -- iter 50000/55000, training for: 853.58s\n",
            "Epoch 7, step (batch no.): 1522 -- acc: 0.94, loss 0.75 -- iter 50500/55000, training for: 854.63s\n",
            "Epoch 7, step (batch no.): 1524 -- acc: 0.95, loss 0.63 -- iter 51000/55000, training for: 855.69s\n",
            "Epoch 7, step (batch no.): 1526 -- acc: 0.96, loss 0.52 -- iter 51500/55000, training for: 856.75s\n",
            "Epoch 7, step (batch no.): 1528 -- acc: 0.96, loss 0.44 -- iter 52000/55000, training for: 857.85s\n",
            "Epoch 7, step (batch no.): 1530 -- acc: 0.97, loss 0.36 -- iter 52500/55000, training for: 858.92s\n",
            "Epoch 7, step (batch no.): 1532 -- acc: 0.97, loss 0.30 -- iter 53000/55000, training for: 859.98s\n",
            "Epoch 7, step (batch no.): 1534 -- acc: 0.97, loss 0.25 -- iter 53500/55000, training for: 861.03s\n",
            "Epoch 7, step (batch no.): 1536 -- acc: 0.98, loss 0.21 -- iter 54000/55000, training for: 862.10s\n",
            "Epoch 7, step (batch no.): 1538 -- acc: 0.98, loss 0.18 -- iter 54500/55000, training for: 863.16s\n",
            "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.15161\u001b[0m\u001b[0m | time: 124.148s\n",
            "| Adam | epoch: 007 | loss: 0.15161 - acc: 0.9807 | val_loss: 0.05900 - val_acc: 0.9862 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 7, step (batch no.): 1540 -- acc: 0.98, loss 0.15 -- iter 55000/55000, training for: 869.58s\n",
            "Epoch 8, step (batch no.): 1542 -- acc: 0.98, loss 0.13 -- iter 00500/55000, training for: 870.66s\n",
            "Epoch 8, step (batch no.): 1544 -- acc: 0.98, loss 0.11 -- iter 01000/55000, training for: 871.73s\n",
            "Epoch 8, step (batch no.): 1546 -- acc: 0.98, loss 0.10 -- iter 01500/55000, training for: 872.79s\n",
            "Epoch 8, step (batch no.): 1548 -- acc: 0.99, loss 0.09 -- iter 02000/55000, training for: 873.86s\n",
            "Epoch 8, step (batch no.): 1550 -- acc: 0.98, loss 0.09 -- iter 02500/55000, training for: 874.93s\n",
            "Epoch 8, step (batch no.): 1552 -- acc: 0.98, loss 0.09 -- iter 03000/55000, training for: 876.03s\n",
            "Epoch 8, step (batch no.): 1554 -- acc: 0.98, loss 0.08 -- iter 03500/55000, training for: 877.09s\n",
            "Epoch 8, step (batch no.): 1556 -- acc: 0.99, loss 0.07 -- iter 04000/55000, training for: 878.18s\n",
            "Epoch 8, step (batch no.): 1558 -- acc: 0.99, loss 0.07 -- iter 04500/55000, training for: 879.27s\n",
            "Epoch 8, step (batch no.): 1560 -- acc: 0.98, loss 0.07 -- iter 05000/55000, training for: 880.35s\n",
            "Epoch 8, step (batch no.): 1562 -- acc: 0.98, loss 0.06 -- iter 05500/55000, training for: 881.44s\n",
            "Epoch 8, step (batch no.): 1564 -- acc: 0.99, loss 0.06 -- iter 06000/55000, training for: 882.57s\n",
            "Epoch 8, step (batch no.): 1566 -- acc: 0.99, loss 0.06 -- iter 06500/55000, training for: 883.64s\n",
            "Epoch 8, step (batch no.): 1568 -- acc: 0.98, loss 0.05 -- iter 07000/55000, training for: 884.72s\n",
            "Epoch 8, step (batch no.): 1570 -- acc: 0.99, loss 0.05 -- iter 07500/55000, training for: 885.78s\n",
            "Epoch 8, step (batch no.): 1572 -- acc: 0.99, loss 0.05 -- iter 08000/55000, training for: 886.85s\n",
            "Epoch 8, step (batch no.): 1574 -- acc: 0.98, loss 0.05 -- iter 08500/55000, training for: 887.91s\n",
            "Epoch 8, step (batch no.): 1576 -- acc: 0.99, loss 0.05 -- iter 09000/55000, training for: 888.97s\n",
            "Epoch 8, step (batch no.): 1578 -- acc: 0.99, loss 0.04 -- iter 09500/55000, training for: 890.02s\n",
            "Epoch 8, step (batch no.): 1580 -- acc: 0.99, loss 0.04 -- iter 10000/55000, training for: 891.08s\n",
            "Epoch 8, step (batch no.): 1582 -- acc: 0.99, loss 0.04 -- iter 10500/55000, training for: 892.13s\n",
            "Epoch 8, step (batch no.): 1584 -- acc: 0.99, loss 0.04 -- iter 11000/55000, training for: 893.19s\n",
            "Epoch 8, step (batch no.): 1586 -- acc: 0.99, loss 0.04 -- iter 11500/55000, training for: 894.26s\n",
            "Epoch 8, step (batch no.): 1588 -- acc: 0.99, loss 0.04 -- iter 12000/55000, training for: 895.33s\n",
            "Epoch 8, step (batch no.): 1590 -- acc: 0.98, loss 0.05 -- iter 12500/55000, training for: 896.38s\n",
            "Epoch 8, step (batch no.): 1592 -- acc: 0.98, loss 0.05 -- iter 13000/55000, training for: 897.44s\n",
            "Epoch 8, step (batch no.): 1594 -- acc: 0.98, loss 0.06 -- iter 13500/55000, training for: 898.50s\n",
            "Epoch 8, step (batch no.): 1596 -- acc: 0.98, loss 0.06 -- iter 14000/55000, training for: 899.59s\n",
            "Epoch 8, step (batch no.): 1598 -- acc: 0.98, loss 0.06 -- iter 14500/55000, training for: 900.64s\n",
            "Epoch 8, step (batch no.): 1600 -- acc: 0.98, loss 0.06 -- iter 15000/55000, training for: 901.71s\n",
            "Epoch 8, step (batch no.): 1602 -- acc: 0.98, loss 0.06 -- iter 15500/55000, training for: 902.76s\n",
            "Epoch 8, step (batch no.): 1604 -- acc: 0.98, loss 0.06 -- iter 16000/55000, training for: 903.85s\n",
            "Epoch 8, step (batch no.): 1606 -- acc: 0.98, loss 0.06 -- iter 16500/55000, training for: 904.95s\n",
            "Epoch 8, step (batch no.): 1608 -- acc: 0.98, loss 0.05 -- iter 17000/55000, training for: 906.02s\n",
            "Epoch 8, step (batch no.): 1610 -- acc: 0.98, loss 0.05 -- iter 17500/55000, training for: 907.14s\n",
            "Epoch 8, step (batch no.): 1612 -- acc: 0.98, loss 0.05 -- iter 18000/55000, training for: 908.23s\n",
            "Epoch 8, step (batch no.): 1614 -- acc: 0.98, loss 0.05 -- iter 18500/55000, training for: 909.32s\n",
            "Epoch 8, step (batch no.): 1616 -- acc: 0.98, loss 0.05 -- iter 19000/55000, training for: 910.39s\n",
            "Epoch 8, step (batch no.): 1618 -- acc: 0.99, loss 0.05 -- iter 19500/55000, training for: 911.46s\n",
            "Epoch 8, step (batch no.): 1620 -- acc: 0.98, loss 0.05 -- iter 20000/55000, training for: 912.56s\n",
            "Epoch 8, step (batch no.): 1622 -- acc: 0.98, loss 0.06 -- iter 20500/55000, training for: 913.67s\n",
            "Epoch 8, step (batch no.): 1624 -- acc: 0.99, loss 0.05 -- iter 21000/55000, training for: 914.75s\n",
            "Epoch 8, step (batch no.): 1626 -- acc: 0.98, loss 0.06 -- iter 21500/55000, training for: 915.81s\n",
            "Epoch 8, step (batch no.): 1628 -- acc: 0.98, loss 0.06 -- iter 22000/55000, training for: 916.87s\n",
            "Epoch 8, step (batch no.): 1630 -- acc: 0.98, loss 0.05 -- iter 22500/55000, training for: 917.95s\n",
            "Epoch 8, step (batch no.): 1632 -- acc: 0.99, loss 0.05 -- iter 23000/55000, training for: 919.03s\n",
            "Epoch 8, step (batch no.): 1634 -- acc: 0.99, loss 0.05 -- iter 23500/55000, training for: 920.12s\n",
            "Epoch 8, step (batch no.): 1636 -- acc: 0.99, loss 0.04 -- iter 24000/55000, training for: 921.19s\n",
            "Epoch 8, step (batch no.): 1638 -- acc: 0.99, loss 0.04 -- iter 24500/55000, training for: 922.26s\n",
            "Epoch 8, step (batch no.): 1640 -- acc: 0.99, loss 0.04 -- iter 25000/55000, training for: 923.33s\n",
            "Epoch 8, step (batch no.): 1642 -- acc: 0.99, loss 0.04 -- iter 25500/55000, training for: 924.40s\n",
            "Epoch 8, step (batch no.): 1644 -- acc: 0.99, loss 0.04 -- iter 26000/55000, training for: 925.48s\n",
            "Epoch 8, step (batch no.): 1646 -- acc: 0.99, loss 0.04 -- iter 26500/55000, training for: 926.55s\n",
            "Epoch 8, step (batch no.): 1648 -- acc: 0.99, loss 0.04 -- iter 27000/55000, training for: 927.64s\n",
            "Epoch 8, step (batch no.): 1650 -- acc: 0.99, loss 0.04 -- iter 27500/55000, training for: 928.69s\n",
            "Epoch 8, step (batch no.): 1652 -- acc: 0.99, loss 0.04 -- iter 28000/55000, training for: 929.78s\n",
            "Epoch 8, step (batch no.): 1654 -- acc: 0.99, loss 0.04 -- iter 28500/55000, training for: 930.84s\n",
            "Epoch 8, step (batch no.): 1656 -- acc: 0.99, loss 0.04 -- iter 29000/55000, training for: 931.90s\n",
            "Epoch 8, step (batch no.): 1658 -- acc: 0.99, loss 0.04 -- iter 29500/55000, training for: 932.96s\n",
            "Epoch 8, step (batch no.): 1660 -- acc: 0.99, loss 0.03 -- iter 30000/55000, training for: 934.10s\n",
            "Epoch 8, step (batch no.): 1662 -- acc: 0.99, loss 0.04 -- iter 30500/55000, training for: 935.19s\n",
            "Epoch 8, step (batch no.): 1664 -- acc: 0.99, loss 0.03 -- iter 31000/55000, training for: 936.25s\n",
            "Epoch 8, step (batch no.): 1666 -- acc: 0.99, loss 0.03 -- iter 31500/55000, training for: 937.32s\n",
            "Epoch 8, step (batch no.): 1668 -- acc: 0.99, loss 0.04 -- iter 32000/55000, training for: 938.39s\n",
            "Epoch 8, step (batch no.): 1670 -- acc: 0.99, loss 0.04 -- iter 32500/55000, training for: 939.46s\n",
            "Epoch 8, step (batch no.): 1672 -- acc: 0.99, loss 0.04 -- iter 33000/55000, training for: 940.54s\n",
            "Epoch 8, step (batch no.): 1674 -- acc: 0.99, loss 0.04 -- iter 33500/55000, training for: 941.63s\n",
            "Epoch 8, step (batch no.): 1676 -- acc: 0.99, loss 0.04 -- iter 34000/55000, training for: 942.71s\n",
            "Epoch 8, step (batch no.): 1678 -- acc: 0.99, loss 0.05 -- iter 34500/55000, training for: 943.78s\n",
            "Epoch 8, step (batch no.): 1680 -- acc: 0.99, loss 0.05 -- iter 35000/55000, training for: 944.85s\n",
            "Epoch 8, step (batch no.): 1682 -- acc: 0.99, loss 0.05 -- iter 35500/55000, training for: 945.92s\n",
            "Epoch 8, step (batch no.): 1684 -- acc: 0.99, loss 0.04 -- iter 36000/55000, training for: 946.99s\n",
            "Epoch 8, step (batch no.): 1686 -- acc: 0.98, loss 0.05 -- iter 36500/55000, training for: 948.12s\n",
            "Epoch 8, step (batch no.): 1688 -- acc: 0.99, loss 0.05 -- iter 37000/55000, training for: 949.19s\n",
            "Epoch 8, step (batch no.): 1690 -- acc: 0.99, loss 0.05 -- iter 37500/55000, training for: 950.33s\n",
            "Epoch 8, step (batch no.): 1692 -- acc: 0.99, loss 0.04 -- iter 38000/55000, training for: 951.40s\n",
            "Epoch 8, step (batch no.): 1694 -- acc: 0.99, loss 0.04 -- iter 38500/55000, training for: 952.47s\n",
            "Epoch 8, step (batch no.): 1696 -- acc: 0.99, loss 0.04 -- iter 39000/55000, training for: 953.53s\n",
            "Epoch 8, step (batch no.): 1698 -- acc: 0.99, loss 0.04 -- iter 39500/55000, training for: 954.60s\n",
            "Epoch 8, step (batch no.): 1700 -- acc: 0.99, loss 0.05 -- iter 40000/55000, training for: 955.65s\n",
            "Epoch 8, step (batch no.): 1702 -- acc: 0.99, loss 0.04 -- iter 40500/55000, training for: 956.71s\n",
            "Epoch 8, step (batch no.): 1704 -- acc: 0.99, loss 0.04 -- iter 41000/55000, training for: 957.78s\n",
            "Epoch 8, step (batch no.): 1706 -- acc: 0.99, loss 0.04 -- iter 41500/55000, training for: 958.83s\n",
            "Epoch 8, step (batch no.): 1708 -- acc: 0.99, loss 0.04 -- iter 42000/55000, training for: 959.90s\n",
            "Epoch 8, step (batch no.): 1710 -- acc: 0.99, loss 0.05 -- iter 42500/55000, training for: 960.98s\n",
            "Epoch 8, step (batch no.): 1712 -- acc: 0.99, loss 0.04 -- iter 43000/55000, training for: 962.04s\n",
            "Epoch 8, step (batch no.): 1714 -- acc: 0.98, loss 0.05 -- iter 43500/55000, training for: 963.10s\n",
            "Epoch 8, step (batch no.): 1716 -- acc: 0.99, loss 0.04 -- iter 44000/55000, training for: 964.23s\n",
            "Epoch 8, step (batch no.): 1718 -- acc: 0.98, loss 0.05 -- iter 44500/55000, training for: 965.29s\n",
            "Epoch 8, step (batch no.): 1720 -- acc: 0.99, loss 0.04 -- iter 45000/55000, training for: 966.38s\n",
            "Epoch 8, step (batch no.): 1722 -- acc: 0.99, loss 0.04 -- iter 45500/55000, training for: 967.47s\n",
            "Epoch 8, step (batch no.): 1724 -- acc: 0.99, loss 0.04 -- iter 46000/55000, training for: 968.55s\n",
            "Epoch 8, step (batch no.): 1726 -- acc: 0.99, loss 0.04 -- iter 46500/55000, training for: 969.63s\n",
            "Epoch 8, step (batch no.): 1728 -- acc: 0.98, loss 0.04 -- iter 47000/55000, training for: 970.70s\n",
            "Epoch 8, step (batch no.): 1730 -- acc: 0.98, loss 0.05 -- iter 47500/55000, training for: 971.79s\n",
            "Epoch 8, step (batch no.): 1732 -- acc: 0.99, loss 0.04 -- iter 48000/55000, training for: 972.88s\n",
            "Epoch 8, step (batch no.): 1734 -- acc: 0.99, loss 0.04 -- iter 48500/55000, training for: 973.97s\n",
            "Epoch 8, step (batch no.): 1736 -- acc: 0.89, loss 1.37 -- iter 49000/55000, training for: 975.07s\n",
            "Epoch 8, step (batch no.): 1738 -- acc: 0.91, loss 1.14 -- iter 49500/55000, training for: 976.16s\n",
            "Epoch 8, step (batch no.): 1740 -- acc: 0.92, loss 0.97 -- iter 50000/55000, training for: 977.24s\n",
            "Epoch 8, step (batch no.): 1742 -- acc: 0.93, loss 0.82 -- iter 50500/55000, training for: 978.33s\n",
            "Epoch 8, step (batch no.): 1744 -- acc: 0.94, loss 0.70 -- iter 51000/55000, training for: 979.40s\n",
            "Epoch 8, step (batch no.): 1746 -- acc: 0.95, loss 0.59 -- iter 51500/55000, training for: 980.57s\n",
            "Epoch 8, step (batch no.): 1748 -- acc: 0.96, loss 0.49 -- iter 52000/55000, training for: 981.65s\n",
            "Epoch 8, step (batch no.): 1750 -- acc: 0.96, loss 0.40 -- iter 52500/55000, training for: 982.73s\n",
            "Epoch 8, step (batch no.): 1752 -- acc: 0.97, loss 0.33 -- iter 53000/55000, training for: 983.79s\n",
            "Epoch 8, step (batch no.): 1754 -- acc: 0.97, loss 0.28 -- iter 53500/55000, training for: 984.87s\n",
            "Epoch 8, step (batch no.): 1756 -- acc: 0.97, loss 0.24 -- iter 54000/55000, training for: 985.96s\n",
            "Epoch 8, step (batch no.): 1758 -- acc: 0.98, loss 0.20 -- iter 54500/55000, training for: 987.03s\n",
            "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.16503\u001b[0m\u001b[0m | time: 123.967s\n",
            "| Adam | epoch: 008 | loss: 0.16503 - acc: 0.9786 | val_loss: 0.04385 - val_acc: 0.9862 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 8, step (batch no.): 1760 -- acc: 0.98, loss 0.17 -- iter 55000/55000, training for: 993.60s\n",
            "Epoch 9, step (batch no.): 1762 -- acc: 0.98, loss 0.14 -- iter 00500/55000, training for: 994.67s\n",
            "Epoch 9, step (batch no.): 1764 -- acc: 0.98, loss 0.13 -- iter 01000/55000, training for: 995.74s\n",
            "Epoch 9, step (batch no.): 1766 -- acc: 0.98, loss 0.11 -- iter 01500/55000, training for: 996.81s\n",
            "Epoch 9, step (batch no.): 1768 -- acc: 0.98, loss 0.10 -- iter 02000/55000, training for: 997.89s\n",
            "Epoch 9, step (batch no.): 1770 -- acc: 0.98, loss 0.10 -- iter 02500/55000, training for: 998.98s\n",
            "Epoch 9, step (batch no.): 1772 -- acc: 0.98, loss 0.09 -- iter 03000/55000, training for: 1000.06s\n",
            "Epoch 9, step (batch no.): 1774 -- acc: 0.98, loss 0.07 -- iter 03500/55000, training for: 1001.13s\n",
            "Epoch 9, step (batch no.): 1776 -- acc: 0.98, loss 0.07 -- iter 04000/55000, training for: 1002.21s\n",
            "Epoch 9, step (batch no.): 1778 -- acc: 0.98, loss 0.07 -- iter 04500/55000, training for: 1003.29s\n",
            "Epoch 9, step (batch no.): 1780 -- acc: 0.99, loss 0.06 -- iter 05000/55000, training for: 1004.36s\n",
            "Epoch 9, step (batch no.): 1782 -- acc: 0.99, loss 0.05 -- iter 05500/55000, training for: 1005.43s\n",
            "Epoch 9, step (batch no.): 1784 -- acc: 0.99, loss 0.05 -- iter 06000/55000, training for: 1006.50s\n",
            "Epoch 9, step (batch no.): 1786 -- acc: 0.99, loss 0.05 -- iter 06500/55000, training for: 1007.60s\n",
            "Epoch 9, step (batch no.): 1788 -- acc: 0.99, loss 0.05 -- iter 07000/55000, training for: 1008.69s\n",
            "Epoch 9, step (batch no.): 1790 -- acc: 0.99, loss 0.05 -- iter 07500/55000, training for: 1009.78s\n",
            "Epoch 9, step (batch no.): 1792 -- acc: 0.99, loss 0.04 -- iter 08000/55000, training for: 1010.84s\n",
            "Epoch 9, step (batch no.): 1794 -- acc: 0.98, loss 0.06 -- iter 08500/55000, training for: 1011.92s\n",
            "Epoch 9, step (batch no.): 1796 -- acc: 0.98, loss 0.07 -- iter 09000/55000, training for: 1013.07s\n",
            "Epoch 9, step (batch no.): 1798 -- acc: 0.98, loss 0.07 -- iter 09500/55000, training for: 1014.15s\n",
            "Epoch 9, step (batch no.): 1800 -- acc: 0.98, loss 0.07 -- iter 10000/55000, training for: 1015.22s\n",
            "Epoch 9, step (batch no.): 1802 -- acc: 0.98, loss 0.06 -- iter 10500/55000, training for: 1016.32s\n",
            "Epoch 9, step (batch no.): 1804 -- acc: 0.98, loss 0.07 -- iter 11000/55000, training for: 1017.38s\n",
            "Epoch 9, step (batch no.): 1806 -- acc: 0.98, loss 0.07 -- iter 11500/55000, training for: 1018.44s\n",
            "Epoch 9, step (batch no.): 1808 -- acc: 0.98, loss 0.07 -- iter 12000/55000, training for: 1019.50s\n",
            "Epoch 9, step (batch no.): 1810 -- acc: 0.98, loss 0.07 -- iter 12500/55000, training for: 1020.54s\n",
            "Epoch 9, step (batch no.): 1812 -- acc: 0.98, loss 0.07 -- iter 13000/55000, training for: 1021.62s\n",
            "Epoch 9, step (batch no.): 1814 -- acc: 0.98, loss 0.07 -- iter 13500/55000, training for: 1022.68s\n",
            "Epoch 9, step (batch no.): 1816 -- acc: 0.98, loss 0.07 -- iter 14000/55000, training for: 1023.74s\n",
            "Epoch 9, step (batch no.): 1818 -- acc: 0.98, loss 0.06 -- iter 14500/55000, training for: 1024.83s\n",
            "Epoch 9, step (batch no.): 1820 -- acc: 0.98, loss 0.06 -- iter 15000/55000, training for: 1025.94s\n",
            "Epoch 9, step (batch no.): 1822 -- acc: 0.98, loss 0.06 -- iter 15500/55000, training for: 1027.05s\n",
            "Epoch 9, step (batch no.): 1824 -- acc: 0.98, loss 0.06 -- iter 16000/55000, training for: 1028.13s\n",
            "Epoch 9, step (batch no.): 1826 -- acc: 0.98, loss 0.06 -- iter 16500/55000, training for: 1029.21s\n",
            "Epoch 9, step (batch no.): 1828 -- acc: 0.98, loss 0.07 -- iter 17000/55000, training for: 1030.26s\n",
            "Epoch 9, step (batch no.): 1830 -- acc: 0.98, loss 0.06 -- iter 17500/55000, training for: 1031.33s\n",
            "Epoch 9, step (batch no.): 1832 -- acc: 0.98, loss 0.06 -- iter 18000/55000, training for: 1032.39s\n",
            "Epoch 9, step (batch no.): 1834 -- acc: 0.98, loss 0.06 -- iter 18500/55000, training for: 1033.48s\n",
            "Epoch 9, step (batch no.): 1836 -- acc: 0.98, loss 0.05 -- iter 19000/55000, training for: 1034.54s\n",
            "Epoch 9, step (batch no.): 1838 -- acc: 0.99, loss 0.05 -- iter 19500/55000, training for: 1035.61s\n",
            "Epoch 9, step (batch no.): 1840 -- acc: 0.99, loss 0.05 -- iter 20000/55000, training for: 1036.68s\n",
            "Epoch 9, step (batch no.): 1842 -- acc: 0.99, loss 0.04 -- iter 20500/55000, training for: 1037.75s\n",
            "Epoch 9, step (batch no.): 1844 -- acc: 0.99, loss 0.04 -- iter 21000/55000, training for: 1038.82s\n",
            "Epoch 9, step (batch no.): 1846 -- acc: 0.98, loss 0.04 -- iter 21500/55000, training for: 1039.89s\n",
            "Epoch 9, step (batch no.): 1848 -- acc: 0.98, loss 0.04 -- iter 22000/55000, training for: 1040.99s\n",
            "Epoch 9, step (batch no.): 1850 -- acc: 0.99, loss 0.04 -- iter 22500/55000, training for: 1042.06s\n",
            "Epoch 9, step (batch no.): 1852 -- acc: 0.99, loss 0.04 -- iter 23000/55000, training for: 1043.15s\n",
            "Epoch 9, step (batch no.): 1854 -- acc: 0.99, loss 0.05 -- iter 23500/55000, training for: 1044.21s\n",
            "Epoch 9, step (batch no.): 1856 -- acc: 0.99, loss 0.05 -- iter 24000/55000, training for: 1045.26s\n",
            "Epoch 9, step (batch no.): 1858 -- acc: 0.99, loss 0.05 -- iter 24500/55000, training for: 1046.39s\n",
            "Epoch 9, step (batch no.): 1860 -- acc: 0.99, loss 0.05 -- iter 25000/55000, training for: 1047.47s\n",
            "Epoch 9, step (batch no.): 1862 -- acc: 0.98, loss 0.05 -- iter 25500/55000, training for: 1048.53s\n",
            "Epoch 9, step (batch no.): 1864 -- acc: 0.99, loss 0.05 -- iter 26000/55000, training for: 1049.60s\n",
            "Epoch 9, step (batch no.): 1866 -- acc: 0.98, loss 0.05 -- iter 26500/55000, training for: 1050.65s\n",
            "Epoch 9, step (batch no.): 1868 -- acc: 0.98, loss 0.05 -- iter 27000/55000, training for: 1051.71s\n",
            "Epoch 9, step (batch no.): 1870 -- acc: 0.98, loss 0.05 -- iter 27500/55000, training for: 1052.77s\n",
            "Epoch 9, step (batch no.): 1872 -- acc: 0.99, loss 0.05 -- iter 28000/55000, training for: 1053.83s\n",
            "Epoch 9, step (batch no.): 1874 -- acc: 0.99, loss 0.04 -- iter 28500/55000, training for: 1054.90s\n",
            "Epoch 9, step (batch no.): 1876 -- acc: 0.99, loss 0.04 -- iter 29000/55000, training for: 1055.97s\n",
            "Epoch 9, step (batch no.): 1878 -- acc: 0.98, loss 0.05 -- iter 29500/55000, training for: 1057.04s\n",
            "Epoch 9, step (batch no.): 1880 -- acc: 0.98, loss 0.05 -- iter 30000/55000, training for: 1058.11s\n",
            "Epoch 9, step (batch no.): 1882 -- acc: 0.98, loss 0.07 -- iter 30500/55000, training for: 1059.18s\n",
            "Epoch 9, step (batch no.): 1884 -- acc: 0.98, loss 0.06 -- iter 31000/55000, training for: 1060.23s\n",
            "Epoch 9, step (batch no.): 1886 -- acc: 0.98, loss 0.07 -- iter 31500/55000, training for: 1061.28s\n",
            "Epoch 9, step (batch no.): 1888 -- acc: 0.98, loss 0.06 -- iter 32000/55000, training for: 1062.35s\n",
            "Epoch 9, step (batch no.): 1890 -- acc: 0.98, loss 0.05 -- iter 32500/55000, training for: 1063.41s\n",
            "Epoch 9, step (batch no.): 1892 -- acc: 0.98, loss 0.05 -- iter 33000/55000, training for: 1064.50s\n",
            "Epoch 9, step (batch no.): 1894 -- acc: 0.98, loss 0.05 -- iter 33500/55000, training for: 1065.55s\n",
            "Epoch 9, step (batch no.): 1896 -- acc: 0.98, loss 0.05 -- iter 34000/55000, training for: 1066.61s\n",
            "Epoch 9, step (batch no.): 1898 -- acc: 0.98, loss 0.04 -- iter 34500/55000, training for: 1067.68s\n",
            "Epoch 9, step (batch no.): 1900 -- acc: 0.99, loss 0.04 -- iter 35000/55000, training for: 1068.77s\n",
            "Epoch 9, step (batch no.): 1902 -- acc: 0.99, loss 0.04 -- iter 35500/55000, training for: 1069.85s\n",
            "Epoch 9, step (batch no.): 1904 -- acc: 0.99, loss 0.04 -- iter 36000/55000, training for: 1070.91s\n",
            "Epoch 9, step (batch no.): 1906 -- acc: 0.99, loss 0.05 -- iter 36500/55000, training for: 1071.95s\n",
            "Epoch 9, step (batch no.): 1908 -- acc: 0.99, loss 0.05 -- iter 37000/55000, training for: 1073.03s\n",
            "Epoch 9, step (batch no.): 1910 -- acc: 0.99, loss 0.05 -- iter 37500/55000, training for: 1074.14s\n",
            "Epoch 9, step (batch no.): 1912 -- acc: 0.99, loss 0.05 -- iter 38000/55000, training for: 1075.22s\n",
            "Epoch 9, step (batch no.): 1914 -- acc: 0.99, loss 0.05 -- iter 38500/55000, training for: 1076.28s\n",
            "Epoch 9, step (batch no.): 1916 -- acc: 0.99, loss 0.04 -- iter 39000/55000, training for: 1077.36s\n",
            "Epoch 9, step (batch no.): 1918 -- acc: 0.99, loss 0.05 -- iter 39500/55000, training for: 1078.41s\n",
            "Epoch 9, step (batch no.): 1920 -- acc: 0.98, loss 0.05 -- iter 40000/55000, training for: 1079.55s\n",
            "Epoch 9, step (batch no.): 1922 -- acc: 0.98, loss 0.05 -- iter 40500/55000, training for: 1080.62s\n",
            "Epoch 9, step (batch no.): 1924 -- acc: 0.98, loss 0.05 -- iter 41000/55000, training for: 1081.68s\n",
            "Epoch 9, step (batch no.): 1926 -- acc: 0.99, loss 0.05 -- iter 41500/55000, training for: 1082.75s\n",
            "Epoch 9, step (batch no.): 1928 -- acc: 0.99, loss 0.05 -- iter 42000/55000, training for: 1083.83s\n",
            "Epoch 9, step (batch no.): 1930 -- acc: 0.99, loss 0.05 -- iter 42500/55000, training for: 1084.95s\n",
            "Epoch 9, step (batch no.): 1932 -- acc: 0.99, loss 0.04 -- iter 43000/55000, training for: 1086.05s\n",
            "Epoch 9, step (batch no.): 1934 -- acc: 0.99, loss 0.04 -- iter 43500/55000, training for: 1087.15s\n",
            "Epoch 9, step (batch no.): 1936 -- acc: 0.99, loss 0.04 -- iter 44000/55000, training for: 1088.23s\n",
            "Epoch 9, step (batch no.): 1938 -- acc: 0.99, loss 0.04 -- iter 44500/55000, training for: 1089.33s\n",
            "Epoch 9, step (batch no.): 1940 -- acc: 0.99, loss 0.04 -- iter 45000/55000, training for: 1090.39s\n",
            "Epoch 9, step (batch no.): 1942 -- acc: 0.99, loss 0.04 -- iter 45500/55000, training for: 1091.45s\n",
            "Epoch 9, step (batch no.): 1944 -- acc: 0.99, loss 0.04 -- iter 46000/55000, training for: 1092.51s\n",
            "Epoch 9, step (batch no.): 1946 -- acc: 0.99, loss 0.04 -- iter 46500/55000, training for: 1093.58s\n",
            "Epoch 9, step (batch no.): 1948 -- acc: 0.99, loss 0.04 -- iter 47000/55000, training for: 1094.67s\n",
            "Epoch 9, step (batch no.): 1950 -- acc: 0.99, loss 0.04 -- iter 47500/55000, training for: 1095.72s\n",
            "Epoch 9, step (batch no.): 1952 -- acc: 0.99, loss 0.04 -- iter 48000/55000, training for: 1096.79s\n",
            "Epoch 9, step (batch no.): 1954 -- acc: 0.99, loss 0.04 -- iter 48500/55000, training for: 1097.86s\n",
            "Epoch 9, step (batch no.): 1956 -- acc: 0.99, loss 0.04 -- iter 49000/55000, training for: 1098.93s\n",
            "Epoch 9, step (batch no.): 1958 -- acc: 0.90, loss 1.21 -- iter 49500/55000, training for: 1099.98s\n",
            "Epoch 9, step (batch no.): 1960 -- acc: 0.92, loss 1.02 -- iter 50000/55000, training for: 1101.04s\n",
            "Epoch 9, step (batch no.): 1962 -- acc: 0.93, loss 0.88 -- iter 50500/55000, training for: 1102.09s\n",
            "Epoch 9, step (batch no.): 1964 -- acc: 0.94, loss 0.75 -- iter 51000/55000, training for: 1103.14s\n",
            "Epoch 9, step (batch no.): 1966 -- acc: 0.95, loss 0.64 -- iter 51500/55000, training for: 1104.21s\n",
            "Epoch 9, step (batch no.): 1968 -- acc: 0.95, loss 0.54 -- iter 52000/55000, training for: 1105.28s\n",
            "Epoch 9, step (batch no.): 1970 -- acc: 0.96, loss 0.45 -- iter 52500/55000, training for: 1106.37s\n",
            "Epoch 9, step (batch no.): 1972 -- acc: 0.96, loss 0.37 -- iter 53000/55000, training for: 1107.45s\n",
            "Epoch 9, step (batch no.): 1974 -- acc: 0.97, loss 0.31 -- iter 53500/55000, training for: 1108.52s\n",
            "Epoch 9, step (batch no.): 1976 -- acc: 0.97, loss 0.27 -- iter 54000/55000, training for: 1109.61s\n",
            "Epoch 9, step (batch no.): 1978 -- acc: 0.97, loss 0.22 -- iter 54500/55000, training for: 1110.69s\n",
            "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.18414\u001b[0m\u001b[0m | time: 123.624s\n",
            "| Adam | epoch: 009 | loss: 0.18414 - acc: 0.9779 | val_loss: 0.05121 - val_acc: 0.9852 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 9, step (batch no.): 1980 -- acc: 0.98, loss 0.18 -- iter 55000/55000, training for: 1117.26s\n",
            "Epoch 10, step (batch no.): 1982 -- acc: 0.98, loss 0.16 -- iter 00500/55000, training for: 1118.33s\n",
            "Epoch 10, step (batch no.): 1984 -- acc: 0.98, loss 0.14 -- iter 01000/55000, training for: 1119.38s\n",
            "Epoch 10, step (batch no.): 1986 -- acc: 0.98, loss 0.12 -- iter 01500/55000, training for: 1120.44s\n",
            "Epoch 10, step (batch no.): 1988 -- acc: 0.99, loss 0.10 -- iter 02000/55000, training for: 1121.49s\n",
            "Epoch 10, step (batch no.): 1990 -- acc: 0.98, loss 0.09 -- iter 02500/55000, training for: 1122.54s\n",
            "Epoch 10, step (batch no.): 1992 -- acc: 0.99, loss 0.08 -- iter 03000/55000, training for: 1123.61s\n",
            "Epoch 10, step (batch no.): 1994 -- acc: 0.99, loss 0.07 -- iter 03500/55000, training for: 1124.68s\n",
            "Epoch 10, step (batch no.): 1996 -- acc: 0.99, loss 0.07 -- iter 04000/55000, training for: 1125.75s\n",
            "Epoch 10, step (batch no.): 1998 -- acc: 0.99, loss 0.06 -- iter 04500/55000, training for: 1126.82s\n",
            "Epoch 10, step (batch no.): 2000 -- acc: 0.99, loss 0.06 -- iter 05000/55000, training for: 1127.90s\n",
            "Epoch 10, step (batch no.): 2002 -- acc: 0.99, loss 0.06 -- iter 05500/55000, training for: 1128.97s\n",
            "Epoch 10, step (batch no.): 2004 -- acc: 0.99, loss 0.06 -- iter 06000/55000, training for: 1130.04s\n",
            "Epoch 10, step (batch no.): 2006 -- acc: 0.99, loss 0.06 -- iter 06500/55000, training for: 1131.11s\n",
            "Epoch 10, step (batch no.): 2008 -- acc: 0.99, loss 0.05 -- iter 07000/55000, training for: 1132.18s\n",
            "Epoch 10, step (batch no.): 2010 -- acc: 0.99, loss 0.05 -- iter 07500/55000, training for: 1133.24s\n",
            "Epoch 10, step (batch no.): 2012 -- acc: 0.99, loss 0.05 -- iter 08000/55000, training for: 1134.29s\n",
            "Epoch 10, step (batch no.): 2014 -- acc: 0.99, loss 0.05 -- iter 08500/55000, training for: 1135.34s\n",
            "Epoch 10, step (batch no.): 2016 -- acc: 0.99, loss 0.05 -- iter 09000/55000, training for: 1136.44s\n",
            "Epoch 10, step (batch no.): 2018 -- acc: 0.99, loss 0.05 -- iter 09500/55000, training for: 1137.52s\n",
            "Epoch 10, step (batch no.): 2020 -- acc: 0.99, loss 0.05 -- iter 10000/55000, training for: 1138.58s\n",
            "Epoch 10, step (batch no.): 2022 -- acc: 0.99, loss 0.05 -- iter 10500/55000, training for: 1139.68s\n",
            "Epoch 10, step (batch no.): 2024 -- acc: 0.99, loss 0.05 -- iter 11000/55000, training for: 1140.75s\n",
            "Epoch 10, step (batch no.): 2026 -- acc: 0.99, loss 0.05 -- iter 11500/55000, training for: 1141.83s\n",
            "Epoch 10, step (batch no.): 2028 -- acc: 0.99, loss 0.05 -- iter 12000/55000, training for: 1142.89s\n",
            "Epoch 10, step (batch no.): 2030 -- acc: 0.99, loss 0.06 -- iter 12500/55000, training for: 1143.97s\n",
            "Epoch 10, step (batch no.): 2032 -- acc: 0.98, loss 0.05 -- iter 13000/55000, training for: 1145.11s\n",
            "Epoch 10, step (batch no.): 2034 -- acc: 0.98, loss 0.05 -- iter 13500/55000, training for: 1146.20s\n",
            "Epoch 10, step (batch no.): 2036 -- acc: 0.99, loss 0.05 -- iter 14000/55000, training for: 1147.42s\n",
            "Epoch 10, step (batch no.): 2038 -- acc: 0.99, loss 0.05 -- iter 14500/55000, training for: 1148.50s\n",
            "Epoch 10, step (batch no.): 2040 -- acc: 0.99, loss 0.05 -- iter 15000/55000, training for: 1149.62s\n",
            "Epoch 10, step (batch no.): 2042 -- acc: 0.99, loss 0.06 -- iter 15500/55000, training for: 1150.72s\n",
            "Epoch 10, step (batch no.): 2044 -- acc: 0.99, loss 0.06 -- iter 16000/55000, training for: 1151.80s\n",
            "Epoch 10, step (batch no.): 2046 -- acc: 0.99, loss 0.05 -- iter 16500/55000, training for: 1152.88s\n",
            "Epoch 10, step (batch no.): 2048 -- acc: 0.99, loss 0.05 -- iter 17000/55000, training for: 1153.95s\n",
            "Epoch 10, step (batch no.): 2050 -- acc: 0.99, loss 0.05 -- iter 17500/55000, training for: 1155.02s\n",
            "Epoch 10, step (batch no.): 2052 -- acc: 0.99, loss 0.05 -- iter 18000/55000, training for: 1156.09s\n",
            "Epoch 10, step (batch no.): 2054 -- acc: 0.99, loss 0.05 -- iter 18500/55000, training for: 1157.17s\n",
            "Epoch 10, step (batch no.): 2056 -- acc: 0.99, loss 0.04 -- iter 19000/55000, training for: 1158.25s\n",
            "Epoch 10, step (batch no.): 2058 -- acc: 0.99, loss 0.04 -- iter 19500/55000, training for: 1159.31s\n",
            "Epoch 10, step (batch no.): 2060 -- acc: 0.99, loss 0.04 -- iter 20000/55000, training for: 1160.39s\n",
            "Epoch 10, step (batch no.): 2062 -- acc: 0.99, loss 0.04 -- iter 20500/55000, training for: 1161.49s\n",
            "Epoch 10, step (batch no.): 2064 -- acc: 0.99, loss 0.05 -- iter 21000/55000, training for: 1162.58s\n",
            "Epoch 10, step (batch no.): 2066 -- acc: 0.99, loss 0.04 -- iter 21500/55000, training for: 1163.65s\n",
            "Epoch 10, step (batch no.): 2068 -- acc: 0.99, loss 0.05 -- iter 22000/55000, training for: 1164.72s\n",
            "Epoch 10, step (batch no.): 2070 -- acc: 0.99, loss 0.05 -- iter 22500/55000, training for: 1165.79s\n",
            "Epoch 10, step (batch no.): 2072 -- acc: 0.99, loss 0.04 -- iter 23000/55000, training for: 1166.87s\n",
            "Epoch 10, step (batch no.): 2074 -- acc: 0.99, loss 0.04 -- iter 23500/55000, training for: 1167.96s\n",
            "Epoch 10, step (batch no.): 2076 -- acc: 0.99, loss 0.04 -- iter 24000/55000, training for: 1169.03s\n",
            "Epoch 10, step (batch no.): 2078 -- acc: 0.99, loss 0.04 -- iter 24500/55000, training for: 1170.11s\n",
            "Epoch 10, step (batch no.): 2080 -- acc: 0.99, loss 0.04 -- iter 25000/55000, training for: 1171.19s\n",
            "Epoch 10, step (batch no.): 2082 -- acc: 0.99, loss 0.04 -- iter 25500/55000, training for: 1172.25s\n",
            "Epoch 10, step (batch no.): 2084 -- acc: 0.99, loss 0.04 -- iter 26000/55000, training for: 1173.33s\n",
            "Epoch 10, step (batch no.): 2086 -- acc: 0.99, loss 0.04 -- iter 26500/55000, training for: 1174.39s\n",
            "Epoch 10, step (batch no.): 2088 -- acc: 0.99, loss 0.03 -- iter 27000/55000, training for: 1175.45s\n",
            "Epoch 10, step (batch no.): 2090 -- acc: 0.99, loss 0.04 -- iter 27500/55000, training for: 1176.53s\n",
            "Epoch 10, step (batch no.): 2092 -- acc: 0.99, loss 0.03 -- iter 28000/55000, training for: 1177.62s\n",
            "Epoch 10, step (batch no.): 2094 -- acc: 0.99, loss 0.03 -- iter 28500/55000, training for: 1178.73s\n",
            "Epoch 10, step (batch no.): 2096 -- acc: 0.99, loss 0.03 -- iter 29000/55000, training for: 1179.78s\n",
            "Epoch 10, step (batch no.): 2098 -- acc: 0.99, loss 0.03 -- iter 29500/55000, training for: 1180.85s\n",
            "Epoch 10, step (batch no.): 2100 -- acc: 0.99, loss 0.03 -- iter 30000/55000, training for: 1181.91s\n",
            "Epoch 10, step (batch no.): 2102 -- acc: 0.99, loss 0.04 -- iter 30500/55000, training for: 1182.97s\n",
            "Epoch 10, step (batch no.): 2104 -- acc: 0.99, loss 0.03 -- iter 31000/55000, training for: 1184.03s\n",
            "Epoch 10, step (batch no.): 2106 -- acc: 0.99, loss 0.04 -- iter 31500/55000, training for: 1185.09s\n",
            "Epoch 10, step (batch no.): 2108 -- acc: 0.99, loss 0.04 -- iter 32000/55000, training for: 1186.15s\n",
            "Epoch 10, step (batch no.): 2110 -- acc: 0.99, loss 0.04 -- iter 32500/55000, training for: 1187.21s\n",
            "Epoch 10, step (batch no.): 2112 -- acc: 0.99, loss 0.04 -- iter 33000/55000, training for: 1188.30s\n",
            "Epoch 10, step (batch no.): 2114 -- acc: 0.99, loss 0.04 -- iter 33500/55000, training for: 1189.36s\n",
            "Epoch 10, step (batch no.): 2116 -- acc: 0.99, loss 0.04 -- iter 34000/55000, training for: 1190.44s\n",
            "Epoch 10, step (batch no.): 2118 -- acc: 0.99, loss 0.04 -- iter 34500/55000, training for: 1191.50s\n",
            "Epoch 10, step (batch no.): 2120 -- acc: 0.99, loss 0.04 -- iter 35000/55000, training for: 1192.55s\n",
            "Epoch 10, step (batch no.): 2122 -- acc: 0.99, loss 0.04 -- iter 35500/55000, training for: 1193.61s\n",
            "Epoch 10, step (batch no.): 2124 -- acc: 0.99, loss 0.04 -- iter 36000/55000, training for: 1194.68s\n",
            "Epoch 10, step (batch no.): 2126 -- acc: 0.99, loss 0.04 -- iter 36500/55000, training for: 1195.74s\n",
            "Epoch 10, step (batch no.): 2128 -- acc: 0.99, loss 0.04 -- iter 37000/55000, training for: 1196.80s\n",
            "Epoch 10, step (batch no.): 2130 -- acc: 0.98, loss 0.05 -- iter 37500/55000, training for: 1197.86s\n",
            "Epoch 10, step (batch no.): 2132 -- acc: 0.98, loss 0.05 -- iter 38000/55000, training for: 1198.93s\n",
            "Epoch 10, step (batch no.): 2134 -- acc: 0.98, loss 0.05 -- iter 38500/55000, training for: 1199.99s\n",
            "Epoch 10, step (batch no.): 2136 -- acc: 0.99, loss 0.05 -- iter 39000/55000, training for: 1201.04s\n",
            "Epoch 10, step (batch no.): 2138 -- acc: 0.98, loss 0.05 -- iter 39500/55000, training for: 1202.10s\n",
            "Epoch 10, step (batch no.): 2140 -- acc: 0.98, loss 0.05 -- iter 40000/55000, training for: 1203.19s\n",
            "Epoch 10, step (batch no.): 2142 -- acc: 0.98, loss 0.06 -- iter 40500/55000, training for: 1204.30s\n",
            "Epoch 10, step (batch no.): 2144 -- acc: 0.98, loss 0.07 -- iter 41000/55000, training for: 1205.41s\n",
            "Epoch 10, step (batch no.): 2146 -- acc: 0.98, loss 0.06 -- iter 41500/55000, training for: 1206.52s\n",
            "Epoch 10, step (batch no.): 2148 -- acc: 0.98, loss 0.06 -- iter 42000/55000, training for: 1207.59s\n",
            "Epoch 10, step (batch no.): 2150 -- acc: 0.98, loss 0.06 -- iter 42500/55000, training for: 1208.68s\n",
            "Epoch 10, step (batch no.): 2152 -- acc: 0.98, loss 0.06 -- iter 43000/55000, training for: 1209.77s\n",
            "Epoch 10, step (batch no.): 2154 -- acc: 0.98, loss 0.05 -- iter 43500/55000, training for: 1210.85s\n",
            "Epoch 10, step (batch no.): 2156 -- acc: 0.98, loss 0.05 -- iter 44000/55000, training for: 1211.93s\n",
            "Epoch 10, step (batch no.): 2158 -- acc: 0.98, loss 0.05 -- iter 44500/55000, training for: 1213.09s\n",
            "Epoch 10, step (batch no.): 2160 -- acc: 0.98, loss 0.05 -- iter 45000/55000, training for: 1214.17s\n",
            "Epoch 10, step (batch no.): 2162 -- acc: 0.98, loss 0.05 -- iter 45500/55000, training for: 1215.24s\n",
            "Epoch 10, step (batch no.): 2164 -- acc: 0.98, loss 0.05 -- iter 46000/55000, training for: 1216.32s\n",
            "Epoch 10, step (batch no.): 2166 -- acc: 0.98, loss 0.05 -- iter 46500/55000, training for: 1217.39s\n",
            "Epoch 10, step (batch no.): 2168 -- acc: 0.98, loss 0.05 -- iter 47000/55000, training for: 1218.46s\n",
            "Epoch 10, step (batch no.): 2170 -- acc: 0.99, loss 0.04 -- iter 47500/55000, training for: 1219.53s\n",
            "Epoch 10, step (batch no.): 2172 -- acc: 0.99, loss 0.05 -- iter 48000/55000, training for: 1220.59s\n",
            "Epoch 10, step (batch no.): 2174 -- acc: 0.99, loss 0.04 -- iter 48500/55000, training for: 1221.65s\n",
            "Epoch 10, step (batch no.): 2176 -- acc: 0.99, loss 0.04 -- iter 49000/55000, training for: 1222.72s\n",
            "Epoch 10, step (batch no.): 2178 -- acc: 0.90, loss 1.30 -- iter 49500/55000, training for: 1223.78s\n",
            "Epoch 10, step (batch no.): 2180 -- acc: 0.91, loss 1.09 -- iter 50000/55000, training for: 1224.86s\n",
            "Epoch 10, step (batch no.): 2182 -- acc: 0.92, loss 0.92 -- iter 50500/55000, training for: 1225.91s\n",
            "Epoch 10, step (batch no.): 2184 -- acc: 0.93, loss 0.78 -- iter 51000/55000, training for: 1226.97s\n",
            "Epoch 10, step (batch no.): 2186 -- acc: 0.94, loss 0.66 -- iter 51500/55000, training for: 1228.03s\n",
            "Epoch 10, step (batch no.): 2188 -- acc: 0.95, loss 0.56 -- iter 52000/55000, training for: 1229.11s\n",
            "Epoch 10, step (batch no.): 2190 -- acc: 0.95, loss 0.47 -- iter 52500/55000, training for: 1230.19s\n",
            "Epoch 10, step (batch no.): 2192 -- acc: 0.96, loss 0.39 -- iter 53000/55000, training for: 1231.28s\n",
            "Epoch 10, step (batch no.): 2194 -- acc: 0.96, loss 0.32 -- iter 53500/55000, training for: 1232.32s\n",
            "Epoch 10, step (batch no.): 2196 -- acc: 0.97, loss 0.27 -- iter 54000/55000, training for: 1233.40s\n",
            "Epoch 10, step (batch no.): 2198 -- acc: 0.97, loss 0.23 -- iter 54500/55000, training for: 1234.44s\n",
            "Training Step: 2200  | total loss: \u001b[1m\u001b[32m0.18922\u001b[0m\u001b[0m | time: 123.558s\n",
            "| Adam | epoch: 010 | loss: 0.18922 - acc: 0.9761 | val_loss: 0.04550 - val_acc: 0.9870 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 10, step (batch no.): 2200 -- acc: 0.98, loss 0.19 -- iter 55000/55000, training for: 1240.86s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "ghlCmbtakulz",
        "outputId": "d0cdbd21-ce62-4085-998b-241fa2d9b705"
      },
      "source": [
        "vall_acc_for_epochs =[metrics['val_acc'] for metrics in scores.epoch_data]\n",
        "\n",
        "data = pd.DataFrame(vall_acc_for_epochs)\n",
        "data.plot(label='accuracy')\n",
        "plt.legend([\"validation_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f123e532610>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-gTz-gIWXYC"
      },
      "source": [
        "# Zadanie B - funkcje aktywacji\n",
        "\n",
        "Narysuj wykres zależności accuracy dla danych **walidacyjnych** (val_acc) od liczby epok, dla sieci konwolucyjnej: \n",
        "32-3x3, podpróbkowanie x2, 32-3x3, podpróbkowanie x2, 32-3x3, z wartstwami pełnymi: 128, 256, 10\n",
        "\n",
        "(przez 32-3x3 rozumiemy warstwę konwolucyjną z 32 filtrami rozmiaru 3x3)\n",
        "\n",
        "w zależności od funkcji aktywacji (ustawianej w parametrze `activation=`):\n",
        "* `'relu'`\n",
        "* `'sigmoid'`\n",
        "* `'elu'`\n",
        "\n",
        "** Uwaga! ** Nie należy zmieniać funkcji aktywacji `softmax` w ostatniej warstwie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU71ewdBlFWJ"
      },
      "source": [
        "\n",
        "def create_model(activation='relu'):\n",
        "  network = input_data(shape=[None, 28, 28, 1], name='input') # None oznacza, że ta \n",
        "\n",
        "  network = conv_2d(network, 32, 3, activation=activation, regularizer=\"L2\")\n",
        "  network = max_pool_2d(network, 2) # teraz obrazki są [14x14]\n",
        "\n",
        "\n",
        "  network = conv_2d(network, 32, 3, activation=activation, regularizer=\"L2\")\n",
        "  network = max_pool_2d(network, 2) # teraz obrazki są [7x7]\n",
        "\n",
        "  network = conv_2d(network, 32, 3, activation=activation, regularizer=\"L2\")\n",
        "\n",
        "  network = fully_connected(network, 128, activation=activation) # 128 neuronów, aktywacja \"relu\", przyjmuje wejście [7x7] do [128] neuronów\n",
        "  network = fully_connected(network, 256, activation=activation) # 256 neuronów, aktywacja \"relu\"\n",
        "\n",
        "  network = fully_connected(network, 10, activation='softmax')\n",
        "\n",
        "  network = regression(network, optimizer='adam', learning_rate=0.01,\n",
        "                     batch_size=250,\n",
        "                     loss='categorical_crossentropy', name='target')\n",
        "  return network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GVJWhav6lmvC",
        "outputId": "47a407d6-f452-4164-b2cd-6ec13bdd5f43"
      },
      "source": [
        "for activation in {'relu','sigmoid','elu'}:\n",
        "  tf.reset_default_graph()\n",
        "  # Wyłączamy warningi z tensorflow\n",
        "  tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "  network = create_model(activation=activation)\n",
        "  # uruchamiamy trenowanie naszej sieci\n",
        "  # n_epoch - liczba epok, czyli przejść przez cały zbiór danych treningowych\n",
        "  scores = Stats(examples=len(X))\n",
        "  model = tflearn.DNN(network, tensorboard_verbose=0)\n",
        "  model.fit({'input': X}, {'target': Y},\n",
        "            n_epoch=EPOCHS,  # <-- DO ZMIANY\n",
        "            validation_set=({'input': testX}, {'target': testY}), show_metric=True, callbacks=[scores])\n",
        "  vall_acc_for_epochs =[metrics['val_acc'] for metrics in scores.epoch_data]\n",
        "\n",
        "  data = pd.DataFrame(vall_acc_for_epochs)\n",
        "  data.plot(label='accuracy')\n",
        "  plt.title(f'Validation accuracy by epoch for {activation} activation function.')\n",
        "  plt.legend([\"validation_accuracy\"])\n",
        "  plt.show()\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: 97PNRW\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "---------------------------------\n",
            "Training samples: 55000\n",
            "Validation samples: 10000\n",
            "--\n",
            "Epoch 1, step (batch no.): 1 -- acc: 0.00, loss 0.00 -- iter 00250/55000, training for: 0.31s\n",
            "Epoch 1, step (batch no.): 7 -- acc: 0.24, loss 2.24 -- iter 01750/55000, training for: 1.38s\n",
            "Epoch 1, step (batch no.): 13 -- acc: 0.43, loss 1.55 -- iter 03250/55000, training for: 2.42s\n",
            "Epoch 1, step (batch no.): 19 -- acc: 0.67, loss 0.91 -- iter 04750/55000, training for: 3.48s\n",
            "Epoch 1, step (batch no.): 25 -- acc: 0.80, loss 0.61 -- iter 06250/55000, training for: 4.50s\n",
            "Epoch 1, step (batch no.): 31 -- acc: 0.87, loss 0.41 -- iter 07750/55000, training for: 5.55s\n",
            "Epoch 1, step (batch no.): 37 -- acc: 0.89, loss 0.36 -- iter 09250/55000, training for: 6.58s\n",
            "Epoch 1, step (batch no.): 43 -- acc: 0.91, loss 0.28 -- iter 10750/55000, training for: 7.61s\n",
            "Epoch 1, step (batch no.): 49 -- acc: 0.93, loss 0.23 -- iter 12250/55000, training for: 8.63s\n",
            "Epoch 1, step (batch no.): 55 -- acc: 0.95, loss 0.17 -- iter 13750/55000, training for: 9.65s\n",
            "Epoch 1, step (batch no.): 61 -- acc: 0.94, loss 0.19 -- iter 15250/55000, training for: 10.66s\n",
            "Epoch 1, step (batch no.): 67 -- acc: 0.94, loss 0.19 -- iter 16750/55000, training for: 11.67s\n",
            "Epoch 1, step (batch no.): 73 -- acc: 0.94, loss 0.20 -- iter 18250/55000, training for: 12.67s\n",
            "Epoch 1, step (batch no.): 79 -- acc: 0.95, loss 0.17 -- iter 19750/55000, training for: 13.68s\n",
            "Epoch 1, step (batch no.): 85 -- acc: 0.96, loss 0.15 -- iter 21250/55000, training for: 14.69s\n",
            "Epoch 1, step (batch no.): 91 -- acc: 0.96, loss 0.13 -- iter 22750/55000, training for: 15.71s\n",
            "Epoch 1, step (batch no.): 97 -- acc: 0.96, loss 0.13 -- iter 24250/55000, training for: 16.72s\n",
            "Epoch 1, step (batch no.): 103 -- acc: 0.96, loss 0.12 -- iter 25750/55000, training for: 17.73s\n",
            "Epoch 1, step (batch no.): 109 -- acc: 0.96, loss 0.11 -- iter 27250/55000, training for: 18.74s\n",
            "Epoch 1, step (batch no.): 115 -- acc: 0.97, loss 0.10 -- iter 28750/55000, training for: 19.75s\n",
            "Epoch 1, step (batch no.): 121 -- acc: 0.97, loss 0.11 -- iter 30250/55000, training for: 20.76s\n",
            "Epoch 1, step (batch no.): 127 -- acc: 0.97, loss 0.11 -- iter 31750/55000, training for: 21.77s\n",
            "Epoch 1, step (batch no.): 133 -- acc: 0.97, loss 0.10 -- iter 33250/55000, training for: 22.82s\n",
            "Epoch 1, step (batch no.): 140 -- acc: 0.97, loss 0.09 -- iter 35000/55000, training for: 23.99s\n",
            "Epoch 1, step (batch no.): 146 -- acc: 0.97, loss 0.09 -- iter 36500/55000, training for: 25.00s\n",
            "Epoch 1, step (batch no.): 152 -- acc: 0.97, loss 0.10 -- iter 38000/55000, training for: 26.02s\n",
            "Epoch 1, step (batch no.): 158 -- acc: 0.97, loss 0.12 -- iter 39500/55000, training for: 27.03s\n",
            "Epoch 1, step (batch no.): 164 -- acc: 0.97, loss 0.10 -- iter 41000/55000, training for: 28.03s\n",
            "Epoch 1, step (batch no.): 170 -- acc: 0.97, loss 0.09 -- iter 42500/55000, training for: 29.05s\n",
            "Epoch 1, step (batch no.): 176 -- acc: 0.97, loss 0.09 -- iter 44000/55000, training for: 30.07s\n",
            "Epoch 1, step (batch no.): 182 -- acc: 0.97, loss 0.08 -- iter 45500/55000, training for: 31.09s\n",
            "Epoch 1, step (batch no.): 188 -- acc: 0.98, loss 0.08 -- iter 47000/55000, training for: 32.11s\n",
            "Epoch 1, step (batch no.): 194 -- acc: 0.92, loss 0.90 -- iter 48500/55000, training for: 33.14s\n",
            "Epoch 1, step (batch no.): 200 -- acc: 0.95, loss 0.52 -- iter 50000/55000, training for: 34.15s\n",
            "Epoch 1, step (batch no.): 206 -- acc: 0.96, loss 0.32 -- iter 51500/55000, training for: 35.20s\n",
            "Epoch 1, step (batch no.): 212 -- acc: 0.96, loss 0.25 -- iter 53000/55000, training for: 36.23s\n",
            "Epoch 1, step (batch no.): 218 -- acc: 0.96, loss 0.22 -- iter 54500/55000, training for: 37.24s\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.21676\u001b[0m\u001b[0m | time: 39.643s\n",
            "| Adam | epoch: 001 | loss: 0.21676 - acc: 0.9516 | val_loss: 0.11658 - val_acc: 0.9658 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 1, step (batch no.): 220 -- acc: 0.95, loss 0.22 -- iter 55000/55000, training for: 39.66s\n",
            "Epoch 2, step (batch no.): 226 -- acc: 0.96, loss 0.18 -- iter 01500/55000, training for: 40.67s\n",
            "Epoch 2, step (batch no.): 232 -- acc: 0.96, loss 0.14 -- iter 03000/55000, training for: 41.69s\n",
            "Epoch 2, step (batch no.): 238 -- acc: 0.97, loss 0.11 -- iter 04500/55000, training for: 42.69s\n",
            "Epoch 2, step (batch no.): 244 -- acc: 0.97, loss 0.10 -- iter 06000/55000, training for: 43.69s\n",
            "Epoch 2, step (batch no.): 250 -- acc: 0.97, loss 0.09 -- iter 07500/55000, training for: 44.69s\n",
            "Epoch 2, step (batch no.): 256 -- acc: 0.97, loss 0.09 -- iter 09000/55000, training for: 45.70s\n",
            "Epoch 2, step (batch no.): 262 -- acc: 0.98, loss 0.08 -- iter 10500/55000, training for: 46.71s\n",
            "Epoch 2, step (batch no.): 268 -- acc: 0.98, loss 0.08 -- iter 12000/55000, training for: 47.73s\n",
            "Epoch 2, step (batch no.): 274 -- acc: 0.97, loss 0.08 -- iter 13500/55000, training for: 48.74s\n",
            "Epoch 2, step (batch no.): 280 -- acc: 0.98, loss 0.07 -- iter 15000/55000, training for: 49.77s\n",
            "Epoch 2, step (batch no.): 286 -- acc: 0.98, loss 0.08 -- iter 16500/55000, training for: 50.79s\n",
            "Epoch 2, step (batch no.): 292 -- acc: 0.98, loss 0.07 -- iter 18000/55000, training for: 51.80s\n",
            "Epoch 2, step (batch no.): 298 -- acc: 0.98, loss 0.08 -- iter 19500/55000, training for: 52.82s\n",
            "Epoch 2, step (batch no.): 304 -- acc: 0.98, loss 0.08 -- iter 21000/55000, training for: 53.83s\n",
            "Epoch 2, step (batch no.): 310 -- acc: 0.98, loss 0.07 -- iter 22500/55000, training for: 54.83s\n",
            "Epoch 2, step (batch no.): 316 -- acc: 0.98, loss 0.06 -- iter 24000/55000, training for: 55.83s\n",
            "Epoch 2, step (batch no.): 322 -- acc: 0.98, loss 0.07 -- iter 25500/55000, training for: 56.85s\n",
            "Epoch 2, step (batch no.): 329 -- acc: 0.98, loss 0.06 -- iter 27250/55000, training for: 58.01s\n",
            "Epoch 2, step (batch no.): 335 -- acc: 0.98, loss 0.07 -- iter 28750/55000, training for: 59.01s\n",
            "Epoch 2, step (batch no.): 341 -- acc: 0.98, loss 0.06 -- iter 30250/55000, training for: 60.03s\n",
            "Epoch 2, step (batch no.): 347 -- acc: 0.98, loss 0.07 -- iter 31750/55000, training for: 61.06s\n",
            "Epoch 2, step (batch no.): 353 -- acc: 0.98, loss 0.07 -- iter 33250/55000, training for: 62.07s\n",
            "Epoch 2, step (batch no.): 359 -- acc: 0.98, loss 0.07 -- iter 34750/55000, training for: 63.09s\n",
            "Epoch 2, step (batch no.): 365 -- acc: 0.98, loss 0.07 -- iter 36250/55000, training for: 64.09s\n",
            "Epoch 2, step (batch no.): 371 -- acc: 0.98, loss 0.06 -- iter 37750/55000, training for: 65.11s\n",
            "Epoch 2, step (batch no.): 377 -- acc: 0.98, loss 0.06 -- iter 39250/55000, training for: 66.12s\n",
            "Epoch 2, step (batch no.): 383 -- acc: 0.98, loss 0.06 -- iter 40750/55000, training for: 67.13s\n",
            "Epoch 2, step (batch no.): 389 -- acc: 0.98, loss 0.06 -- iter 42250/55000, training for: 68.15s\n",
            "Epoch 2, step (batch no.): 395 -- acc: 0.98, loss 0.06 -- iter 43750/55000, training for: 69.19s\n",
            "Epoch 2, step (batch no.): 401 -- acc: 0.98, loss 0.06 -- iter 45250/55000, training for: 70.22s\n",
            "Epoch 2, step (batch no.): 407 -- acc: 0.98, loss 0.07 -- iter 46750/55000, training for: 71.23s\n",
            "Epoch 2, step (batch no.): 413 -- acc: 0.98, loss 0.07 -- iter 48250/55000, training for: 72.24s\n",
            "Epoch 2, step (batch no.): 419 -- acc: 0.98, loss 0.06 -- iter 49750/55000, training for: 73.25s\n",
            "Epoch 2, step (batch no.): 426 -- acc: 0.98, loss 0.06 -- iter 51500/55000, training for: 74.41s\n",
            "Epoch 2, step (batch no.): 432 -- acc: 0.98, loss 0.05 -- iter 53000/55000, training for: 75.42s\n",
            "Epoch 2, step (batch no.): 438 -- acc: 0.98, loss 0.05 -- iter 54500/55000, training for: 76.43s\n",
            "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.04802\u001b[0m\u001b[0m | time: 39.109s\n",
            "| Adam | epoch: 002 | loss: 0.04802 - acc: 0.9851 | val_loss: 0.05633 - val_acc: 0.9828 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 2, step (batch no.): 440 -- acc: 0.99, loss 0.05 -- iter 55000/55000, training for: 78.79s\n",
            "Epoch 3, step (batch no.): 446 -- acc: 0.99, loss 0.05 -- iter 01500/55000, training for: 79.82s\n",
            "Epoch 3, step (batch no.): 452 -- acc: 0.99, loss 0.04 -- iter 03000/55000, training for: 80.85s\n",
            "Epoch 3, step (batch no.): 458 -- acc: 0.99, loss 0.04 -- iter 04500/55000, training for: 81.86s\n",
            "Epoch 3, step (batch no.): 464 -- acc: 0.99, loss 0.04 -- iter 06000/55000, training for: 82.90s\n",
            "Epoch 3, step (batch no.): 470 -- acc: 0.99, loss 0.04 -- iter 07500/55000, training for: 83.93s\n",
            "Epoch 3, step (batch no.): 476 -- acc: 0.99, loss 0.05 -- iter 09000/55000, training for: 84.95s\n",
            "Epoch 3, step (batch no.): 482 -- acc: 0.99, loss 0.05 -- iter 10500/55000, training for: 85.98s\n",
            "Epoch 3, step (batch no.): 488 -- acc: 0.98, loss 0.06 -- iter 12000/55000, training for: 87.00s\n",
            "Epoch 3, step (batch no.): 494 -- acc: 0.98, loss 0.06 -- iter 13500/55000, training for: 88.03s\n",
            "Epoch 3, step (batch no.): 500 -- acc: 0.98, loss 0.05 -- iter 15000/55000, training for: 89.06s\n",
            "Epoch 3, step (batch no.): 506 -- acc: 0.98, loss 0.05 -- iter 16500/55000, training for: 90.08s\n",
            "Epoch 3, step (batch no.): 512 -- acc: 0.98, loss 0.06 -- iter 18000/55000, training for: 91.11s\n",
            "Epoch 3, step (batch no.): 518 -- acc: 0.98, loss 0.06 -- iter 19500/55000, training for: 92.12s\n",
            "Epoch 3, step (batch no.): 524 -- acc: 0.98, loss 0.06 -- iter 21000/55000, training for: 93.13s\n",
            "Epoch 3, step (batch no.): 530 -- acc: 0.98, loss 0.06 -- iter 22500/55000, training for: 94.15s\n",
            "Epoch 3, step (batch no.): 536 -- acc: 0.98, loss 0.06 -- iter 24000/55000, training for: 95.18s\n",
            "Epoch 3, step (batch no.): 542 -- acc: 0.98, loss 0.06 -- iter 25500/55000, training for: 96.20s\n",
            "Epoch 3, step (batch no.): 548 -- acc: 0.98, loss 0.06 -- iter 27000/55000, training for: 97.22s\n",
            "Epoch 3, step (batch no.): 554 -- acc: 0.98, loss 0.06 -- iter 28500/55000, training for: 98.26s\n",
            "Epoch 3, step (batch no.): 560 -- acc: 0.98, loss 0.06 -- iter 30000/55000, training for: 99.27s\n",
            "Epoch 3, step (batch no.): 566 -- acc: 0.98, loss 0.07 -- iter 31500/55000, training for: 100.28s\n",
            "Epoch 3, step (batch no.): 572 -- acc: 0.98, loss 0.07 -- iter 33000/55000, training for: 101.32s\n",
            "Epoch 3, step (batch no.): 578 -- acc: 0.98, loss 0.06 -- iter 34500/55000, training for: 102.35s\n",
            "Epoch 3, step (batch no.): 584 -- acc: 0.98, loss 0.06 -- iter 36000/55000, training for: 103.36s\n",
            "Epoch 3, step (batch no.): 590 -- acc: 0.98, loss 0.06 -- iter 37500/55000, training for: 104.38s\n",
            "Epoch 3, step (batch no.): 596 -- acc: 0.99, loss 0.05 -- iter 39000/55000, training for: 105.38s\n",
            "Epoch 3, step (batch no.): 602 -- acc: 0.98, loss 0.05 -- iter 40500/55000, training for: 106.40s\n",
            "Epoch 3, step (batch no.): 608 -- acc: 0.99, loss 0.05 -- iter 42000/55000, training for: 107.40s\n",
            "Epoch 3, step (batch no.): 614 -- acc: 0.98, loss 0.05 -- iter 43500/55000, training for: 108.40s\n",
            "Epoch 3, step (batch no.): 620 -- acc: 0.98, loss 0.06 -- iter 45000/55000, training for: 109.42s\n",
            "Epoch 3, step (batch no.): 626 -- acc: 0.98, loss 0.06 -- iter 46500/55000, training for: 110.42s\n",
            "Epoch 3, step (batch no.): 632 -- acc: 0.90, loss 1.10 -- iter 48000/55000, training for: 111.44s\n",
            "Epoch 3, step (batch no.): 639 -- acc: 0.94, loss 0.66 -- iter 49750/55000, training for: 112.60s\n",
            "Epoch 3, step (batch no.): 645 -- acc: 0.96, loss 0.39 -- iter 51250/55000, training for: 113.63s\n",
            "Epoch 3, step (batch no.): 651 -- acc: 0.97, loss 0.24 -- iter 52750/55000, training for: 114.64s\n",
            "Epoch 3, step (batch no.): 657 -- acc: 0.97, loss 0.18 -- iter 54250/55000, training for: 115.65s\n",
            "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.16142\u001b[0m\u001b[0m | time: 39.362s\n",
            "| Adam | epoch: 003 | loss: 0.16142 - acc: 0.9734 | val_loss: 0.08648 - val_acc: 0.9771 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 3, step (batch no.): 660 -- acc: 0.97, loss 0.16 -- iter 55000/55000, training for: 118.16s\n",
            "Epoch 4, step (batch no.): 666 -- acc: 0.98, loss 0.11 -- iter 01500/55000, training for: 119.17s\n",
            "Epoch 4, step (batch no.): 672 -- acc: 0.98, loss 0.10 -- iter 03000/55000, training for: 120.17s\n",
            "Epoch 4, step (batch no.): 678 -- acc: 0.98, loss 0.07 -- iter 04500/55000, training for: 121.21s\n",
            "Epoch 4, step (batch no.): 685 -- acc: 0.98, loss 0.06 -- iter 06250/55000, training for: 122.37s\n",
            "Epoch 4, step (batch no.): 691 -- acc: 0.98, loss 0.06 -- iter 07750/55000, training for: 123.39s\n",
            "Epoch 4, step (batch no.): 697 -- acc: 0.98, loss 0.05 -- iter 09250/55000, training for: 124.40s\n",
            "Epoch 4, step (batch no.): 703 -- acc: 0.99, loss 0.05 -- iter 10750/55000, training for: 125.42s\n",
            "Epoch 4, step (batch no.): 709 -- acc: 0.99, loss 0.05 -- iter 12250/55000, training for: 126.42s\n",
            "Epoch 4, step (batch no.): 715 -- acc: 0.99, loss 0.05 -- iter 13750/55000, training for: 127.42s\n",
            "Epoch 4, step (batch no.): 721 -- acc: 0.98, loss 0.06 -- iter 15250/55000, training for: 128.44s\n",
            "Epoch 4, step (batch no.): 727 -- acc: 0.99, loss 0.05 -- iter 16750/55000, training for: 129.46s\n",
            "Epoch 4, step (batch no.): 733 -- acc: 0.99, loss 0.05 -- iter 18250/55000, training for: 130.48s\n",
            "Epoch 4, step (batch no.): 739 -- acc: 0.99, loss 0.05 -- iter 19750/55000, training for: 131.50s\n",
            "Epoch 4, step (batch no.): 745 -- acc: 0.98, loss 0.05 -- iter 21250/55000, training for: 132.52s\n",
            "Epoch 4, step (batch no.): 751 -- acc: 0.99, loss 0.05 -- iter 22750/55000, training for: 133.53s\n",
            "Epoch 4, step (batch no.): 757 -- acc: 0.99, loss 0.05 -- iter 24250/55000, training for: 134.56s\n",
            "Epoch 4, step (batch no.): 763 -- acc: 0.99, loss 0.05 -- iter 25750/55000, training for: 135.64s\n",
            "Epoch 4, step (batch no.): 769 -- acc: 0.99, loss 0.04 -- iter 27250/55000, training for: 136.65s\n",
            "Epoch 4, step (batch no.): 776 -- acc: 0.99, loss 0.04 -- iter 29000/55000, training for: 137.82s\n",
            "Epoch 4, step (batch no.): 782 -- acc: 0.99, loss 0.04 -- iter 30500/55000, training for: 138.83s\n",
            "Epoch 4, step (batch no.): 788 -- acc: 0.99, loss 0.04 -- iter 32000/55000, training for: 139.85s\n",
            "Epoch 4, step (batch no.): 794 -- acc: 0.99, loss 0.04 -- iter 33500/55000, training for: 140.85s\n",
            "Epoch 4, step (batch no.): 800 -- acc: 0.99, loss 0.04 -- iter 35000/55000, training for: 141.87s\n",
            "Epoch 4, step (batch no.): 806 -- acc: 0.98, loss 0.06 -- iter 36500/55000, training for: 142.91s\n",
            "Epoch 4, step (batch no.): 812 -- acc: 0.98, loss 0.05 -- iter 38000/55000, training for: 143.93s\n",
            "Epoch 4, step (batch no.): 818 -- acc: 0.99, loss 0.05 -- iter 39500/55000, training for: 145.07s\n",
            "Epoch 4, step (batch no.): 824 -- acc: 0.99, loss 0.04 -- iter 41000/55000, training for: 146.09s\n",
            "Epoch 4, step (batch no.): 830 -- acc: 0.98, loss 0.06 -- iter 42500/55000, training for: 147.10s\n",
            "Epoch 4, step (batch no.): 836 -- acc: 0.99, loss 0.05 -- iter 44000/55000, training for: 148.12s\n",
            "Epoch 4, step (batch no.): 842 -- acc: 0.98, loss 0.05 -- iter 45500/55000, training for: 149.14s\n",
            "Epoch 4, step (batch no.): 848 -- acc: 0.99, loss 0.05 -- iter 47000/55000, training for: 150.17s\n",
            "Epoch 4, step (batch no.): 854 -- acc: 0.91, loss 1.10 -- iter 48500/55000, training for: 151.24s\n",
            "Epoch 4, step (batch no.): 860 -- acc: 0.94, loss 0.71 -- iter 50000/55000, training for: 152.25s\n",
            "Epoch 4, step (batch no.): 866 -- acc: 0.96, loss 0.41 -- iter 51500/55000, training for: 153.26s\n",
            "Epoch 4, step (batch no.): 872 -- acc: 0.97, loss 0.24 -- iter 53000/55000, training for: 154.28s\n",
            "Epoch 4, step (batch no.): 878 -- acc: 0.97, loss 0.17 -- iter 54500/55000, training for: 155.29s\n",
            "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.15221\u001b[0m\u001b[0m | time: 39.481s\n",
            "| Adam | epoch: 004 | loss: 0.15221 - acc: 0.9746 | val_loss: 0.07636 - val_acc: 0.9811 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 4, step (batch no.): 880 -- acc: 0.97, loss 0.15 -- iter 55000/55000, training for: 157.66s\n",
            "Epoch 5, step (batch no.): 886 -- acc: 0.98, loss 0.11 -- iter 01500/55000, training for: 158.67s\n",
            "Epoch 5, step (batch no.): 892 -- acc: 0.98, loss 0.09 -- iter 03000/55000, training for: 159.68s\n",
            "Epoch 5, step (batch no.): 899 -- acc: 0.98, loss 0.07 -- iter 04750/55000, training for: 160.86s\n",
            "Epoch 5, step (batch no.): 905 -- acc: 0.99, loss 0.05 -- iter 06250/55000, training for: 161.89s\n",
            "Epoch 5, step (batch no.): 911 -- acc: 0.99, loss 0.04 -- iter 07750/55000, training for: 162.93s\n",
            "Epoch 5, step (batch no.): 917 -- acc: 0.99, loss 0.05 -- iter 09250/55000, training for: 163.95s\n",
            "Epoch 5, step (batch no.): 923 -- acc: 0.99, loss 0.05 -- iter 10750/55000, training for: 164.96s\n",
            "Epoch 5, step (batch no.): 929 -- acc: 0.99, loss 0.05 -- iter 12250/55000, training for: 165.99s\n",
            "Epoch 5, step (batch no.): 935 -- acc: 0.98, loss 0.05 -- iter 13750/55000, training for: 167.03s\n",
            "Epoch 5, step (batch no.): 941 -- acc: 0.99, loss 0.05 -- iter 15250/55000, training for: 168.05s\n",
            "Epoch 5, step (batch no.): 947 -- acc: 0.99, loss 0.05 -- iter 16750/55000, training for: 169.06s\n",
            "Epoch 5, step (batch no.): 953 -- acc: 0.99, loss 0.05 -- iter 18250/55000, training for: 170.08s\n",
            "Epoch 5, step (batch no.): 959 -- acc: 0.99, loss 0.05 -- iter 19750/55000, training for: 171.10s\n",
            "Epoch 5, step (batch no.): 966 -- acc: 0.99, loss 0.04 -- iter 21500/55000, training for: 172.27s\n",
            "Epoch 5, step (batch no.): 972 -- acc: 0.99, loss 0.04 -- iter 23000/55000, training for: 173.31s\n",
            "Epoch 5, step (batch no.): 978 -- acc: 0.99, loss 0.05 -- iter 24500/55000, training for: 174.38s\n",
            "Epoch 5, step (batch no.): 984 -- acc: 0.98, loss 0.06 -- iter 26000/55000, training for: 175.39s\n",
            "Epoch 5, step (batch no.): 990 -- acc: 0.99, loss 0.05 -- iter 27500/55000, training for: 176.40s\n",
            "Epoch 5, step (batch no.): 997 -- acc: 0.98, loss 0.05 -- iter 29250/55000, training for: 177.56s\n",
            "Epoch 5, step (batch no.): 1003 -- acc: 0.98, loss 0.06 -- iter 30750/55000, training for: 178.56s\n",
            "Epoch 5, step (batch no.): 1010 -- acc: 0.98, loss 0.05 -- iter 32500/55000, training for: 179.72s\n",
            "Epoch 5, step (batch no.): 1017 -- acc: 0.98, loss 0.05 -- iter 34250/55000, training for: 180.89s\n",
            "Epoch 5, step (batch no.): 1023 -- acc: 0.98, loss 0.05 -- iter 35750/55000, training for: 181.90s\n",
            "Epoch 5, step (batch no.): 1029 -- acc: 0.98, loss 0.05 -- iter 37250/55000, training for: 182.90s\n",
            "Epoch 5, step (batch no.): 1035 -- acc: 0.99, loss 0.05 -- iter 38750/55000, training for: 183.92s\n",
            "Epoch 5, step (batch no.): 1041 -- acc: 0.99, loss 0.05 -- iter 40250/55000, training for: 184.93s\n",
            "Epoch 5, step (batch no.): 1047 -- acc: 0.99, loss 0.05 -- iter 41750/55000, training for: 185.93s\n",
            "Epoch 5, step (batch no.): 1053 -- acc: 0.99, loss 0.04 -- iter 43250/55000, training for: 186.94s\n",
            "Epoch 5, step (batch no.): 1059 -- acc: 0.99, loss 0.04 -- iter 44750/55000, training for: 187.95s\n",
            "Epoch 5, step (batch no.): 1065 -- acc: 0.99, loss 0.04 -- iter 46250/55000, training for: 188.95s\n",
            "Epoch 5, step (batch no.): 1071 -- acc: 0.99, loss 0.04 -- iter 47750/55000, training for: 189.96s\n",
            "Epoch 5, step (batch no.): 1077 -- acc: 0.99, loss 0.03 -- iter 49250/55000, training for: 190.97s\n",
            "Epoch 5, step (batch no.): 1083 -- acc: 0.99, loss 0.03 -- iter 50750/55000, training for: 191.98s\n",
            "Epoch 5, step (batch no.): 1089 -- acc: 0.99, loss 0.03 -- iter 52250/55000, training for: 193.00s\n",
            "Epoch 5, step (batch no.): 1095 -- acc: 0.99, loss 0.03 -- iter 53750/55000, training for: 194.01s\n",
            "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.03241\u001b[0m\u001b[0m | time: 39.275s\n",
            "| Adam | epoch: 005 | loss: 0.03241 - acc: 0.9893 | val_loss: 0.04335 - val_acc: 0.9871 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 5, step (batch no.): 1100 -- acc: 0.99, loss 0.03 -- iter 55000/55000, training for: 196.95s\n",
            "Epoch 6, step (batch no.): 1106 -- acc: 0.99, loss 0.03 -- iter 01500/55000, training for: 197.97s\n",
            "Epoch 6, step (batch no.): 1112 -- acc: 0.99, loss 0.04 -- iter 03000/55000, training for: 198.98s\n",
            "Epoch 6, step (batch no.): 1118 -- acc: 0.99, loss 0.05 -- iter 04500/55000, training for: 200.04s\n",
            "Epoch 6, step (batch no.): 1124 -- acc: 0.99, loss 0.04 -- iter 06000/55000, training for: 201.08s\n",
            "Epoch 6, step (batch no.): 1130 -- acc: 0.99, loss 0.04 -- iter 07500/55000, training for: 202.08s\n",
            "Epoch 6, step (batch no.): 1136 -- acc: 0.99, loss 0.03 -- iter 09000/55000, training for: 203.12s\n",
            "Epoch 6, step (batch no.): 1142 -- acc: 0.99, loss 0.03 -- iter 10500/55000, training for: 204.12s\n",
            "Epoch 6, step (batch no.): 1148 -- acc: 0.99, loss 0.04 -- iter 12000/55000, training for: 205.15s\n",
            "Epoch 6, step (batch no.): 1154 -- acc: 0.99, loss 0.04 -- iter 13500/55000, training for: 206.15s\n",
            "Epoch 6, step (batch no.): 1160 -- acc: 0.99, loss 0.04 -- iter 15000/55000, training for: 207.18s\n",
            "Epoch 6, step (batch no.): 1166 -- acc: 0.99, loss 0.04 -- iter 16500/55000, training for: 208.19s\n",
            "Epoch 6, step (batch no.): 1172 -- acc: 0.99, loss 0.03 -- iter 18000/55000, training for: 209.21s\n",
            "Epoch 6, step (batch no.): 1178 -- acc: 0.99, loss 0.03 -- iter 19500/55000, training for: 210.21s\n",
            "Epoch 6, step (batch no.): 1184 -- acc: 0.99, loss 0.04 -- iter 21000/55000, training for: 211.24s\n",
            "Epoch 6, step (batch no.): 1190 -- acc: 0.99, loss 0.04 -- iter 22500/55000, training for: 212.27s\n",
            "Epoch 6, step (batch no.): 1196 -- acc: 0.99, loss 0.03 -- iter 24000/55000, training for: 213.27s\n",
            "Epoch 6, step (batch no.): 1203 -- acc: 0.99, loss 0.04 -- iter 25750/55000, training for: 214.43s\n",
            "Epoch 6, step (batch no.): 1209 -- acc: 0.99, loss 0.03 -- iter 27250/55000, training for: 215.46s\n",
            "Epoch 6, step (batch no.): 1215 -- acc: 0.99, loss 0.04 -- iter 28750/55000, training for: 216.49s\n",
            "Epoch 6, step (batch no.): 1221 -- acc: 0.99, loss 0.04 -- iter 30250/55000, training for: 217.50s\n",
            "Epoch 6, step (batch no.): 1228 -- acc: 0.99, loss 0.04 -- iter 32000/55000, training for: 218.65s\n",
            "Epoch 6, step (batch no.): 1234 -- acc: 0.99, loss 0.04 -- iter 33500/55000, training for: 219.66s\n",
            "Epoch 6, step (batch no.): 1241 -- acc: 0.98, loss 0.04 -- iter 35250/55000, training for: 220.82s\n",
            "Epoch 6, step (batch no.): 1247 -- acc: 0.99, loss 0.04 -- iter 36750/55000, training for: 221.83s\n",
            "Epoch 6, step (batch no.): 1253 -- acc: 0.99, loss 0.03 -- iter 38250/55000, training for: 222.84s\n",
            "Epoch 6, step (batch no.): 1259 -- acc: 0.99, loss 0.04 -- iter 39750/55000, training for: 223.85s\n",
            "Epoch 6, step (batch no.): 1265 -- acc: 0.99, loss 0.03 -- iter 41250/55000, training for: 224.85s\n",
            "Epoch 6, step (batch no.): 1271 -- acc: 0.99, loss 0.03 -- iter 42750/55000, training for: 225.89s\n",
            "Epoch 6, step (batch no.): 1277 -- acc: 0.99, loss 0.03 -- iter 44250/55000, training for: 226.89s\n",
            "Epoch 6, step (batch no.): 1283 -- acc: 0.99, loss 0.04 -- iter 45750/55000, training for: 227.91s\n",
            "Epoch 6, step (batch no.): 1289 -- acc: 0.99, loss 0.04 -- iter 47250/55000, training for: 228.96s\n",
            "Epoch 6, step (batch no.): 1295 -- acc: 0.99, loss 0.04 -- iter 48750/55000, training for: 229.98s\n",
            "Epoch 6, step (batch no.): 1301 -- acc: 0.99, loss 0.03 -- iter 50250/55000, training for: 231.01s\n",
            "Epoch 6, step (batch no.): 1307 -- acc: 0.99, loss 0.03 -- iter 51750/55000, training for: 232.05s\n",
            "Epoch 6, step (batch no.): 1313 -- acc: 0.99, loss 0.03 -- iter 53250/55000, training for: 233.06s\n",
            "Epoch 6, step (batch no.): 1319 -- acc: 0.99, loss 0.02 -- iter 54750/55000, training for: 234.17s\n",
            "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.02233\u001b[0m\u001b[0m | time: 39.431s\n",
            "| Adam | epoch: 006 | loss: 0.02233 - acc: 0.9933 | val_loss: 0.03823 - val_acc: 0.9892 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 6, step (batch no.): 1320 -- acc: 0.99, loss 0.02 -- iter 55000/55000, training for: 236.40s\n",
            "Epoch 7, step (batch no.): 1327 -- acc: 0.99, loss 0.03 -- iter 01750/55000, training for: 237.57s\n",
            "Epoch 7, step (batch no.): 1333 -- acc: 0.99, loss 0.03 -- iter 03250/55000, training for: 238.57s\n",
            "Epoch 7, step (batch no.): 1339 -- acc: 0.99, loss 0.05 -- iter 04750/55000, training for: 239.58s\n",
            "Epoch 7, step (batch no.): 1345 -- acc: 0.99, loss 0.04 -- iter 06250/55000, training for: 240.58s\n",
            "Epoch 7, step (batch no.): 1351 -- acc: 0.99, loss 0.04 -- iter 07750/55000, training for: 241.59s\n",
            "Epoch 7, step (batch no.): 1357 -- acc: 0.99, loss 0.03 -- iter 09250/55000, training for: 242.62s\n",
            "Epoch 7, step (batch no.): 1363 -- acc: 0.99, loss 0.03 -- iter 10750/55000, training for: 243.63s\n",
            "Epoch 7, step (batch no.): 1369 -- acc: 0.99, loss 0.03 -- iter 12250/55000, training for: 244.64s\n",
            "Epoch 7, step (batch no.): 1375 -- acc: 0.99, loss 0.03 -- iter 13750/55000, training for: 245.66s\n",
            "Epoch 7, step (batch no.): 1381 -- acc: 0.99, loss 0.03 -- iter 15250/55000, training for: 246.68s\n",
            "Epoch 7, step (batch no.): 1387 -- acc: 0.99, loss 0.03 -- iter 16750/55000, training for: 247.70s\n",
            "Epoch 7, step (batch no.): 1393 -- acc: 0.99, loss 0.03 -- iter 18250/55000, training for: 248.70s\n",
            "Epoch 7, step (batch no.): 1399 -- acc: 0.99, loss 0.03 -- iter 19750/55000, training for: 249.72s\n",
            "Epoch 7, step (batch no.): 1405 -- acc: 0.99, loss 0.04 -- iter 21250/55000, training for: 250.75s\n",
            "Epoch 7, step (batch no.): 1411 -- acc: 0.99, loss 0.04 -- iter 22750/55000, training for: 251.77s\n",
            "Epoch 7, step (batch no.): 1417 -- acc: 0.98, loss 0.05 -- iter 24250/55000, training for: 252.77s\n",
            "Epoch 7, step (batch no.): 1423 -- acc: 0.99, loss 0.04 -- iter 25750/55000, training for: 253.78s\n",
            "Epoch 7, step (batch no.): 1429 -- acc: 0.99, loss 0.04 -- iter 27250/55000, training for: 254.79s\n",
            "Epoch 7, step (batch no.): 1435 -- acc: 0.99, loss 0.04 -- iter 28750/55000, training for: 255.79s\n",
            "Epoch 7, step (batch no.): 1441 -- acc: 0.99, loss 0.03 -- iter 30250/55000, training for: 256.81s\n",
            "Epoch 7, step (batch no.): 1447 -- acc: 0.99, loss 0.03 -- iter 31750/55000, training for: 257.81s\n",
            "Epoch 7, step (batch no.): 1454 -- acc: 0.99, loss 0.03 -- iter 33500/55000, training for: 258.97s\n",
            "Epoch 7, step (batch no.): 1460 -- acc: 0.99, loss 0.03 -- iter 35000/55000, training for: 259.98s\n",
            "Epoch 7, step (batch no.): 1466 -- acc: 0.99, loss 0.04 -- iter 36500/55000, training for: 261.00s\n",
            "Epoch 7, step (batch no.): 1472 -- acc: 0.99, loss 0.04 -- iter 38000/55000, training for: 262.01s\n",
            "Epoch 7, step (batch no.): 1478 -- acc: 0.99, loss 0.05 -- iter 39500/55000, training for: 263.05s\n",
            "Epoch 7, step (batch no.): 1484 -- acc: 0.99, loss 0.04 -- iter 41000/55000, training for: 264.08s\n",
            "Epoch 7, step (batch no.): 1490 -- acc: 0.99, loss 0.03 -- iter 42500/55000, training for: 265.10s\n",
            "Epoch 7, step (batch no.): 1496 -- acc: 0.99, loss 0.03 -- iter 44000/55000, training for: 266.12s\n",
            "Epoch 7, step (batch no.): 1502 -- acc: 0.99, loss 0.03 -- iter 45500/55000, training for: 267.22s\n",
            "Epoch 7, step (batch no.): 1508 -- acc: 0.99, loss 0.04 -- iter 47000/55000, training for: 268.26s\n",
            "Epoch 7, step (batch no.): 1514 -- acc: 0.99, loss 0.04 -- iter 48500/55000, training for: 269.27s\n",
            "Epoch 7, step (batch no.): 1520 -- acc: 0.93, loss 0.97 -- iter 50000/55000, training for: 270.29s\n",
            "Epoch 7, step (batch no.): 1526 -- acc: 0.95, loss 0.58 -- iter 51500/55000, training for: 271.30s\n",
            "Epoch 7, step (batch no.): 1532 -- acc: 0.97, loss 0.33 -- iter 53000/55000, training for: 272.32s\n",
            "Epoch 7, step (batch no.): 1538 -- acc: 0.97, loss 0.20 -- iter 54500/55000, training for: 273.33s\n",
            "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.17563\u001b[0m\u001b[0m | time: 39.303s\n",
            "| Adam | epoch: 007 | loss: 0.17563 - acc: 0.9765 | val_loss: 0.07306 - val_acc: 0.9842 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 7, step (batch no.): 1540 -- acc: 0.98, loss 0.18 -- iter 55000/55000, training for: 275.72s\n",
            "Epoch 8, step (batch no.): 1546 -- acc: 0.98, loss 0.12 -- iter 01500/55000, training for: 276.75s\n",
            "Epoch 8, step (batch no.): 1552 -- acc: 0.98, loss 0.10 -- iter 03000/55000, training for: 277.78s\n",
            "Epoch 8, step (batch no.): 1558 -- acc: 0.98, loss 0.09 -- iter 04500/55000, training for: 278.78s\n",
            "Epoch 8, step (batch no.): 1564 -- acc: 0.98, loss 0.07 -- iter 06000/55000, training for: 279.79s\n",
            "Epoch 8, step (batch no.): 1570 -- acc: 0.98, loss 0.06 -- iter 07500/55000, training for: 280.80s\n",
            "Epoch 8, step (batch no.): 1576 -- acc: 0.98, loss 0.06 -- iter 09000/55000, training for: 281.80s\n",
            "Epoch 8, step (batch no.): 1583 -- acc: 0.99, loss 0.04 -- iter 10750/55000, training for: 282.96s\n",
            "Epoch 8, step (batch no.): 1589 -- acc: 0.99, loss 0.04 -- iter 12250/55000, training for: 283.97s\n",
            "Epoch 8, step (batch no.): 1595 -- acc: 0.99, loss 0.04 -- iter 13750/55000, training for: 284.97s\n",
            "Epoch 8, step (batch no.): 1601 -- acc: 0.99, loss 0.04 -- iter 15250/55000, training for: 285.98s\n",
            "Epoch 8, step (batch no.): 1607 -- acc: 0.99, loss 0.04 -- iter 16750/55000, training for: 287.00s\n",
            "Epoch 8, step (batch no.): 1613 -- acc: 0.99, loss 0.03 -- iter 18250/55000, training for: 288.00s\n",
            "Epoch 8, step (batch no.): 1619 -- acc: 0.99, loss 0.04 -- iter 19750/55000, training for: 289.01s\n",
            "Epoch 8, step (batch no.): 1625 -- acc: 0.99, loss 0.04 -- iter 21250/55000, training for: 290.02s\n",
            "Epoch 8, step (batch no.): 1631 -- acc: 0.99, loss 0.05 -- iter 22750/55000, training for: 291.03s\n",
            "Epoch 8, step (batch no.): 1637 -- acc: 0.98, loss 0.06 -- iter 24250/55000, training for: 292.08s\n",
            "Epoch 8, step (batch no.): 1643 -- acc: 0.99, loss 0.05 -- iter 25750/55000, training for: 293.15s\n",
            "Epoch 8, step (batch no.): 1649 -- acc: 0.99, loss 0.04 -- iter 27250/55000, training for: 294.18s\n",
            "Epoch 8, step (batch no.): 1655 -- acc: 0.99, loss 0.04 -- iter 28750/55000, training for: 295.20s\n",
            "Epoch 8, step (batch no.): 1661 -- acc: 0.99, loss 0.04 -- iter 30250/55000, training for: 296.22s\n",
            "Epoch 8, step (batch no.): 1667 -- acc: 0.99, loss 0.03 -- iter 31750/55000, training for: 297.26s\n",
            "Epoch 8, step (batch no.): 1673 -- acc: 0.99, loss 0.04 -- iter 33250/55000, training for: 298.30s\n",
            "Epoch 8, step (batch no.): 1679 -- acc: 0.99, loss 0.04 -- iter 34750/55000, training for: 299.32s\n",
            "Epoch 8, step (batch no.): 1685 -- acc: 0.99, loss 0.04 -- iter 36250/55000, training for: 300.36s\n",
            "Epoch 8, step (batch no.): 1691 -- acc: 0.99, loss 0.05 -- iter 37750/55000, training for: 301.44s\n",
            "Epoch 8, step (batch no.): 1697 -- acc: 0.99, loss 0.05 -- iter 39250/55000, training for: 302.44s\n",
            "Epoch 8, step (batch no.): 1703 -- acc: 0.98, loss 0.06 -- iter 40750/55000, training for: 303.44s\n",
            "Epoch 8, step (batch no.): 1709 -- acc: 0.98, loss 0.05 -- iter 42250/55000, training for: 304.45s\n",
            "Epoch 8, step (batch no.): 1715 -- acc: 0.99, loss 0.04 -- iter 43750/55000, training for: 305.48s\n",
            "Epoch 8, step (batch no.): 1721 -- acc: 0.99, loss 0.04 -- iter 45250/55000, training for: 306.49s\n",
            "Epoch 8, step (batch no.): 1728 -- acc: 0.99, loss 0.04 -- iter 47000/55000, training for: 307.65s\n",
            "Epoch 8, step (batch no.): 1734 -- acc: 0.99, loss 0.04 -- iter 48500/55000, training for: 308.67s\n",
            "Epoch 8, step (batch no.): 1741 -- acc: 0.99, loss 0.04 -- iter 50250/55000, training for: 309.83s\n",
            "Epoch 8, step (batch no.): 1747 -- acc: 0.99, loss 0.04 -- iter 51750/55000, training for: 310.84s\n",
            "Epoch 8, step (batch no.): 1753 -- acc: 0.99, loss 0.03 -- iter 53250/55000, training for: 311.87s\n",
            "Epoch 8, step (batch no.): 1759 -- acc: 0.99, loss 0.03 -- iter 54750/55000, training for: 312.91s\n",
            "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.03046\u001b[0m\u001b[0m | time: 39.385s\n",
            "| Adam | epoch: 008 | loss: 0.03046 - acc: 0.9897 | val_loss: 0.04063 - val_acc: 0.9881 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 8, step (batch no.): 1760 -- acc: 0.99, loss 0.03 -- iter 55000/55000, training for: 315.12s\n",
            "Epoch 9, step (batch no.): 1766 -- acc: 0.99, loss 0.03 -- iter 01500/55000, training for: 316.13s\n",
            "Epoch 9, step (batch no.): 1772 -- acc: 0.99, loss 0.03 -- iter 03000/55000, training for: 317.13s\n",
            "Epoch 9, step (batch no.): 1779 -- acc: 0.99, loss 0.02 -- iter 04750/55000, training for: 318.31s\n",
            "Epoch 9, step (batch no.): 1786 -- acc: 0.99, loss 0.02 -- iter 06500/55000, training for: 319.48s\n",
            "Epoch 9, step (batch no.): 1793 -- acc: 0.99, loss 0.03 -- iter 08250/55000, training for: 320.65s\n",
            "Epoch 9, step (batch no.): 1799 -- acc: 0.99, loss 0.03 -- iter 09750/55000, training for: 321.65s\n",
            "Epoch 9, step (batch no.): 1805 -- acc: 0.99, loss 0.03 -- iter 11250/55000, training for: 322.67s\n",
            "Epoch 9, step (batch no.): 1811 -- acc: 0.99, loss 0.03 -- iter 12750/55000, training for: 323.70s\n",
            "Epoch 9, step (batch no.): 1817 -- acc: 0.99, loss 0.04 -- iter 14250/55000, training for: 324.71s\n",
            "Epoch 9, step (batch no.): 1823 -- acc: 0.99, loss 0.03 -- iter 15750/55000, training for: 325.74s\n",
            "Epoch 9, step (batch no.): 1829 -- acc: 0.99, loss 0.03 -- iter 17250/55000, training for: 326.79s\n",
            "Epoch 9, step (batch no.): 1835 -- acc: 0.99, loss 0.03 -- iter 18750/55000, training for: 327.81s\n",
            "Epoch 9, step (batch no.): 1841 -- acc: 0.99, loss 0.04 -- iter 20250/55000, training for: 328.84s\n",
            "Epoch 9, step (batch no.): 1847 -- acc: 0.99, loss 0.04 -- iter 21750/55000, training for: 329.87s\n",
            "Epoch 9, step (batch no.): 1853 -- acc: 0.99, loss 0.04 -- iter 23250/55000, training for: 330.89s\n",
            "Epoch 9, step (batch no.): 1859 -- acc: 0.99, loss 0.03 -- iter 24750/55000, training for: 331.90s\n",
            "Epoch 9, step (batch no.): 1865 -- acc: 0.99, loss 0.03 -- iter 26250/55000, training for: 332.94s\n",
            "Epoch 9, step (batch no.): 1871 -- acc: 0.99, loss 0.03 -- iter 27750/55000, training for: 333.96s\n",
            "Epoch 9, step (batch no.): 1877 -- acc: 0.99, loss 0.03 -- iter 29250/55000, training for: 334.98s\n",
            "Epoch 9, step (batch no.): 1883 -- acc: 0.99, loss 0.03 -- iter 30750/55000, training for: 336.00s\n",
            "Epoch 9, step (batch no.): 1889 -- acc: 0.99, loss 0.03 -- iter 32250/55000, training for: 337.01s\n",
            "Epoch 9, step (batch no.): 1896 -- acc: 0.99, loss 0.04 -- iter 34000/55000, training for: 338.16s\n",
            "Epoch 9, step (batch no.): 1902 -- acc: 0.99, loss 0.04 -- iter 35500/55000, training for: 339.18s\n",
            "Epoch 9, step (batch no.): 1909 -- acc: 0.99, loss 0.04 -- iter 37250/55000, training for: 340.34s\n",
            "Epoch 9, step (batch no.): 1915 -- acc: 0.99, loss 0.03 -- iter 38750/55000, training for: 341.35s\n",
            "Epoch 9, step (batch no.): 1921 -- acc: 0.99, loss 0.05 -- iter 40250/55000, training for: 342.36s\n",
            "Epoch 9, step (batch no.): 1928 -- acc: 0.98, loss 0.05 -- iter 42000/55000, training for: 343.53s\n",
            "Epoch 9, step (batch no.): 1935 -- acc: 0.99, loss 0.05 -- iter 43750/55000, training for: 344.69s\n",
            "Epoch 9, step (batch no.): 1941 -- acc: 0.99, loss 0.04 -- iter 45250/55000, training for: 345.70s\n",
            "Epoch 9, step (batch no.): 1948 -- acc: 0.99, loss 0.04 -- iter 47000/55000, training for: 346.86s\n",
            "Epoch 9, step (batch no.): 1955 -- acc: 0.99, loss 0.04 -- iter 48750/55000, training for: 348.02s\n",
            "Epoch 9, step (batch no.): 1962 -- acc: 0.93, loss 0.93 -- iter 50500/55000, training for: 349.18s\n",
            "Epoch 9, step (batch no.): 1968 -- acc: 0.95, loss 0.54 -- iter 52000/55000, training for: 350.18s\n",
            "Epoch 9, step (batch no.): 1974 -- acc: 0.97, loss 0.32 -- iter 53500/55000, training for: 351.19s\n",
            "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.19226\u001b[0m\u001b[0m | time: 39.093s\n",
            "| Adam | epoch: 009 | loss: 0.19226 - acc: 0.9767 | val_loss: 0.06747 - val_acc: 0.9817 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 9, step (batch no.): 1980 -- acc: 0.98, loss 0.19 -- iter 55000/55000, training for: 354.23s\n",
            "Epoch 10, step (batch no.): 1986 -- acc: 0.98, loss 0.12 -- iter 01500/55000, training for: 355.23s\n",
            "Epoch 10, step (batch no.): 1993 -- acc: 0.99, loss 0.08 -- iter 03250/55000, training for: 356.39s\n",
            "Epoch 10, step (batch no.): 2000 -- acc: 0.99, loss 0.06 -- iter 05000/55000, training for: 357.55s\n",
            "Epoch 10, step (batch no.): 2007 -- acc: 0.99, loss 0.06 -- iter 06750/55000, training for: 358.72s\n",
            "Epoch 10, step (batch no.): 2013 -- acc: 0.99, loss 0.05 -- iter 08250/55000, training for: 359.77s\n",
            "Epoch 10, step (batch no.): 2019 -- acc: 0.99, loss 0.05 -- iter 09750/55000, training for: 360.78s\n",
            "Epoch 10, step (batch no.): 2025 -- acc: 0.99, loss 0.05 -- iter 11250/55000, training for: 361.81s\n",
            "Epoch 10, step (batch no.): 2031 -- acc: 0.99, loss 0.05 -- iter 12750/55000, training for: 362.85s\n",
            "Epoch 10, step (batch no.): 2037 -- acc: 0.99, loss 0.04 -- iter 14250/55000, training for: 363.86s\n",
            "Epoch 10, step (batch no.): 2043 -- acc: 0.99, loss 0.04 -- iter 15750/55000, training for: 364.88s\n",
            "Epoch 10, step (batch no.): 2049 -- acc: 0.99, loss 0.04 -- iter 17250/55000, training for: 365.90s\n",
            "Epoch 10, step (batch no.): 2055 -- acc: 0.99, loss 0.03 -- iter 18750/55000, training for: 367.00s\n",
            "Epoch 10, step (batch no.): 2061 -- acc: 0.99, loss 0.04 -- iter 20250/55000, training for: 368.03s\n",
            "Epoch 10, step (batch no.): 2067 -- acc: 0.99, loss 0.04 -- iter 21750/55000, training for: 369.04s\n",
            "Epoch 10, step (batch no.): 2073 -- acc: 0.99, loss 0.04 -- iter 23250/55000, training for: 370.06s\n",
            "Epoch 10, step (batch no.): 2080 -- acc: 0.99, loss 0.04 -- iter 25000/55000, training for: 371.24s\n",
            "Epoch 10, step (batch no.): 2086 -- acc: 0.99, loss 0.04 -- iter 26500/55000, training for: 372.26s\n",
            "Epoch 10, step (batch no.): 2092 -- acc: 0.99, loss 0.03 -- iter 28000/55000, training for: 373.26s\n",
            "Epoch 10, step (batch no.): 2099 -- acc: 0.99, loss 0.03 -- iter 29750/55000, training for: 374.43s\n",
            "Epoch 10, step (batch no.): 2105 -- acc: 0.99, loss 0.03 -- iter 31250/55000, training for: 375.44s\n",
            "Epoch 10, step (batch no.): 2112 -- acc: 0.99, loss 0.03 -- iter 33000/55000, training for: 376.61s\n",
            "Epoch 10, step (batch no.): 2118 -- acc: 0.99, loss 0.03 -- iter 34500/55000, training for: 377.62s\n",
            "Epoch 10, step (batch no.): 2125 -- acc: 0.99, loss 0.04 -- iter 36250/55000, training for: 378.78s\n",
            "Epoch 10, step (batch no.): 2132 -- acc: 0.99, loss 0.04 -- iter 38000/55000, training for: 379.94s\n",
            "Epoch 10, step (batch no.): 2138 -- acc: 0.99, loss 0.04 -- iter 39500/55000, training for: 380.96s\n",
            "Epoch 10, step (batch no.): 2144 -- acc: 0.99, loss 0.03 -- iter 41000/55000, training for: 381.96s\n",
            "Epoch 10, step (batch no.): 2150 -- acc: 0.99, loss 0.03 -- iter 42500/55000, training for: 383.00s\n",
            "Epoch 10, step (batch no.): 2156 -- acc: 0.99, loss 0.03 -- iter 44000/55000, training for: 384.03s\n",
            "Epoch 10, step (batch no.): 2162 -- acc: 0.99, loss 0.03 -- iter 45500/55000, training for: 385.04s\n",
            "Epoch 10, step (batch no.): 2169 -- acc: 0.99, loss 0.03 -- iter 47250/55000, training for: 386.19s\n",
            "Epoch 10, step (batch no.): 2176 -- acc: 0.99, loss 0.03 -- iter 49000/55000, training for: 387.35s\n",
            "Epoch 10, step (batch no.): 2183 -- acc: 0.93, loss 1.04 -- iter 50750/55000, training for: 388.51s\n",
            "Epoch 10, step (batch no.): 2189 -- acc: 0.96, loss 0.63 -- iter 52250/55000, training for: 389.52s\n",
            "Epoch 10, step (batch no.): 2195 -- acc: 0.97, loss 0.36 -- iter 53750/55000, training for: 390.53s\n",
            "Training Step: 2200  | total loss: \u001b[1m\u001b[32m0.22732\u001b[0m\u001b[0m | time: 39.269s\n",
            "| Adam | epoch: 010 | loss: 0.22732 - acc: 0.9780 | val_loss: 0.05753 - val_acc: 0.9838 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 10, step (batch no.): 2200 -- acc: 0.98, loss 0.23 -- iter 55000/55000, training for: 393.51s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: HINMH3\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "---------------------------------\n",
            "Training samples: 55000\n",
            "Validation samples: 10000\n",
            "--\n",
            "Epoch 1, step (batch no.): 1 -- acc: 0.00, loss 0.00 -- iter 00250/55000, training for: 0.31s\n",
            "Epoch 1, step (batch no.): 7 -- acc: 0.10, loss 2.43 -- iter 01750/55000, training for: 1.36s\n",
            "Epoch 1, step (batch no.): 13 -- acc: 0.10, loss 2.32 -- iter 03250/55000, training for: 2.42s\n",
            "Epoch 1, step (batch no.): 19 -- acc: 0.10, loss 2.30 -- iter 04750/55000, training for: 3.47s\n",
            "Epoch 1, step (batch no.): 25 -- acc: 0.11, loss 2.30 -- iter 06250/55000, training for: 4.50s\n",
            "Epoch 1, step (batch no.): 31 -- acc: 0.10, loss 2.30 -- iter 07750/55000, training for: 5.56s\n",
            "Epoch 1, step (batch no.): 37 -- acc: 0.10, loss 2.30 -- iter 09250/55000, training for: 6.63s\n",
            "Epoch 1, step (batch no.): 43 -- acc: 0.12, loss 2.30 -- iter 10750/55000, training for: 7.66s\n",
            "Epoch 1, step (batch no.): 49 -- acc: 0.11, loss 2.30 -- iter 12250/55000, training for: 8.69s\n",
            "Epoch 1, step (batch no.): 55 -- acc: 0.11, loss 2.30 -- iter 13750/55000, training for: 9.73s\n",
            "Epoch 1, step (batch no.): 61 -- acc: 0.12, loss 2.30 -- iter 15250/55000, training for: 10.79s\n",
            "Epoch 1, step (batch no.): 67 -- acc: 0.12, loss 2.30 -- iter 16750/55000, training for: 11.83s\n",
            "Epoch 1, step (batch no.): 73 -- acc: 0.12, loss 2.30 -- iter 18250/55000, training for: 12.87s\n",
            "Epoch 1, step (batch no.): 79 -- acc: 0.12, loss 2.30 -- iter 19750/55000, training for: 13.91s\n",
            "Epoch 1, step (batch no.): 85 -- acc: 0.12, loss 2.30 -- iter 21250/55000, training for: 14.95s\n",
            "Epoch 1, step (batch no.): 91 -- acc: 0.12, loss 2.30 -- iter 22750/55000, training for: 15.98s\n",
            "Epoch 1, step (batch no.): 97 -- acc: 0.11, loss 2.30 -- iter 24250/55000, training for: 17.04s\n",
            "Epoch 1, step (batch no.): 103 -- acc: 0.11, loss 2.30 -- iter 25750/55000, training for: 18.09s\n",
            "Epoch 1, step (batch no.): 109 -- acc: 0.12, loss 2.30 -- iter 27250/55000, training for: 19.18s\n",
            "Epoch 1, step (batch no.): 115 -- acc: 0.12, loss 2.30 -- iter 28750/55000, training for: 20.21s\n",
            "Epoch 1, step (batch no.): 121 -- acc: 0.11, loss 2.30 -- iter 30250/55000, training for: 21.24s\n",
            "Epoch 1, step (batch no.): 127 -- acc: 0.11, loss 2.30 -- iter 31750/55000, training for: 22.25s\n",
            "Epoch 1, step (batch no.): 133 -- acc: 0.11, loss 2.30 -- iter 33250/55000, training for: 23.27s\n",
            "Epoch 1, step (batch no.): 139 -- acc: 0.12, loss 2.30 -- iter 34750/55000, training for: 24.30s\n",
            "Epoch 1, step (batch no.): 145 -- acc: 0.11, loss 2.30 -- iter 36250/55000, training for: 25.31s\n",
            "Epoch 1, step (batch no.): 151 -- acc: 0.11, loss 2.30 -- iter 37750/55000, training for: 26.32s\n",
            "Epoch 1, step (batch no.): 157 -- acc: 0.11, loss 2.30 -- iter 39250/55000, training for: 27.37s\n",
            "Epoch 1, step (batch no.): 163 -- acc: 0.11, loss 2.30 -- iter 40750/55000, training for: 28.39s\n",
            "Epoch 1, step (batch no.): 169 -- acc: 0.11, loss 2.30 -- iter 42250/55000, training for: 29.41s\n",
            "Epoch 1, step (batch no.): 175 -- acc: 0.11, loss 2.30 -- iter 43750/55000, training for: 30.42s\n",
            "Epoch 1, step (batch no.): 181 -- acc: 0.11, loss 2.30 -- iter 45250/55000, training for: 31.47s\n",
            "Epoch 1, step (batch no.): 187 -- acc: 0.11, loss 2.30 -- iter 46750/55000, training for: 32.51s\n",
            "Epoch 1, step (batch no.): 193 -- acc: 0.11, loss 2.30 -- iter 48250/55000, training for: 33.54s\n",
            "Epoch 1, step (batch no.): 199 -- acc: 0.12, loss 2.30 -- iter 49750/55000, training for: 34.56s\n",
            "Epoch 1, step (batch no.): 205 -- acc: 0.11, loss 2.30 -- iter 51250/55000, training for: 35.58s\n",
            "Epoch 1, step (batch no.): 211 -- acc: 0.11, loss 2.30 -- iter 52750/55000, training for: 36.60s\n",
            "Epoch 1, step (batch no.): 217 -- acc: 0.12, loss 2.30 -- iter 54250/55000, training for: 37.71s\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m2.30187\u001b[0m\u001b[0m | time: 40.323s\n",
            "| Adam | epoch: 001 | loss: 2.30187 - acc: 0.1106 | val_loss: 2.30158 - val_acc: 0.1135 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 1, step (batch no.): 220 -- acc: 0.11, loss 2.30 -- iter 55000/55000, training for: 40.34s\n",
            "Epoch 2, step (batch no.): 226 -- acc: 0.12, loss 2.30 -- iter 01500/55000, training for: 41.38s\n",
            "Epoch 2, step (batch no.): 232 -- acc: 0.11, loss 2.30 -- iter 03000/55000, training for: 42.39s\n",
            "Epoch 2, step (batch no.): 238 -- acc: 0.12, loss 2.30 -- iter 04500/55000, training for: 43.42s\n",
            "Epoch 2, step (batch no.): 244 -- acc: 0.11, loss 2.30 -- iter 06000/55000, training for: 44.45s\n",
            "Epoch 2, step (batch no.): 250 -- acc: 0.11, loss 2.30 -- iter 07500/55000, training for: 45.45s\n",
            "Epoch 2, step (batch no.): 256 -- acc: 0.11, loss 2.30 -- iter 09000/55000, training for: 46.46s\n",
            "Epoch 2, step (batch no.): 262 -- acc: 0.11, loss 2.30 -- iter 10500/55000, training for: 47.48s\n",
            "Epoch 2, step (batch no.): 268 -- acc: 0.11, loss 2.30 -- iter 12000/55000, training for: 48.52s\n",
            "Epoch 2, step (batch no.): 274 -- acc: 0.11, loss 2.30 -- iter 13500/55000, training for: 49.57s\n",
            "Epoch 2, step (batch no.): 280 -- acc: 0.11, loss 2.30 -- iter 15000/55000, training for: 50.60s\n",
            "Epoch 2, step (batch no.): 286 -- acc: 0.12, loss 2.30 -- iter 16500/55000, training for: 51.61s\n",
            "Epoch 2, step (batch no.): 292 -- acc: 0.12, loss 2.30 -- iter 18000/55000, training for: 52.61s\n",
            "Epoch 2, step (batch no.): 298 -- acc: 0.11, loss 2.30 -- iter 19500/55000, training for: 53.62s\n",
            "Epoch 2, step (batch no.): 304 -- acc: 0.11, loss 2.30 -- iter 21000/55000, training for: 54.63s\n",
            "Epoch 2, step (batch no.): 310 -- acc: 0.12, loss 2.30 -- iter 22500/55000, training for: 55.63s\n",
            "Epoch 2, step (batch no.): 316 -- acc: 0.11, loss 2.30 -- iter 24000/55000, training for: 56.64s\n",
            "Epoch 2, step (batch no.): 322 -- acc: 0.11, loss 2.30 -- iter 25500/55000, training for: 57.66s\n",
            "Epoch 2, step (batch no.): 328 -- acc: 0.12, loss 2.30 -- iter 27000/55000, training for: 58.70s\n",
            "Epoch 2, step (batch no.): 334 -- acc: 0.12, loss 2.30 -- iter 28500/55000, training for: 59.72s\n",
            "Epoch 2, step (batch no.): 340 -- acc: 0.12, loss 2.30 -- iter 30000/55000, training for: 60.73s\n",
            "Epoch 2, step (batch no.): 347 -- acc: 0.11, loss 2.30 -- iter 31750/55000, training for: 61.88s\n",
            "Epoch 2, step (batch no.): 353 -- acc: 0.11, loss 2.30 -- iter 33250/55000, training for: 62.90s\n",
            "Epoch 2, step (batch no.): 360 -- acc: 0.11, loss 2.30 -- iter 35000/55000, training for: 64.07s\n",
            "Epoch 2, step (batch no.): 366 -- acc: 0.11, loss 2.30 -- iter 36500/55000, training for: 65.10s\n",
            "Epoch 2, step (batch no.): 373 -- acc: 0.11, loss 2.30 -- iter 38250/55000, training for: 66.26s\n",
            "Epoch 2, step (batch no.): 380 -- acc: 0.10, loss 2.30 -- iter 40000/55000, training for: 67.43s\n",
            "Epoch 2, step (batch no.): 387 -- acc: 0.11, loss 2.30 -- iter 41750/55000, training for: 68.58s\n",
            "Epoch 2, step (batch no.): 394 -- acc: 0.10, loss 2.30 -- iter 43500/55000, training for: 69.74s\n",
            "Epoch 2, step (batch no.): 400 -- acc: 0.11, loss 2.30 -- iter 45000/55000, training for: 70.80s\n",
            "Epoch 2, step (batch no.): 407 -- acc: 0.11, loss 2.30 -- iter 46750/55000, training for: 71.96s\n",
            "Epoch 2, step (batch no.): 414 -- acc: 0.11, loss 2.30 -- iter 48500/55000, training for: 73.11s\n",
            "Epoch 2, step (batch no.): 421 -- acc: 0.12, loss 2.30 -- iter 50250/55000, training for: 74.25s\n",
            "Epoch 2, step (batch no.): 428 -- acc: 0.12, loss 2.30 -- iter 52000/55000, training for: 75.40s\n",
            "Epoch 2, step (batch no.): 435 -- acc: 0.11, loss 2.30 -- iter 53750/55000, training for: 76.56s\n",
            "Training Step: 440  | total loss: \u001b[1m\u001b[32m2.29950\u001b[0m\u001b[0m | time: 39.150s\n",
            "| Adam | epoch: 002 | loss: 2.29950 - acc: 0.1205 | val_loss: 2.30165 - val_acc: 0.1135 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 2, step (batch no.): 440 -- acc: 0.12, loss 2.30 -- iter 55000/55000, training for: 79.51s\n",
            "Epoch 3, step (batch no.): 446 -- acc: 0.11, loss 2.30 -- iter 01500/55000, training for: 80.55s\n",
            "Epoch 3, step (batch no.): 453 -- acc: 0.10, loss 2.30 -- iter 03250/55000, training for: 81.72s\n",
            "Epoch 3, step (batch no.): 459 -- acc: 0.11, loss 2.30 -- iter 04750/55000, training for: 82.72s\n",
            "Epoch 3, step (batch no.): 465 -- acc: 0.11, loss 2.30 -- iter 06250/55000, training for: 83.73s\n",
            "Epoch 3, step (batch no.): 472 -- acc: 0.11, loss 2.30 -- iter 08000/55000, training for: 84.88s\n",
            "Epoch 3, step (batch no.): 479 -- acc: 0.11, loss 2.30 -- iter 09750/55000, training for: 86.03s\n",
            "Epoch 3, step (batch no.): 486 -- acc: 0.11, loss 2.30 -- iter 11500/55000, training for: 87.19s\n",
            "Epoch 3, step (batch no.): 492 -- acc: 0.11, loss 2.30 -- iter 13000/55000, training for: 88.19s\n",
            "Epoch 3, step (batch no.): 498 -- acc: 0.11, loss 2.30 -- iter 14500/55000, training for: 89.20s\n",
            "Epoch 3, step (batch no.): 505 -- acc: 0.11, loss 2.30 -- iter 16250/55000, training for: 90.36s\n",
            "Epoch 3, step (batch no.): 511 -- acc: 0.12, loss 2.30 -- iter 17750/55000, training for: 91.37s\n",
            "Epoch 3, step (batch no.): 517 -- acc: 0.11, loss 2.30 -- iter 19250/55000, training for: 92.37s\n",
            "Epoch 3, step (batch no.): 523 -- acc: 0.11, loss 2.30 -- iter 20750/55000, training for: 93.38s\n",
            "Epoch 3, step (batch no.): 529 -- acc: 0.11, loss 2.30 -- iter 22250/55000, training for: 94.39s\n",
            "Epoch 3, step (batch no.): 535 -- acc: 0.12, loss 2.30 -- iter 23750/55000, training for: 95.40s\n",
            "Epoch 3, step (batch no.): 541 -- acc: 0.11, loss 2.30 -- iter 25250/55000, training for: 96.40s\n",
            "Epoch 3, step (batch no.): 547 -- acc: 0.11, loss 2.30 -- iter 26750/55000, training for: 97.42s\n",
            "Epoch 3, step (batch no.): 553 -- acc: 0.11, loss 2.30 -- iter 28250/55000, training for: 98.46s\n",
            "Epoch 3, step (batch no.): 559 -- acc: 0.11, loss 2.30 -- iter 29750/55000, training for: 99.48s\n",
            "Epoch 3, step (batch no.): 565 -- acc: 0.12, loss 2.30 -- iter 31250/55000, training for: 100.51s\n",
            "Epoch 3, step (batch no.): 571 -- acc: 0.11, loss 2.30 -- iter 32750/55000, training for: 101.53s\n",
            "Epoch 3, step (batch no.): 578 -- acc: 0.11, loss 2.30 -- iter 34500/55000, training for: 102.68s\n",
            "Epoch 3, step (batch no.): 584 -- acc: 0.11, loss 2.30 -- iter 36000/55000, training for: 103.77s\n",
            "Epoch 3, step (batch no.): 590 -- acc: 0.12, loss 2.30 -- iter 37500/55000, training for: 104.77s\n",
            "Epoch 3, step (batch no.): 596 -- acc: 0.12, loss 2.30 -- iter 39000/55000, training for: 105.78s\n",
            "Epoch 3, step (batch no.): 602 -- acc: 0.11, loss 2.30 -- iter 40500/55000, training for: 106.79s\n",
            "Epoch 3, step (batch no.): 608 -- acc: 0.11, loss 2.30 -- iter 42000/55000, training for: 107.79s\n",
            "Epoch 3, step (batch no.): 614 -- acc: 0.11, loss 2.30 -- iter 43500/55000, training for: 108.82s\n",
            "Epoch 3, step (batch no.): 620 -- acc: 0.11, loss 2.30 -- iter 45000/55000, training for: 109.84s\n",
            "Epoch 3, step (batch no.): 626 -- acc: 0.11, loss 2.30 -- iter 46500/55000, training for: 110.86s\n",
            "Epoch 3, step (batch no.): 633 -- acc: 0.12, loss 2.30 -- iter 48250/55000, training for: 112.01s\n",
            "Epoch 3, step (batch no.): 640 -- acc: 0.12, loss 2.30 -- iter 50000/55000, training for: 113.18s\n",
            "Epoch 3, step (batch no.): 647 -- acc: 0.11, loss 2.30 -- iter 51750/55000, training for: 114.35s\n",
            "Epoch 3, step (batch no.): 654 -- acc: 0.11, loss 2.30 -- iter 53500/55000, training for: 115.51s\n",
            "Training Step: 660  | total loss: \u001b[1m\u001b[32m2.30249\u001b[0m\u001b[0m | time: 39.073s\n",
            "| Adam | epoch: 003 | loss: 2.30249 - acc: 0.1084 | val_loss: 2.30133 - val_acc: 0.1135 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 3, step (batch no.): 660 -- acc: 0.11, loss 2.30 -- iter 55000/55000, training for: 118.60s\n",
            "Epoch 4, step (batch no.): 666 -- acc: 0.11, loss 2.30 -- iter 01500/55000, training for: 119.60s\n",
            "Epoch 4, step (batch no.): 673 -- acc: 0.12, loss 2.30 -- iter 03250/55000, training for: 120.76s\n",
            "Epoch 4, step (batch no.): 679 -- acc: 0.12, loss 2.30 -- iter 04750/55000, training for: 121.76s\n",
            "Epoch 4, step (batch no.): 685 -- acc: 0.11, loss 2.30 -- iter 06250/55000, training for: 122.76s\n",
            "Epoch 4, step (batch no.): 692 -- acc: 0.11, loss 2.30 -- iter 08000/55000, training for: 123.93s\n",
            "Epoch 4, step (batch no.): 699 -- acc: 0.11, loss 2.30 -- iter 09750/55000, training for: 125.08s\n",
            "Epoch 4, step (batch no.): 705 -- acc: 0.11, loss 2.30 -- iter 11250/55000, training for: 126.08s\n",
            "Epoch 4, step (batch no.): 712 -- acc: 0.11, loss 2.30 -- iter 13000/55000, training for: 127.24s\n",
            "Epoch 4, step (batch no.): 719 -- acc: 0.11, loss 2.30 -- iter 14750/55000, training for: 128.39s\n",
            "Epoch 4, step (batch no.): 726 -- acc: 0.11, loss 2.30 -- iter 16500/55000, training for: 129.55s\n",
            "Epoch 4, step (batch no.): 732 -- acc: 0.11, loss 2.30 -- iter 18000/55000, training for: 130.56s\n",
            "Epoch 4, step (batch no.): 738 -- acc: 0.11, loss 2.30 -- iter 19500/55000, training for: 131.57s\n",
            "Epoch 4, step (batch no.): 744 -- acc: 0.11, loss 2.30 -- iter 21000/55000, training for: 132.57s\n",
            "Epoch 4, step (batch no.): 750 -- acc: 0.11, loss 2.30 -- iter 22500/55000, training for: 133.58s\n",
            "Epoch 4, step (batch no.): 756 -- acc: 0.12, loss 2.30 -- iter 24000/55000, training for: 134.59s\n",
            "Epoch 4, step (batch no.): 762 -- acc: 0.12, loss 2.30 -- iter 25500/55000, training for: 135.59s\n",
            "Epoch 4, step (batch no.): 768 -- acc: 0.12, loss 2.30 -- iter 27000/55000, training for: 136.66s\n",
            "Epoch 4, step (batch no.): 774 -- acc: 0.12, loss 2.30 -- iter 28500/55000, training for: 137.69s\n",
            "Epoch 4, step (batch no.): 781 -- acc: 0.11, loss 2.30 -- iter 30250/55000, training for: 138.84s\n",
            "Epoch 4, step (batch no.): 787 -- acc: 0.11, loss 2.30 -- iter 31750/55000, training for: 139.85s\n",
            "Epoch 4, step (batch no.): 793 -- acc: 0.11, loss 2.30 -- iter 33250/55000, training for: 140.86s\n",
            "Epoch 4, step (batch no.): 799 -- acc: 0.11, loss 2.30 -- iter 34750/55000, training for: 141.86s\n",
            "Epoch 4, step (batch no.): 806 -- acc: 0.11, loss 2.30 -- iter 36500/55000, training for: 143.02s\n",
            "Epoch 4, step (batch no.): 812 -- acc: 0.10, loss 2.30 -- iter 38000/55000, training for: 144.03s\n",
            "Epoch 4, step (batch no.): 818 -- acc: 0.11, loss 2.30 -- iter 39500/55000, training for: 145.04s\n",
            "Epoch 4, step (batch no.): 825 -- acc: 0.10, loss 2.30 -- iter 41250/55000, training for: 146.19s\n",
            "Epoch 4, step (batch no.): 832 -- acc: 0.10, loss 2.30 -- iter 43000/55000, training for: 147.35s\n",
            "Epoch 4, step (batch no.): 839 -- acc: 0.11, loss 2.30 -- iter 44750/55000, training for: 148.51s\n",
            "Epoch 4, step (batch no.): 846 -- acc: 0.12, loss 2.30 -- iter 46500/55000, training for: 149.67s\n",
            "Epoch 4, step (batch no.): 853 -- acc: 0.12, loss 2.30 -- iter 48250/55000, training for: 150.83s\n",
            "Epoch 4, step (batch no.): 860 -- acc: 0.12, loss 2.30 -- iter 50000/55000, training for: 151.99s\n",
            "Epoch 4, step (batch no.): 867 -- acc: 0.12, loss 2.30 -- iter 51750/55000, training for: 153.15s\n",
            "Epoch 4, step (batch no.): 874 -- acc: 0.11, loss 2.30 -- iter 53500/55000, training for: 154.30s\n",
            "Training Step: 880  | total loss: \u001b[1m\u001b[32m2.29982\u001b[0m\u001b[0m | time: 38.736s\n",
            "| Adam | epoch: 004 | loss: 2.29982 - acc: 0.1176 | val_loss: 2.30132 - val_acc: 0.1135 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 4, step (batch no.): 880 -- acc: 0.12, loss 2.30 -- iter 55000/55000, training for: 157.35s\n",
            "Epoch 5, step (batch no.): 886 -- acc: 0.12, loss 2.30 -- iter 01500/55000, training for: 158.36s\n",
            "Epoch 5, step (batch no.): 892 -- acc: 0.12, loss 2.30 -- iter 03000/55000, training for: 159.36s\n",
            "Epoch 5, step (batch no.): 898 -- acc: 0.12, loss 2.30 -- iter 04500/55000, training for: 160.39s\n",
            "Epoch 5, step (batch no.): 904 -- acc: 0.13, loss 2.30 -- iter 06000/55000, training for: 161.42s\n",
            "Epoch 5, step (batch no.): 910 -- acc: 0.12, loss 2.30 -- iter 07500/55000, training for: 162.42s\n",
            "Epoch 5, step (batch no.): 916 -- acc: 0.12, loss 2.30 -- iter 09000/55000, training for: 163.45s\n",
            "Epoch 5, step (batch no.): 922 -- acc: 0.12, loss 2.30 -- iter 10500/55000, training for: 164.46s\n",
            "Epoch 5, step (batch no.): 929 -- acc: 0.11, loss 2.30 -- iter 12250/55000, training for: 165.61s\n",
            "Epoch 5, step (batch no.): 935 -- acc: 0.11, loss 2.30 -- iter 13750/55000, training for: 166.61s\n",
            "Epoch 5, step (batch no.): 942 -- acc: 0.11, loss 2.30 -- iter 15500/55000, training for: 167.77s\n",
            "Epoch 5, step (batch no.): 948 -- acc: 0.11, loss 2.30 -- iter 17000/55000, training for: 168.80s\n",
            "Epoch 5, step (batch no.): 954 -- acc: 0.11, loss 2.30 -- iter 18500/55000, training for: 169.91s\n",
            "Epoch 5, step (batch no.): 960 -- acc: 0.11, loss 2.30 -- iter 20000/55000, training for: 170.93s\n",
            "Epoch 5, step (batch no.): 966 -- acc: 0.11, loss 2.30 -- iter 21500/55000, training for: 171.96s\n",
            "Epoch 5, step (batch no.): 972 -- acc: 0.11, loss 2.30 -- iter 23000/55000, training for: 172.97s\n",
            "Epoch 5, step (batch no.): 978 -- acc: 0.12, loss 2.30 -- iter 24500/55000, training for: 173.99s\n",
            "Epoch 5, step (batch no.): 984 -- acc: 0.11, loss 2.30 -- iter 26000/55000, training for: 175.01s\n",
            "Epoch 5, step (batch no.): 990 -- acc: 0.11, loss 2.30 -- iter 27500/55000, training for: 176.03s\n",
            "Epoch 5, step (batch no.): 996 -- acc: 0.11, loss 2.30 -- iter 29000/55000, training for: 177.04s\n",
            "Epoch 5, step (batch no.): 1002 -- acc: 0.11, loss 2.30 -- iter 30500/55000, training for: 178.04s\n",
            "Epoch 5, step (batch no.): 1008 -- acc: 0.11, loss 2.30 -- iter 32000/55000, training for: 179.05s\n",
            "Epoch 5, step (batch no.): 1014 -- acc: 0.11, loss 2.30 -- iter 33500/55000, training for: 180.06s\n",
            "Epoch 5, step (batch no.): 1020 -- acc: 0.11, loss 2.30 -- iter 35000/55000, training for: 181.10s\n",
            "Epoch 5, step (batch no.): 1026 -- acc: 0.11, loss 2.30 -- iter 36500/55000, training for: 182.12s\n",
            "Epoch 5, step (batch no.): 1032 -- acc: 0.11, loss 2.30 -- iter 38000/55000, training for: 183.12s\n",
            "Epoch 5, step (batch no.): 1039 -- acc: 0.12, loss 2.30 -- iter 39750/55000, training for: 184.29s\n",
            "Epoch 5, step (batch no.): 1046 -- acc: 0.11, loss 2.30 -- iter 41500/55000, training for: 185.45s\n",
            "Epoch 5, step (batch no.): 1052 -- acc: 0.11, loss 2.30 -- iter 43000/55000, training for: 186.46s\n",
            "Epoch 5, step (batch no.): 1059 -- acc: 0.11, loss 2.30 -- iter 44750/55000, training for: 187.62s\n",
            "Epoch 5, step (batch no.): 1065 -- acc: 0.11, loss 2.30 -- iter 46250/55000, training for: 188.63s\n",
            "Epoch 5, step (batch no.): 1072 -- acc: 0.11, loss 2.30 -- iter 48000/55000, training for: 189.78s\n",
            "Epoch 5, step (batch no.): 1079 -- acc: 0.11, loss 2.30 -- iter 49750/55000, training for: 190.95s\n",
            "Epoch 5, step (batch no.): 1086 -- acc: 0.11, loss 2.30 -- iter 51500/55000, training for: 192.09s\n",
            "Epoch 5, step (batch no.): 1093 -- acc: 0.11, loss 2.30 -- iter 53250/55000, training for: 193.25s\n",
            "Epoch 5, step (batch no.): 1099 -- acc: 0.11, loss 2.30 -- iter 54750/55000, training for: 194.25s\n",
            "Training Step: 1100  | total loss: \u001b[1m\u001b[32m2.30128\u001b[0m\u001b[0m | time: 39.134s\n",
            "| Adam | epoch: 005 | loss: 2.30128 - acc: 0.1141 | val_loss: 2.30134 - val_acc: 0.1135 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 5, step (batch no.): 1100 -- acc: 0.11, loss 2.30 -- iter 55000/55000, training for: 196.50s\n",
            "Epoch 6, step (batch no.): 1106 -- acc: 0.11, loss 2.30 -- iter 01500/55000, training for: 197.52s\n",
            "Epoch 6, step (batch no.): 1112 -- acc: 0.11, loss 2.30 -- iter 03000/55000, training for: 198.52s\n",
            "Epoch 6, step (batch no.): 1118 -- acc: 0.11, loss 2.30 -- iter 04500/55000, training for: 199.54s\n",
            "Epoch 6, step (batch no.): 1124 -- acc: 0.11, loss 2.30 -- iter 06000/55000, training for: 200.54s\n",
            "Epoch 6, step (batch no.): 1131 -- acc: 0.12, loss 2.30 -- iter 07750/55000, training for: 201.70s\n",
            "Epoch 6, step (batch no.): 1137 -- acc: 0.11, loss 2.30 -- iter 09250/55000, training for: 202.74s\n",
            "Epoch 6, step (batch no.): 1143 -- acc: 0.11, loss 2.30 -- iter 10750/55000, training for: 203.77s\n",
            "Epoch 6, step (batch no.): 1149 -- acc: 0.11, loss 2.30 -- iter 12250/55000, training for: 204.79s\n",
            "Epoch 6, step (batch no.): 1155 -- acc: 0.12, loss 2.30 -- iter 13750/55000, training for: 205.79s\n",
            "Epoch 6, step (batch no.): 1161 -- acc: 0.11, loss 2.30 -- iter 15250/55000, training for: 206.81s\n",
            "Epoch 6, step (batch no.): 1167 -- acc: 0.11, loss 2.30 -- iter 16750/55000, training for: 207.83s\n",
            "Epoch 6, step (batch no.): 1173 -- acc: 0.10, loss 2.30 -- iter 18250/55000, training for: 208.84s\n",
            "Epoch 6, step (batch no.): 1179 -- acc: 0.11, loss 2.30 -- iter 19750/55000, training for: 209.84s\n",
            "Epoch 6, step (batch no.): 1185 -- acc: 0.11, loss 2.30 -- iter 21250/55000, training for: 210.94s\n",
            "Epoch 6, step (batch no.): 1191 -- acc: 0.11, loss 2.30 -- iter 22750/55000, training for: 211.98s\n",
            "Epoch 6, step (batch no.): 1197 -- acc: 0.11, loss 2.30 -- iter 24250/55000, training for: 212.99s\n",
            "Epoch 6, step (batch no.): 1204 -- acc: 0.11, loss 2.30 -- iter 26000/55000, training for: 214.15s\n",
            "Epoch 6, step (batch no.): 1210 -- acc: 0.10, loss 2.30 -- iter 27500/55000, training for: 215.15s\n",
            "Epoch 6, step (batch no.): 1217 -- acc: 0.11, loss 2.30 -- iter 29250/55000, training for: 216.32s\n",
            "Epoch 6, step (batch no.): 1224 -- acc: 0.11, loss 2.30 -- iter 31000/55000, training for: 217.48s\n",
            "Epoch 6, step (batch no.): 1231 -- acc: 0.11, loss 2.30 -- iter 32750/55000, training for: 218.64s\n",
            "Epoch 6, step (batch no.): 1237 -- acc: 0.11, loss 2.30 -- iter 34250/55000, training for: 219.64s\n",
            "Epoch 6, step (batch no.): 1243 -- acc: 0.11, loss 2.30 -- iter 35750/55000, training for: 220.66s\n",
            "Epoch 6, step (batch no.): 1249 -- acc: 0.12, loss 2.30 -- iter 37250/55000, training for: 221.68s\n",
            "Epoch 6, step (batch no.): 1256 -- acc: 0.12, loss 2.30 -- iter 39000/55000, training for: 222.85s\n",
            "Epoch 6, step (batch no.): 1262 -- acc: 0.11, loss 2.30 -- iter 40500/55000, training for: 223.86s\n",
            "Epoch 6, step (batch no.): 1269 -- acc: 0.11, loss 2.30 -- iter 42250/55000, training for: 225.02s\n",
            "Epoch 6, step (batch no.): 1276 -- acc: 0.11, loss 2.30 -- iter 44000/55000, training for: 226.17s\n",
            "Epoch 6, step (batch no.): 1282 -- acc: 0.11, loss 2.30 -- iter 45500/55000, training for: 227.18s\n",
            "Epoch 6, step (batch no.): 1289 -- acc: 0.11, loss 2.30 -- iter 47250/55000, training for: 228.34s\n",
            "Epoch 6, step (batch no.): 1295 -- acc: 0.12, loss 2.30 -- iter 48750/55000, training for: 229.40s\n",
            "Epoch 6, step (batch no.): 1301 -- acc: 0.11, loss 2.30 -- iter 50250/55000, training for: 230.43s\n",
            "Epoch 6, step (batch no.): 1307 -- acc: 0.11, loss 2.30 -- iter 51750/55000, training for: 231.45s\n",
            "Epoch 6, step (batch no.): 1313 -- acc: 0.11, loss 2.30 -- iter 53250/55000, training for: 232.46s\n",
            "Epoch 6, step (batch no.): 1319 -- acc: 0.10, loss 2.30 -- iter 54750/55000, training for: 233.48s\n",
            "Training Step: 1320  | total loss: \u001b[1m\u001b[32m2.30205\u001b[0m\u001b[0m | time: 39.228s\n",
            "| Adam | epoch: 006 | loss: 2.30205 - acc: 0.1060 | val_loss: 2.30156 - val_acc: 0.1135 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 6, step (batch no.): 1320 -- acc: 0.11, loss 2.30 -- iter 55000/55000, training for: 235.75s\n",
            "Epoch 7, step (batch no.): 1326 -- acc: 0.11, loss 2.30 -- iter 01500/55000, training for: 236.82s\n",
            "Epoch 7, step (batch no.): 1332 -- acc: 0.11, loss 2.30 -- iter 03000/55000, training for: 237.83s\n",
            "Epoch 7, step (batch no.): 1338 -- acc: 0.11, loss 2.30 -- iter 04500/55000, training for: 238.83s\n",
            "Epoch 7, step (batch no.): 1344 -- acc: 0.11, loss 2.30 -- iter 06000/55000, training for: 239.83s\n",
            "Epoch 7, step (batch no.): 1350 -- acc: 0.11, loss 2.30 -- iter 07500/55000, training for: 240.83s\n",
            "Epoch 7, step (batch no.): 1356 -- acc: 0.11, loss 2.30 -- iter 09000/55000, training for: 241.92s\n",
            "Epoch 7, step (batch no.): 1362 -- acc: 0.11, loss 2.30 -- iter 10500/55000, training for: 242.94s\n",
            "Epoch 7, step (batch no.): 1369 -- acc: 0.12, loss 2.30 -- iter 12250/55000, training for: 244.10s\n",
            "Epoch 7, step (batch no.): 1376 -- acc: 0.13, loss 2.30 -- iter 14000/55000, training for: 245.27s\n",
            "Epoch 7, step (batch no.): 1383 -- acc: 0.11, loss 2.30 -- iter 15750/55000, training for: 246.43s\n",
            "Epoch 7, step (batch no.): 1390 -- acc: 0.11, loss 2.30 -- iter 17500/55000, training for: 247.59s\n",
            "Epoch 7, step (batch no.): 1396 -- acc: 0.11, loss 2.30 -- iter 19000/55000, training for: 248.59s\n",
            "Epoch 7, step (batch no.): 1403 -- acc: 0.11, loss 2.30 -- iter 20750/55000, training for: 249.75s\n",
            "Epoch 7, step (batch no.): 1410 -- acc: 0.11, loss 2.30 -- iter 22500/55000, training for: 250.91s\n",
            "Epoch 7, step (batch no.): 1417 -- acc: 0.11, loss 2.30 -- iter 24250/55000, training for: 252.08s\n",
            "Epoch 7, step (batch no.): 1423 -- acc: 0.11, loss 2.30 -- iter 25750/55000, training for: 253.08s\n",
            "Epoch 7, step (batch no.): 1429 -- acc: 0.11, loss 2.30 -- iter 27250/55000, training for: 254.11s\n",
            "Epoch 7, step (batch no.): 1435 -- acc: 0.11, loss 2.30 -- iter 28750/55000, training for: 255.12s\n",
            "Epoch 7, step (batch no.): 1441 -- acc: 0.12, loss 2.30 -- iter 30250/55000, training for: 256.13s\n",
            "Epoch 7, step (batch no.): 1447 -- acc: 0.11, loss 2.30 -- iter 31750/55000, training for: 257.14s\n",
            "Epoch 7, step (batch no.): 1454 -- acc: 0.11, loss 2.30 -- iter 33500/55000, training for: 258.30s\n",
            "Epoch 7, step (batch no.): 1460 -- acc: 0.12, loss 2.30 -- iter 35000/55000, training for: 259.31s\n",
            "Epoch 7, step (batch no.): 1466 -- acc: 0.11, loss 2.30 -- iter 36500/55000, training for: 260.33s\n",
            "Epoch 7, step (batch no.): 1472 -- acc: 0.12, loss 2.30 -- iter 38000/55000, training for: 261.33s\n",
            "Epoch 7, step (batch no.): 1478 -- acc: 0.11, loss 2.30 -- iter 39500/55000, training for: 262.36s\n",
            "Epoch 7, step (batch no.): 1485 -- acc: 0.11, loss 2.30 -- iter 41250/55000, training for: 263.53s\n",
            "Epoch 7, step (batch no.): 1491 -- acc: 0.11, loss 2.30 -- iter 42750/55000, training for: 264.54s\n",
            "Epoch 7, step (batch no.): 1497 -- acc: 0.11, loss 2.30 -- iter 44250/55000, training for: 265.55s\n",
            "Epoch 7, step (batch no.): 1503 -- acc: 0.11, loss 2.30 -- iter 45750/55000, training for: 266.56s\n",
            "Epoch 7, step (batch no.): 1509 -- acc: 0.11, loss 2.30 -- iter 47250/55000, training for: 267.57s\n",
            "Epoch 7, step (batch no.): 1515 -- acc: 0.12, loss 2.30 -- iter 48750/55000, training for: 268.63s\n",
            "Epoch 7, step (batch no.): 1522 -- acc: 0.11, loss 2.30 -- iter 50500/55000, training for: 269.80s\n",
            "Epoch 7, step (batch no.): 1528 -- acc: 0.11, loss 2.30 -- iter 52000/55000, training for: 270.80s\n",
            "Epoch 7, step (batch no.): 1534 -- acc: 0.11, loss 2.30 -- iter 53500/55000, training for: 271.81s\n",
            "Training Step: 1540  | total loss: \u001b[1m\u001b[32m2.30150\u001b[0m\u001b[0m | time: 39.075s\n",
            "| Adam | epoch: 007 | loss: 2.30150 - acc: 0.1081 | val_loss: 2.30237 - val_acc: 0.1010 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 7, step (batch no.): 1540 -- acc: 0.11, loss 2.30 -- iter 55000/55000, training for: 274.84s\n",
            "Epoch 8, step (batch no.): 1546 -- acc: 0.10, loss 2.30 -- iter 01500/55000, training for: 275.84s\n",
            "Epoch 8, step (batch no.): 1552 -- acc: 0.11, loss 2.30 -- iter 03000/55000, training for: 276.84s\n",
            "Epoch 8, step (batch no.): 1558 -- acc: 0.11, loss 2.30 -- iter 04500/55000, training for: 277.85s\n",
            "Epoch 8, step (batch no.): 1564 -- acc: 0.11, loss 2.30 -- iter 06000/55000, training for: 278.86s\n",
            "Epoch 8, step (batch no.): 1571 -- acc: 0.11, loss 2.30 -- iter 07750/55000, training for: 280.02s\n",
            "Epoch 8, step (batch no.): 1577 -- acc: 0.12, loss 2.30 -- iter 09250/55000, training for: 281.02s\n",
            "Epoch 8, step (batch no.): 1583 -- acc: 0.13, loss 2.30 -- iter 10750/55000, training for: 282.02s\n",
            "Epoch 8, step (batch no.): 1589 -- acc: 0.12, loss 2.30 -- iter 12250/55000, training for: 283.03s\n",
            "Epoch 8, step (batch no.): 1595 -- acc: 0.11, loss 2.30 -- iter 13750/55000, training for: 284.03s\n",
            "Epoch 8, step (batch no.): 1602 -- acc: 0.11, loss 2.30 -- iter 15500/55000, training for: 285.19s\n",
            "Epoch 8, step (batch no.): 1609 -- acc: 0.11, loss 2.30 -- iter 17250/55000, training for: 286.35s\n",
            "Epoch 8, step (batch no.): 1615 -- acc: 0.11, loss 2.30 -- iter 18750/55000, training for: 287.36s\n",
            "Epoch 8, step (batch no.): 1621 -- acc: 0.12, loss 2.30 -- iter 20250/55000, training for: 288.36s\n",
            "Epoch 8, step (batch no.): 1627 -- acc: 0.12, loss 2.30 -- iter 21750/55000, training for: 289.41s\n",
            "Epoch 8, step (batch no.): 1633 -- acc: 0.11, loss 2.30 -- iter 23250/55000, training for: 290.45s\n",
            "Epoch 8, step (batch no.): 1639 -- acc: 0.12, loss 2.30 -- iter 24750/55000, training for: 291.46s\n",
            "Epoch 8, step (batch no.): 1645 -- acc: 0.11, loss 2.30 -- iter 26250/55000, training for: 292.48s\n",
            "Epoch 8, step (batch no.): 1651 -- acc: 0.11, loss 2.30 -- iter 27750/55000, training for: 293.48s\n",
            "Epoch 8, step (batch no.): 1657 -- acc: 0.11, loss 2.30 -- iter 29250/55000, training for: 294.48s\n",
            "Epoch 8, step (batch no.): 1663 -- acc: 0.12, loss 2.30 -- iter 30750/55000, training for: 295.50s\n",
            "Epoch 8, step (batch no.): 1669 -- acc: 0.12, loss 2.30 -- iter 32250/55000, training for: 296.52s\n",
            "Epoch 8, step (batch no.): 1675 -- acc: 0.12, loss 2.30 -- iter 33750/55000, training for: 297.54s\n",
            "Epoch 8, step (batch no.): 1681 -- acc: 0.11, loss 2.30 -- iter 35250/55000, training for: 298.54s\n",
            "Epoch 8, step (batch no.): 1687 -- acc: 0.11, loss 2.30 -- iter 36750/55000, training for: 299.56s\n",
            "Epoch 8, step (batch no.): 1693 -- acc: 0.11, loss 2.30 -- iter 38250/55000, training for: 300.57s\n",
            "Epoch 8, step (batch no.): 1699 -- acc: 0.11, loss 2.30 -- iter 39750/55000, training for: 301.64s\n",
            "Epoch 8, step (batch no.): 1705 -- acc: 0.11, loss 2.30 -- iter 41250/55000, training for: 302.71s\n",
            "Epoch 8, step (batch no.): 1711 -- acc: 0.11, loss 2.30 -- iter 42750/55000, training for: 303.72s\n",
            "Epoch 8, step (batch no.): 1718 -- acc: 0.11, loss 2.30 -- iter 44500/55000, training for: 304.88s\n",
            "Epoch 8, step (batch no.): 1724 -- acc: 0.12, loss 2.30 -- iter 46000/55000, training for: 305.90s\n",
            "Epoch 8, step (batch no.): 1730 -- acc: 0.12, loss 2.30 -- iter 47500/55000, training for: 306.90s\n",
            "Epoch 8, step (batch no.): 1737 -- acc: 0.11, loss 2.30 -- iter 49250/55000, training for: 308.06s\n",
            "Epoch 8, step (batch no.): 1744 -- acc: 0.11, loss 2.30 -- iter 51000/55000, training for: 309.23s\n",
            "Epoch 8, step (batch no.): 1751 -- acc: 0.11, loss 2.30 -- iter 52750/55000, training for: 310.39s\n",
            "Epoch 8, step (batch no.): 1757 -- acc: 0.11, loss 2.30 -- iter 54250/55000, training for: 311.39s\n",
            "Training Step: 1760  | total loss: \u001b[1m\u001b[32m2.30279\u001b[0m\u001b[0m | time: 39.070s\n",
            "| Adam | epoch: 008 | loss: 2.30279 - acc: 0.1095 | val_loss: 2.30096 - val_acc: 0.1135 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 8, step (batch no.): 1760 -- acc: 0.11, loss 2.30 -- iter 55000/55000, training for: 313.93s\n",
            "Epoch 9, step (batch no.): 1766 -- acc: 0.11, loss 2.30 -- iter 01500/55000, training for: 314.94s\n",
            "Epoch 9, step (batch no.): 1773 -- acc: 0.11, loss 2.30 -- iter 03250/55000, training for: 316.09s\n",
            "Epoch 9, step (batch no.): 1779 -- acc: 0.11, loss 2.30 -- iter 04750/55000, training for: 317.10s\n",
            "Epoch 9, step (batch no.): 1786 -- acc: 0.11, loss 2.30 -- iter 06500/55000, training for: 318.28s\n",
            "Epoch 9, step (batch no.): 1792 -- acc: 0.11, loss 2.30 -- iter 08000/55000, training for: 319.30s\n",
            "Epoch 9, step (batch no.): 1798 -- acc: 0.11, loss 2.30 -- iter 09500/55000, training for: 320.31s\n",
            "Epoch 9, step (batch no.): 1805 -- acc: 0.11, loss 2.30 -- iter 11250/55000, training for: 321.47s\n",
            "Epoch 9, step (batch no.): 1811 -- acc: 0.12, loss 2.30 -- iter 12750/55000, training for: 322.50s\n",
            "Epoch 9, step (batch no.): 1817 -- acc: 0.11, loss 2.30 -- iter 14250/55000, training for: 323.52s\n",
            "Epoch 9, step (batch no.): 1823 -- acc: 0.11, loss 2.30 -- iter 15750/55000, training for: 324.52s\n",
            "Epoch 9, step (batch no.): 1829 -- acc: 0.12, loss 2.30 -- iter 17250/55000, training for: 325.53s\n",
            "Epoch 9, step (batch no.): 1835 -- acc: 0.11, loss 2.30 -- iter 18750/55000, training for: 326.53s\n",
            "Epoch 9, step (batch no.): 1841 -- acc: 0.11, loss 2.30 -- iter 20250/55000, training for: 327.53s\n",
            "Epoch 9, step (batch no.): 1847 -- acc: 0.11, loss 2.30 -- iter 21750/55000, training for: 328.55s\n",
            "Epoch 9, step (batch no.): 1853 -- acc: 0.11, loss 2.30 -- iter 23250/55000, training for: 329.55s\n",
            "Epoch 9, step (batch no.): 1859 -- acc: 0.10, loss 2.30 -- iter 24750/55000, training for: 330.56s\n",
            "Epoch 9, step (batch no.): 1865 -- acc: 0.11, loss 2.30 -- iter 26250/55000, training for: 331.58s\n",
            "Epoch 9, step (batch no.): 1871 -- acc: 0.10, loss 2.30 -- iter 27750/55000, training for: 332.58s\n",
            "Epoch 9, step (batch no.): 1877 -- acc: 0.10, loss 2.30 -- iter 29250/55000, training for: 333.60s\n",
            "Epoch 9, step (batch no.): 1883 -- acc: 0.11, loss 2.30 -- iter 30750/55000, training for: 334.62s\n",
            "Epoch 9, step (batch no.): 1889 -- acc: 0.11, loss 2.30 -- iter 32250/55000, training for: 335.63s\n",
            "Epoch 9, step (batch no.): 1895 -- acc: 0.11, loss 2.30 -- iter 33750/55000, training for: 336.65s\n",
            "Epoch 9, step (batch no.): 1901 -- acc: 0.12, loss 2.30 -- iter 35250/55000, training for: 337.66s\n",
            "Epoch 9, step (batch no.): 1907 -- acc: 0.11, loss 2.30 -- iter 36750/55000, training for: 338.67s\n",
            "Epoch 9, step (batch no.): 1913 -- acc: 0.12, loss 2.30 -- iter 38250/55000, training for: 339.68s\n",
            "Epoch 9, step (batch no.): 1919 -- acc: 0.12, loss 2.30 -- iter 39750/55000, training for: 340.68s\n",
            "Epoch 9, step (batch no.): 1925 -- acc: 0.11, loss 2.30 -- iter 41250/55000, training for: 341.70s\n",
            "Epoch 9, step (batch no.): 1931 -- acc: 0.12, loss 2.30 -- iter 42750/55000, training for: 342.72s\n",
            "Epoch 9, step (batch no.): 1937 -- acc: 0.12, loss 2.30 -- iter 44250/55000, training for: 343.73s\n",
            "Epoch 9, step (batch no.): 1944 -- acc: 0.11, loss 2.30 -- iter 46000/55000, training for: 344.89s\n",
            "Epoch 9, step (batch no.): 1951 -- acc: 0.11, loss 2.30 -- iter 47750/55000, training for: 346.05s\n",
            "Epoch 9, step (batch no.): 1957 -- acc: 0.11, loss 2.30 -- iter 49250/55000, training for: 347.06s\n",
            "Epoch 9, step (batch no.): 1964 -- acc: 0.11, loss 2.30 -- iter 51000/55000, training for: 348.22s\n",
            "Epoch 9, step (batch no.): 1970 -- acc: 0.11, loss 2.30 -- iter 52500/55000, training for: 349.24s\n",
            "Epoch 9, step (batch no.): 1976 -- acc: 0.11, loss 2.30 -- iter 54000/55000, training for: 350.27s\n",
            "Training Step: 1980  | total loss: \u001b[1m\u001b[32m2.30152\u001b[0m\u001b[0m | time: 39.046s\n",
            "| Adam | epoch: 009 | loss: 2.30152 - acc: 0.1132 | val_loss: 2.30130 - val_acc: 0.1135 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 9, step (batch no.): 1980 -- acc: 0.11, loss 2.30 -- iter 55000/55000, training for: 352.99s\n",
            "Epoch 10, step (batch no.): 1986 -- acc: 0.11, loss 2.30 -- iter 01500/55000, training for: 354.00s\n",
            "Epoch 10, step (batch no.): 1992 -- acc: 0.10, loss 2.30 -- iter 03000/55000, training for: 355.00s\n",
            "Epoch 10, step (batch no.): 1998 -- acc: 0.11, loss 2.30 -- iter 04500/55000, training for: 356.01s\n",
            "Epoch 10, step (batch no.): 2004 -- acc: 0.11, loss 2.30 -- iter 06000/55000, training for: 357.03s\n",
            "Epoch 10, step (batch no.): 2011 -- acc: 0.11, loss 2.30 -- iter 07750/55000, training for: 358.19s\n",
            "Epoch 10, step (batch no.): 2017 -- acc: 0.12, loss 2.30 -- iter 09250/55000, training for: 359.20s\n",
            "Epoch 10, step (batch no.): 2023 -- acc: 0.11, loss 2.30 -- iter 10750/55000, training for: 360.20s\n",
            "Epoch 10, step (batch no.): 2029 -- acc: 0.11, loss 2.30 -- iter 12250/55000, training for: 361.22s\n",
            "Epoch 10, step (batch no.): 2035 -- acc: 0.11, loss 2.30 -- iter 13750/55000, training for: 362.24s\n",
            "Epoch 10, step (batch no.): 2041 -- acc: 0.11, loss 2.30 -- iter 15250/55000, training for: 363.24s\n",
            "Epoch 10, step (batch no.): 2048 -- acc: 0.11, loss 2.30 -- iter 17000/55000, training for: 364.41s\n",
            "Epoch 10, step (batch no.): 2054 -- acc: 0.11, loss 2.30 -- iter 18500/55000, training for: 365.41s\n",
            "Epoch 10, step (batch no.): 2060 -- acc: 0.11, loss 2.30 -- iter 20000/55000, training for: 366.42s\n",
            "Epoch 10, step (batch no.): 2066 -- acc: 0.12, loss 2.30 -- iter 21500/55000, training for: 367.50s\n",
            "Epoch 10, step (batch no.): 2072 -- acc: 0.11, loss 2.30 -- iter 23000/55000, training for: 368.51s\n",
            "Epoch 10, step (batch no.): 2079 -- acc: 0.11, loss 2.30 -- iter 24750/55000, training for: 369.66s\n",
            "Epoch 10, step (batch no.): 2085 -- acc: 0.11, loss 2.30 -- iter 26250/55000, training for: 370.67s\n",
            "Epoch 10, step (batch no.): 2092 -- acc: 0.11, loss 2.30 -- iter 28000/55000, training for: 371.82s\n",
            "Epoch 10, step (batch no.): 2098 -- acc: 0.12, loss 2.30 -- iter 29500/55000, training for: 372.82s\n",
            "Epoch 10, step (batch no.): 2104 -- acc: 0.12, loss 2.30 -- iter 31000/55000, training for: 373.83s\n",
            "Epoch 10, step (batch no.): 2111 -- acc: 0.12, loss 2.30 -- iter 32750/55000, training for: 374.99s\n",
            "Epoch 10, step (batch no.): 2117 -- acc: 0.12, loss 2.30 -- iter 34250/55000, training for: 375.99s\n",
            "Epoch 10, step (batch no.): 2124 -- acc: 0.12, loss 2.30 -- iter 36000/55000, training for: 377.16s\n",
            "Epoch 10, step (batch no.): 2131 -- acc: 0.12, loss 2.30 -- iter 37750/55000, training for: 378.31s\n",
            "Epoch 10, step (batch no.): 2137 -- acc: 0.12, loss 2.30 -- iter 39250/55000, training for: 379.33s\n",
            "Epoch 10, step (batch no.): 2143 -- acc: 0.11, loss 2.30 -- iter 40750/55000, training for: 380.38s\n",
            "Epoch 10, step (batch no.): 2150 -- acc: 0.11, loss 2.30 -- iter 42500/55000, training for: 381.54s\n",
            "Epoch 10, step (batch no.): 2156 -- acc: 0.11, loss 2.30 -- iter 44000/55000, training for: 382.54s\n",
            "Epoch 10, step (batch no.): 2163 -- acc: 0.11, loss 2.30 -- iter 45750/55000, training for: 383.69s\n",
            "Epoch 10, step (batch no.): 2170 -- acc: 0.11, loss 2.30 -- iter 47500/55000, training for: 384.85s\n",
            "Epoch 10, step (batch no.): 2177 -- acc: 0.11, loss 2.30 -- iter 49250/55000, training for: 386.01s\n",
            "Epoch 10, step (batch no.): 2183 -- acc: 0.11, loss 2.30 -- iter 50750/55000, training for: 387.03s\n",
            "Epoch 10, step (batch no.): 2189 -- acc: 0.11, loss 2.30 -- iter 52250/55000, training for: 388.03s\n",
            "Epoch 10, step (batch no.): 2195 -- acc: 0.11, loss 2.30 -- iter 53750/55000, training for: 389.03s\n",
            "Training Step: 2200  | total loss: \u001b[1m\u001b[32m2.30055\u001b[0m\u001b[0m | time: 38.934s\n",
            "| Adam | epoch: 010 | loss: 2.30055 - acc: 0.1135 | val_loss: 2.30161 - val_acc: 0.1135 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 10, step (batch no.): 2200 -- acc: 0.11, loss 2.30 -- iter 55000/55000, training for: 391.94s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: VJGT8S\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "---------------------------------\n",
            "Training samples: 55000\n",
            "Validation samples: 10000\n",
            "--\n",
            "Epoch 1, step (batch no.): 1 -- acc: 0.00, loss 0.00 -- iter 00250/55000, training for: 0.33s\n",
            "Epoch 1, step (batch no.): 7 -- acc: 0.34, loss 2.32 -- iter 01750/55000, training for: 1.56s\n",
            "Epoch 1, step (batch no.): 13 -- acc: 0.72, loss 0.84 -- iter 03250/55000, training for: 2.75s\n",
            "Epoch 1, step (batch no.): 18 -- acc: 0.84, loss 0.49 -- iter 04500/55000, training for: 3.76s\n",
            "Epoch 1, step (batch no.): 24 -- acc: 0.90, loss 0.34 -- iter 06000/55000, training for: 4.95s\n",
            "Epoch 1, step (batch no.): 30 -- acc: 0.91, loss 0.30 -- iter 07500/55000, training for: 6.14s\n",
            "Epoch 1, step (batch no.): 36 -- acc: 0.91, loss 0.30 -- iter 09000/55000, training for: 7.33s\n",
            "Epoch 1, step (batch no.): 41 -- acc: 0.93, loss 0.24 -- iter 10250/55000, training for: 8.43s\n",
            "Epoch 1, step (batch no.): 47 -- acc: 0.93, loss 0.24 -- iter 11750/55000, training for: 9.61s\n",
            "Epoch 1, step (batch no.): 52 -- acc: 0.94, loss 0.20 -- iter 13000/55000, training for: 10.63s\n",
            "Epoch 1, step (batch no.): 58 -- acc: 0.95, loss 0.17 -- iter 14500/55000, training for: 11.83s\n",
            "Epoch 1, step (batch no.): 64 -- acc: 0.95, loss 0.16 -- iter 16000/55000, training for: 13.02s\n",
            "Epoch 1, step (batch no.): 70 -- acc: 0.96, loss 0.15 -- iter 17500/55000, training for: 14.19s\n",
            "Epoch 1, step (batch no.): 76 -- acc: 0.96, loss 0.13 -- iter 19000/55000, training for: 15.36s\n",
            "Epoch 1, step (batch no.): 81 -- acc: 0.96, loss 0.13 -- iter 20250/55000, training for: 16.39s\n",
            "Epoch 1, step (batch no.): 86 -- acc: 0.96, loss 0.12 -- iter 21500/55000, training for: 17.46s\n",
            "Epoch 1, step (batch no.): 91 -- acc: 0.97, loss 0.12 -- iter 22750/55000, training for: 18.47s\n",
            "Epoch 1, step (batch no.): 97 -- acc: 0.96, loss 0.12 -- iter 24250/55000, training for: 19.65s\n",
            "Epoch 1, step (batch no.): 103 -- acc: 0.97, loss 0.11 -- iter 25750/55000, training for: 20.84s\n",
            "Epoch 1, step (batch no.): 109 -- acc: 0.97, loss 0.11 -- iter 27250/55000, training for: 22.03s\n",
            "Epoch 1, step (batch no.): 115 -- acc: 0.96, loss 0.13 -- iter 28750/55000, training for: 23.23s\n",
            "Epoch 1, step (batch no.): 121 -- acc: 0.96, loss 0.12 -- iter 30250/55000, training for: 24.42s\n",
            "Epoch 1, step (batch no.): 127 -- acc: 0.97, loss 0.12 -- iter 31750/55000, training for: 25.62s\n",
            "Epoch 1, step (batch no.): 132 -- acc: 0.97, loss 0.11 -- iter 33000/55000, training for: 26.62s\n",
            "Epoch 1, step (batch no.): 137 -- acc: 0.97, loss 0.12 -- iter 34250/55000, training for: 27.63s\n",
            "Epoch 1, step (batch no.): 143 -- acc: 0.97, loss 0.10 -- iter 35750/55000, training for: 28.82s\n",
            "Epoch 1, step (batch no.): 148 -- acc: 0.97, loss 0.11 -- iter 37000/55000, training for: 29.82s\n",
            "Epoch 1, step (batch no.): 153 -- acc: 0.97, loss 0.10 -- iter 38250/55000, training for: 30.83s\n",
            "Epoch 1, step (batch no.): 159 -- acc: 0.97, loss 0.11 -- iter 39750/55000, training for: 32.01s\n",
            "Epoch 1, step (batch no.): 165 -- acc: 0.97, loss 0.11 -- iter 41250/55000, training for: 33.20s\n",
            "Epoch 1, step (batch no.): 171 -- acc: 0.97, loss 0.11 -- iter 42750/55000, training for: 34.38s\n",
            "Epoch 1, step (batch no.): 176 -- acc: 0.97, loss 0.10 -- iter 44000/55000, training for: 35.39s\n",
            "Epoch 1, step (batch no.): 182 -- acc: 0.97, loss 0.11 -- iter 45500/55000, training for: 36.58s\n",
            "Epoch 1, step (batch no.): 188 -- acc: 0.97, loss 0.10 -- iter 47000/55000, training for: 37.77s\n",
            "Epoch 1, step (batch no.): 194 -- acc: 0.90, loss 0.93 -- iter 48500/55000, training for: 38.94s\n",
            "Epoch 1, step (batch no.): 200 -- acc: 0.93, loss 0.55 -- iter 50000/55000, training for: 40.12s\n",
            "Epoch 1, step (batch no.): 205 -- acc: 0.95, loss 0.38 -- iter 51250/55000, training for: 41.17s\n",
            "Epoch 1, step (batch no.): 211 -- acc: 0.95, loss 0.28 -- iter 52750/55000, training for: 42.35s\n",
            "Epoch 1, step (batch no.): 217 -- acc: 0.95, loss 0.24 -- iter 54250/55000, training for: 43.52s\n",
            "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.20859\u001b[0m\u001b[0m | time: 47.131s\n",
            "| Adam | epoch: 001 | loss: 0.20859 - acc: 0.9576 | val_loss: 0.15929 - val_acc: 0.9555 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 1, step (batch no.): 220 -- acc: 0.96, loss 0.21 -- iter 55000/55000, training for: 47.15s\n",
            "Epoch 2, step (batch no.): 225 -- acc: 0.96, loss 0.18 -- iter 01250/55000, training for: 48.15s\n",
            "Epoch 2, step (batch no.): 231 -- acc: 0.96, loss 0.18 -- iter 02750/55000, training for: 49.32s\n",
            "Epoch 2, step (batch no.): 237 -- acc: 0.96, loss 0.17 -- iter 04250/55000, training for: 50.50s\n",
            "Epoch 2, step (batch no.): 243 -- acc: 0.96, loss 0.16 -- iter 05750/55000, training for: 51.69s\n",
            "Epoch 2, step (batch no.): 249 -- acc: 0.96, loss 0.16 -- iter 07250/55000, training for: 52.86s\n",
            "Epoch 2, step (batch no.): 255 -- acc: 0.96, loss 0.16 -- iter 08750/55000, training for: 54.05s\n",
            "Epoch 2, step (batch no.): 261 -- acc: 0.97, loss 0.14 -- iter 10250/55000, training for: 55.22s\n",
            "Epoch 2, step (batch no.): 266 -- acc: 0.97, loss 0.14 -- iter 11500/55000, training for: 56.22s\n",
            "Epoch 2, step (batch no.): 272 -- acc: 0.97, loss 0.14 -- iter 13000/55000, training for: 57.40s\n",
            "Epoch 2, step (batch no.): 278 -- acc: 0.97, loss 0.11 -- iter 14500/55000, training for: 58.58s\n",
            "Epoch 2, step (batch no.): 284 -- acc: 0.97, loss 0.11 -- iter 16000/55000, training for: 59.77s\n",
            "Epoch 2, step (batch no.): 290 -- acc: 0.98, loss 0.10 -- iter 17500/55000, training for: 60.94s\n",
            "Epoch 2, step (batch no.): 296 -- acc: 0.98, loss 0.09 -- iter 19000/55000, training for: 62.13s\n",
            "Epoch 2, step (batch no.): 302 -- acc: 0.98, loss 0.09 -- iter 20500/55000, training for: 63.31s\n",
            "Epoch 2, step (batch no.): 308 -- acc: 0.98, loss 0.08 -- iter 22000/55000, training for: 64.48s\n",
            "Epoch 2, step (batch no.): 314 -- acc: 0.98, loss 0.10 -- iter 23500/55000, training for: 65.66s\n",
            "Epoch 2, step (batch no.): 320 -- acc: 0.97, loss 0.10 -- iter 25000/55000, training for: 66.83s\n",
            "Epoch 2, step (batch no.): 325 -- acc: 0.98, loss 0.10 -- iter 26250/55000, training for: 67.83s\n",
            "Epoch 2, step (batch no.): 331 -- acc: 0.98, loss 0.08 -- iter 27750/55000, training for: 69.03s\n",
            "Epoch 2, step (batch no.): 336 -- acc: 0.98, loss 0.08 -- iter 29000/55000, training for: 70.04s\n",
            "Epoch 2, step (batch no.): 342 -- acc: 0.98, loss 0.07 -- iter 30500/55000, training for: 71.24s\n",
            "Epoch 2, step (batch no.): 348 -- acc: 0.98, loss 0.08 -- iter 32000/55000, training for: 72.42s\n",
            "Epoch 2, step (batch no.): 353 -- acc: 0.98, loss 0.09 -- iter 33250/55000, training for: 73.43s\n",
            "Epoch 2, step (batch no.): 358 -- acc: 0.98, loss 0.10 -- iter 34500/55000, training for: 74.48s\n",
            "Epoch 2, step (batch no.): 364 -- acc: 0.97, loss 0.10 -- iter 36000/55000, training for: 75.67s\n",
            "Epoch 2, step (batch no.): 369 -- acc: 0.98, loss 0.09 -- iter 37250/55000, training for: 76.69s\n",
            "Epoch 2, step (batch no.): 375 -- acc: 0.98, loss 0.08 -- iter 38750/55000, training for: 77.89s\n",
            "Epoch 2, step (batch no.): 380 -- acc: 0.97, loss 0.10 -- iter 40000/55000, training for: 78.89s\n",
            "Epoch 2, step (batch no.): 385 -- acc: 0.98, loss 0.09 -- iter 41250/55000, training for: 79.90s\n",
            "Epoch 2, step (batch no.): 391 -- acc: 0.98, loss 0.09 -- iter 42750/55000, training for: 81.07s\n",
            "Epoch 2, step (batch no.): 397 -- acc: 0.98, loss 0.09 -- iter 44250/55000, training for: 82.26s\n",
            "Epoch 2, step (batch no.): 403 -- acc: 0.98, loss 0.08 -- iter 45750/55000, training for: 83.44s\n",
            "Epoch 2, step (batch no.): 408 -- acc: 0.98, loss 0.08 -- iter 47000/55000, training for: 84.44s\n",
            "Epoch 2, step (batch no.): 414 -- acc: 0.90, loss 1.09 -- iter 48500/55000, training for: 85.62s\n",
            "Epoch 2, step (batch no.): 420 -- acc: 0.94, loss 0.62 -- iter 50000/55000, training for: 86.79s\n",
            "Epoch 2, step (batch no.): 426 -- acc: 0.96, loss 0.36 -- iter 51500/55000, training for: 87.98s\n",
            "Epoch 2, step (batch no.): 432 -- acc: 0.97, loss 0.23 -- iter 53000/55000, training for: 89.17s\n",
            "Epoch 2, step (batch no.): 438 -- acc: 0.98, loss 0.16 -- iter 54500/55000, training for: 90.35s\n",
            "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.14904\u001b[0m\u001b[0m | time: 46.589s\n",
            "| Adam | epoch: 002 | loss: 0.14904 - acc: 0.9746 | val_loss: 0.09210 - val_acc: 0.9739 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 2, step (batch no.): 440 -- acc: 0.97, loss 0.15 -- iter 55000/55000, training for: 93.75s\n",
            "Epoch 3, step (batch no.): 445 -- acc: 0.98, loss 0.13 -- iter 01250/55000, training for: 94.76s\n",
            "Epoch 3, step (batch no.): 451 -- acc: 0.98, loss 0.11 -- iter 02750/55000, training for: 95.95s\n",
            "Epoch 3, step (batch no.): 457 -- acc: 0.98, loss 0.10 -- iter 04250/55000, training for: 97.14s\n",
            "Epoch 3, step (batch no.): 463 -- acc: 0.97, loss 0.10 -- iter 05750/55000, training for: 98.32s\n",
            "Epoch 3, step (batch no.): 469 -- acc: 0.97, loss 0.11 -- iter 07250/55000, training for: 99.50s\n",
            "Epoch 3, step (batch no.): 475 -- acc: 0.97, loss 0.10 -- iter 08750/55000, training for: 100.71s\n",
            "Epoch 3, step (batch no.): 480 -- acc: 0.98, loss 0.08 -- iter 10000/55000, training for: 101.71s\n",
            "Epoch 3, step (batch no.): 485 -- acc: 0.98, loss 0.09 -- iter 11250/55000, training for: 102.72s\n",
            "Epoch 3, step (batch no.): 491 -- acc: 0.98, loss 0.09 -- iter 12750/55000, training for: 103.92s\n",
            "Epoch 3, step (batch no.): 496 -- acc: 0.98, loss 0.08 -- iter 14000/55000, training for: 104.95s\n",
            "Epoch 3, step (batch no.): 501 -- acc: 0.97, loss 0.10 -- iter 15250/55000, training for: 105.95s\n",
            "Epoch 3, step (batch no.): 506 -- acc: 0.97, loss 0.10 -- iter 16500/55000, training for: 106.96s\n",
            "Epoch 3, step (batch no.): 511 -- acc: 0.97, loss 0.10 -- iter 17750/55000, training for: 107.97s\n",
            "Epoch 3, step (batch no.): 516 -- acc: 0.98, loss 0.09 -- iter 19000/55000, training for: 109.00s\n",
            "Epoch 3, step (batch no.): 521 -- acc: 0.98, loss 0.08 -- iter 20250/55000, training for: 110.00s\n",
            "Epoch 3, step (batch no.): 527 -- acc: 0.98, loss 0.07 -- iter 21750/55000, training for: 111.19s\n",
            "Epoch 3, step (batch no.): 532 -- acc: 0.98, loss 0.08 -- iter 23000/55000, training for: 112.21s\n",
            "Epoch 3, step (batch no.): 538 -- acc: 0.98, loss 0.07 -- iter 24500/55000, training for: 113.40s\n",
            "Epoch 3, step (batch no.): 544 -- acc: 0.98, loss 0.08 -- iter 26000/55000, training for: 114.60s\n",
            "Epoch 3, step (batch no.): 550 -- acc: 0.98, loss 0.09 -- iter 27500/55000, training for: 115.78s\n",
            "Epoch 3, step (batch no.): 556 -- acc: 0.98, loss 0.08 -- iter 29000/55000, training for: 116.99s\n",
            "Epoch 3, step (batch no.): 562 -- acc: 0.98, loss 0.07 -- iter 30500/55000, training for: 118.18s\n",
            "Epoch 3, step (batch no.): 568 -- acc: 0.98, loss 0.07 -- iter 32000/55000, training for: 119.35s\n",
            "Epoch 3, step (batch no.): 574 -- acc: 0.98, loss 0.08 -- iter 33500/55000, training for: 120.54s\n",
            "Epoch 3, step (batch no.): 580 -- acc: 0.98, loss 0.08 -- iter 35000/55000, training for: 121.71s\n",
            "Epoch 3, step (batch no.): 586 -- acc: 0.98, loss 0.08 -- iter 36500/55000, training for: 122.89s\n",
            "Epoch 3, step (batch no.): 592 -- acc: 0.98, loss 0.07 -- iter 38000/55000, training for: 124.07s\n",
            "Epoch 3, step (batch no.): 598 -- acc: 0.98, loss 0.07 -- iter 39500/55000, training for: 125.24s\n",
            "Epoch 3, step (batch no.): 604 -- acc: 0.98, loss 0.08 -- iter 41000/55000, training for: 126.41s\n",
            "Epoch 3, step (batch no.): 610 -- acc: 0.98, loss 0.08 -- iter 42500/55000, training for: 127.60s\n",
            "Epoch 3, step (batch no.): 616 -- acc: 0.98, loss 0.07 -- iter 44000/55000, training for: 128.78s\n",
            "Epoch 3, step (batch no.): 622 -- acc: 0.98, loss 0.08 -- iter 45500/55000, training for: 129.95s\n",
            "Epoch 3, step (batch no.): 628 -- acc: 0.98, loss 0.09 -- iter 47000/55000, training for: 131.13s\n",
            "Epoch 3, step (batch no.): 634 -- acc: 0.89, loss 1.19 -- iter 48500/55000, training for: 132.32s\n",
            "Epoch 3, step (batch no.): 640 -- acc: 0.92, loss 0.70 -- iter 50000/55000, training for: 133.52s\n",
            "Epoch 3, step (batch no.): 646 -- acc: 0.95, loss 0.40 -- iter 51500/55000, training for: 134.72s\n",
            "Epoch 3, step (batch no.): 652 -- acc: 0.97, loss 0.25 -- iter 53000/55000, training for: 135.90s\n",
            "Epoch 3, step (batch no.): 657 -- acc: 0.97, loss 0.19 -- iter 54250/55000, training for: 136.94s\n",
            "Training Step: 660  | total loss: \u001b[1m\u001b[32m0.15758\u001b[0m\u001b[0m | time: 46.874s\n",
            "| Adam | epoch: 003 | loss: 0.15758 - acc: 0.9750 | val_loss: 0.13173 - val_acc: 0.9697 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 3, step (batch no.): 660 -- acc: 0.98, loss 0.16 -- iter 55000/55000, training for: 140.65s\n",
            "Epoch 4, step (batch no.): 665 -- acc: 0.97, loss 0.16 -- iter 01250/55000, training for: 141.65s\n",
            "Epoch 4, step (batch no.): 671 -- acc: 0.97, loss 0.13 -- iter 02750/55000, training for: 142.84s\n",
            "Epoch 4, step (batch no.): 677 -- acc: 0.97, loss 0.15 -- iter 04250/55000, training for: 144.03s\n",
            "Epoch 4, step (batch no.): 683 -- acc: 0.97, loss 0.15 -- iter 05750/55000, training for: 145.20s\n",
            "Epoch 4, step (batch no.): 689 -- acc: 0.97, loss 0.12 -- iter 07250/55000, training for: 146.39s\n",
            "Epoch 4, step (batch no.): 695 -- acc: 0.97, loss 0.12 -- iter 08750/55000, training for: 147.58s\n",
            "Epoch 4, step (batch no.): 700 -- acc: 0.97, loss 0.11 -- iter 10000/55000, training for: 148.58s\n",
            "Epoch 4, step (batch no.): 706 -- acc: 0.98, loss 0.10 -- iter 11500/55000, training for: 149.77s\n",
            "Epoch 4, step (batch no.): 712 -- acc: 0.98, loss 0.11 -- iter 13000/55000, training for: 150.96s\n",
            "Epoch 4, step (batch no.): 718 -- acc: 0.97, loss 0.11 -- iter 14500/55000, training for: 152.14s\n",
            "Epoch 4, step (batch no.): 724 -- acc: 0.97, loss 0.14 -- iter 16000/55000, training for: 153.32s\n",
            "Epoch 4, step (batch no.): 730 -- acc: 0.97, loss 0.12 -- iter 17500/55000, training for: 154.50s\n",
            "Epoch 4, step (batch no.): 736 -- acc: 0.97, loss 0.12 -- iter 19000/55000, training for: 155.67s\n",
            "Epoch 4, step (batch no.): 742 -- acc: 0.97, loss 0.14 -- iter 20500/55000, training for: 156.84s\n",
            "Epoch 4, step (batch no.): 748 -- acc: 0.97, loss 0.12 -- iter 22000/55000, training for: 158.02s\n",
            "Epoch 4, step (batch no.): 754 -- acc: 0.98, loss 0.11 -- iter 23500/55000, training for: 159.21s\n",
            "Epoch 4, step (batch no.): 760 -- acc: 0.98, loss 0.10 -- iter 25000/55000, training for: 160.39s\n",
            "Epoch 4, step (batch no.): 766 -- acc: 0.98, loss 0.09 -- iter 26500/55000, training for: 161.56s\n",
            "Epoch 4, step (batch no.): 772 -- acc: 0.98, loss 0.09 -- iter 28000/55000, training for: 162.73s\n",
            "Epoch 4, step (batch no.): 778 -- acc: 0.98, loss 0.08 -- iter 29500/55000, training for: 163.90s\n",
            "Epoch 4, step (batch no.): 784 -- acc: 0.98, loss 0.07 -- iter 31000/55000, training for: 165.08s\n",
            "Epoch 4, step (batch no.): 790 -- acc: 0.98, loss 0.07 -- iter 32500/55000, training for: 166.26s\n",
            "Epoch 4, step (batch no.): 795 -- acc: 0.98, loss 0.08 -- iter 33750/55000, training for: 167.27s\n",
            "Epoch 4, step (batch no.): 801 -- acc: 0.98, loss 0.10 -- iter 35250/55000, training for: 168.46s\n",
            "Epoch 4, step (batch no.): 806 -- acc: 0.98, loss 0.09 -- iter 36500/55000, training for: 169.46s\n",
            "Epoch 4, step (batch no.): 812 -- acc: 0.98, loss 0.08 -- iter 38000/55000, training for: 170.65s\n",
            "Epoch 4, step (batch no.): 818 -- acc: 0.98, loss 0.09 -- iter 39500/55000, training for: 171.83s\n",
            "Epoch 4, step (batch no.): 823 -- acc: 0.98, loss 0.09 -- iter 40750/55000, training for: 172.85s\n",
            "Epoch 4, step (batch no.): 828 -- acc: 0.98, loss 0.09 -- iter 42000/55000, training for: 173.89s\n",
            "Epoch 4, step (batch no.): 834 -- acc: 0.98, loss 0.08 -- iter 43500/55000, training for: 175.08s\n",
            "Epoch 4, step (batch no.): 840 -- acc: 0.98, loss 0.09 -- iter 45000/55000, training for: 176.27s\n",
            "Epoch 4, step (batch no.): 846 -- acc: 0.98, loss 0.08 -- iter 46500/55000, training for: 177.46s\n",
            "Epoch 4, step (batch no.): 852 -- acc: 0.89, loss 1.65 -- iter 48000/55000, training for: 178.64s\n",
            "Epoch 4, step (batch no.): 858 -- acc: 0.91, loss 0.97 -- iter 49500/55000, training for: 179.81s\n",
            "Epoch 4, step (batch no.): 864 -- acc: 0.94, loss 0.56 -- iter 51000/55000, training for: 180.99s\n",
            "Epoch 4, step (batch no.): 870 -- acc: 0.96, loss 0.34 -- iter 52500/55000, training for: 182.15s\n",
            "Epoch 4, step (batch no.): 876 -- acc: 0.97, loss 0.20 -- iter 54000/55000, training for: 183.34s\n",
            "Training Step: 880  | total loss: \u001b[1m\u001b[32m0.15703\u001b[0m\u001b[0m | time: 46.469s\n",
            "| Adam | epoch: 004 | loss: 0.15703 - acc: 0.9761 | val_loss: 0.11395 - val_acc: 0.9786 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 4, step (batch no.): 880 -- acc: 0.98, loss 0.16 -- iter 55000/55000, training for: 187.13s\n",
            "Epoch 5, step (batch no.): 885 -- acc: 0.98, loss 0.13 -- iter 01250/55000, training for: 188.14s\n",
            "Epoch 5, step (batch no.): 890 -- acc: 0.97, loss 0.15 -- iter 02500/55000, training for: 189.14s\n",
            "Epoch 5, step (batch no.): 896 -- acc: 0.97, loss 0.12 -- iter 04000/55000, training for: 190.33s\n",
            "Epoch 5, step (batch no.): 902 -- acc: 0.97, loss 0.13 -- iter 05500/55000, training for: 191.52s\n",
            "Epoch 5, step (batch no.): 908 -- acc: 0.97, loss 0.12 -- iter 07000/55000, training for: 192.71s\n",
            "Epoch 5, step (batch no.): 914 -- acc: 0.98, loss 0.11 -- iter 08500/55000, training for: 193.89s\n",
            "Epoch 5, step (batch no.): 920 -- acc: 0.98, loss 0.10 -- iter 10000/55000, training for: 195.08s\n",
            "Epoch 5, step (batch no.): 926 -- acc: 0.98, loss 0.11 -- iter 11500/55000, training for: 196.25s\n",
            "Epoch 5, step (batch no.): 931 -- acc: 0.98, loss 0.11 -- iter 12750/55000, training for: 197.29s\n",
            "Epoch 5, step (batch no.): 936 -- acc: 0.98, loss 0.09 -- iter 14000/55000, training for: 198.30s\n",
            "Epoch 5, step (batch no.): 942 -- acc: 0.98, loss 0.11 -- iter 15500/55000, training for: 199.50s\n",
            "Epoch 5, step (batch no.): 947 -- acc: 0.98, loss 0.10 -- iter 16750/55000, training for: 200.51s\n",
            "Epoch 5, step (batch no.): 953 -- acc: 0.98, loss 0.10 -- iter 18250/55000, training for: 201.72s\n",
            "Epoch 5, step (batch no.): 958 -- acc: 0.98, loss 0.09 -- iter 19500/55000, training for: 202.73s\n",
            "Epoch 5, step (batch no.): 964 -- acc: 0.98, loss 0.11 -- iter 21000/55000, training for: 203.92s\n",
            "Epoch 5, step (batch no.): 969 -- acc: 0.98, loss 0.11 -- iter 22250/55000, training for: 204.93s\n",
            "Epoch 5, step (batch no.): 974 -- acc: 0.98, loss 0.10 -- iter 23500/55000, training for: 205.97s\n",
            "Epoch 5, step (batch no.): 979 -- acc: 0.98, loss 0.10 -- iter 24750/55000, training for: 207.03s\n",
            "Epoch 5, step (batch no.): 985 -- acc: 0.98, loss 0.09 -- iter 26250/55000, training for: 208.20s\n",
            "Epoch 5, step (batch no.): 991 -- acc: 0.98, loss 0.09 -- iter 27750/55000, training for: 209.41s\n",
            "Epoch 5, step (batch no.): 997 -- acc: 0.98, loss 0.10 -- iter 29250/55000, training for: 210.59s\n",
            "Epoch 5, step (batch no.): 1003 -- acc: 0.98, loss 0.10 -- iter 30750/55000, training for: 211.78s\n",
            "Epoch 5, step (batch no.): 1009 -- acc: 0.97, loss 0.11 -- iter 32250/55000, training for: 212.96s\n",
            "Epoch 5, step (batch no.): 1015 -- acc: 0.98, loss 0.10 -- iter 33750/55000, training for: 214.13s\n",
            "Epoch 5, step (batch no.): 1021 -- acc: 0.97, loss 0.14 -- iter 35250/55000, training for: 215.31s\n",
            "Epoch 5, step (batch no.): 1027 -- acc: 0.97, loss 0.12 -- iter 36750/55000, training for: 216.49s\n",
            "Epoch 5, step (batch no.): 1033 -- acc: 0.98, loss 0.10 -- iter 38250/55000, training for: 217.67s\n",
            "Epoch 5, step (batch no.): 1039 -- acc: 0.98, loss 0.11 -- iter 39750/55000, training for: 218.85s\n",
            "Epoch 5, step (batch no.): 1045 -- acc: 0.98, loss 0.11 -- iter 41250/55000, training for: 220.02s\n",
            "Epoch 5, step (batch no.): 1051 -- acc: 0.98, loss 0.10 -- iter 42750/55000, training for: 221.20s\n",
            "Epoch 5, step (batch no.): 1056 -- acc: 0.98, loss 0.09 -- iter 44000/55000, training for: 222.21s\n",
            "Epoch 5, step (batch no.): 1062 -- acc: 0.98, loss 0.08 -- iter 45500/55000, training for: 223.38s\n",
            "Epoch 5, step (batch no.): 1068 -- acc: 0.98, loss 0.07 -- iter 47000/55000, training for: 224.55s\n",
            "Epoch 5, step (batch no.): 1074 -- acc: 0.98, loss 0.06 -- iter 48500/55000, training for: 225.73s\n",
            "Epoch 5, step (batch no.): 1079 -- acc: 0.98, loss 0.07 -- iter 49750/55000, training for: 226.75s\n",
            "Epoch 5, step (batch no.): 1085 -- acc: 0.98, loss 0.07 -- iter 51250/55000, training for: 227.93s\n",
            "Epoch 5, step (batch no.): 1091 -- acc: 0.98, loss 0.07 -- iter 52750/55000, training for: 229.10s\n",
            "Epoch 5, step (batch no.): 1097 -- acc: 0.98, loss 0.06 -- iter 54250/55000, training for: 230.28s\n",
            "Training Step: 1100  | total loss: \u001b[1m\u001b[32m0.06425\u001b[0m\u001b[0m | time: 46.730s\n",
            "| Adam | epoch: 005 | loss: 0.06425 - acc: 0.9805 | val_loss: 0.06621 - val_acc: 0.9844 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 5, step (batch no.): 1100 -- acc: 0.98, loss 0.06 -- iter 55000/55000, training for: 233.88s\n",
            "Epoch 6, step (batch no.): 1106 -- acc: 0.99, loss 0.05 -- iter 01500/55000, training for: 235.07s\n",
            "Epoch 6, step (batch no.): 1111 -- acc: 0.99, loss 0.05 -- iter 02750/55000, training for: 236.07s\n",
            "Epoch 6, step (batch no.): 1117 -- acc: 0.99, loss 0.05 -- iter 04250/55000, training for: 237.26s\n",
            "Epoch 6, step (batch no.): 1123 -- acc: 0.98, loss 0.07 -- iter 05750/55000, training for: 238.46s\n",
            "Epoch 6, step (batch no.): 1128 -- acc: 0.98, loss 0.07 -- iter 07000/55000, training for: 239.53s\n",
            "Epoch 6, step (batch no.): 1133 -- acc: 0.98, loss 0.07 -- iter 08250/55000, training for: 240.53s\n",
            "Epoch 6, step (batch no.): 1139 -- acc: 0.98, loss 0.07 -- iter 09750/55000, training for: 241.71s\n",
            "Epoch 6, step (batch no.): 1145 -- acc: 0.98, loss 0.06 -- iter 11250/55000, training for: 242.89s\n",
            "Epoch 6, step (batch no.): 1151 -- acc: 0.98, loss 0.07 -- iter 12750/55000, training for: 244.06s\n",
            "Epoch 6, step (batch no.): 1157 -- acc: 0.98, loss 0.07 -- iter 14250/55000, training for: 245.23s\n",
            "Epoch 6, step (batch no.): 1163 -- acc: 0.98, loss 0.07 -- iter 15750/55000, training for: 246.41s\n",
            "Epoch 6, step (batch no.): 1169 -- acc: 0.98, loss 0.07 -- iter 17250/55000, training for: 247.58s\n",
            "Epoch 6, step (batch no.): 1175 -- acc: 0.98, loss 0.08 -- iter 18750/55000, training for: 248.77s\n",
            "Epoch 6, step (batch no.): 1181 -- acc: 0.98, loss 0.08 -- iter 20250/55000, training for: 249.94s\n",
            "Epoch 6, step (batch no.): 1187 -- acc: 0.98, loss 0.09 -- iter 21750/55000, training for: 251.11s\n",
            "Epoch 6, step (batch no.): 1193 -- acc: 0.98, loss 0.10 -- iter 23250/55000, training for: 252.29s\n",
            "Epoch 6, step (batch no.): 1198 -- acc: 0.98, loss 0.10 -- iter 24500/55000, training for: 253.30s\n",
            "Epoch 6, step (batch no.): 1203 -- acc: 0.98, loss 0.09 -- iter 25750/55000, training for: 254.30s\n",
            "Epoch 6, step (batch no.): 1209 -- acc: 0.98, loss 0.08 -- iter 27250/55000, training for: 255.48s\n",
            "Epoch 6, step (batch no.): 1215 -- acc: 0.98, loss 0.09 -- iter 28750/55000, training for: 256.69s\n",
            "Epoch 6, step (batch no.): 1220 -- acc: 0.98, loss 0.11 -- iter 30000/55000, training for: 257.74s\n",
            "Epoch 6, step (batch no.): 1226 -- acc: 0.98, loss 0.09 -- iter 31500/55000, training for: 258.93s\n",
            "Epoch 6, step (batch no.): 1232 -- acc: 0.98, loss 0.09 -- iter 33000/55000, training for: 260.10s\n",
            "Epoch 6, step (batch no.): 1238 -- acc: 0.98, loss 0.08 -- iter 34500/55000, training for: 261.27s\n",
            "Epoch 6, step (batch no.): 1244 -- acc: 0.98, loss 0.06 -- iter 36000/55000, training for: 262.45s\n",
            "Epoch 6, step (batch no.): 1250 -- acc: 0.99, loss 0.06 -- iter 37500/55000, training for: 263.63s\n",
            "Epoch 6, step (batch no.): 1255 -- acc: 0.98, loss 0.05 -- iter 38750/55000, training for: 264.63s\n",
            "Epoch 6, step (batch no.): 1260 -- acc: 0.99, loss 0.06 -- iter 40000/55000, training for: 265.64s\n",
            "Epoch 6, step (batch no.): 1265 -- acc: 0.98, loss 0.07 -- iter 41250/55000, training for: 266.65s\n",
            "Epoch 6, step (batch no.): 1271 -- acc: 0.98, loss 0.07 -- iter 42750/55000, training for: 267.86s\n",
            "Epoch 6, step (batch no.): 1276 -- acc: 0.98, loss 0.06 -- iter 44000/55000, training for: 268.88s\n",
            "Epoch 6, step (batch no.): 1281 -- acc: 0.98, loss 0.06 -- iter 45250/55000, training for: 269.89s\n",
            "Epoch 6, step (batch no.): 1286 -- acc: 0.98, loss 0.06 -- iter 46500/55000, training for: 270.90s\n",
            "Epoch 6, step (batch no.): 1291 -- acc: 0.98, loss 0.07 -- iter 47750/55000, training for: 271.92s\n",
            "Epoch 6, step (batch no.): 1297 -- acc: 0.90, loss 1.39 -- iter 49250/55000, training for: 273.12s\n",
            "Epoch 6, step (batch no.): 1302 -- acc: 0.92, loss 0.90 -- iter 50500/55000, training for: 274.12s\n",
            "Epoch 6, step (batch no.): 1308 -- acc: 0.94, loss 0.52 -- iter 52000/55000, training for: 275.31s\n",
            "Epoch 6, step (batch no.): 1314 -- acc: 0.96, loss 0.33 -- iter 53500/55000, training for: 276.48s\n",
            "Training Step: 1320  | total loss: \u001b[1m\u001b[32m0.20426\u001b[0m\u001b[0m | time: 46.727s\n",
            "| Adam | epoch: 006 | loss: 0.20426 - acc: 0.9713 | val_loss: 0.09439 - val_acc: 0.9773 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 6, step (batch no.): 1320 -- acc: 0.97, loss 0.20 -- iter 55000/55000, training for: 280.63s\n",
            "Epoch 7, step (batch no.): 1326 -- acc: 0.98, loss 0.14 -- iter 01500/55000, training for: 281.81s\n",
            "Epoch 7, step (batch no.): 1332 -- acc: 0.98, loss 0.12 -- iter 03000/55000, training for: 283.00s\n",
            "Epoch 7, step (batch no.): 1338 -- acc: 0.98, loss 0.10 -- iter 04500/55000, training for: 284.17s\n",
            "Epoch 7, step (batch no.): 1344 -- acc: 0.98, loss 0.10 -- iter 06000/55000, training for: 285.35s\n",
            "Epoch 7, step (batch no.): 1350 -- acc: 0.98, loss 0.08 -- iter 07500/55000, training for: 286.53s\n",
            "Epoch 7, step (batch no.): 1355 -- acc: 0.98, loss 0.09 -- iter 08750/55000, training for: 287.56s\n",
            "Epoch 7, step (batch no.): 1360 -- acc: 0.98, loss 0.10 -- iter 10000/55000, training for: 288.56s\n",
            "Epoch 7, step (batch no.): 1366 -- acc: 0.97, loss 0.12 -- iter 11500/55000, training for: 289.74s\n",
            "Epoch 7, step (batch no.): 1372 -- acc: 0.98, loss 0.11 -- iter 13000/55000, training for: 290.91s\n",
            "Epoch 7, step (batch no.): 1378 -- acc: 0.98, loss 0.10 -- iter 14500/55000, training for: 292.08s\n",
            "Epoch 7, step (batch no.): 1384 -- acc: 0.98, loss 0.09 -- iter 16000/55000, training for: 293.26s\n",
            "Epoch 7, step (batch no.): 1390 -- acc: 0.98, loss 0.09 -- iter 17500/55000, training for: 294.45s\n",
            "Epoch 7, step (batch no.): 1396 -- acc: 0.98, loss 0.11 -- iter 19000/55000, training for: 295.63s\n",
            "Epoch 7, step (batch no.): 1402 -- acc: 0.98, loss 0.10 -- iter 20500/55000, training for: 296.80s\n",
            "Epoch 7, step (batch no.): 1408 -- acc: 0.98, loss 0.09 -- iter 22000/55000, training for: 297.97s\n",
            "Epoch 7, step (batch no.): 1413 -- acc: 0.98, loss 0.08 -- iter 23250/55000, training for: 298.97s\n",
            "Epoch 7, step (batch no.): 1418 -- acc: 0.98, loss 0.09 -- iter 24500/55000, training for: 299.98s\n",
            "Epoch 7, step (batch no.): 1424 -- acc: 0.98, loss 0.10 -- iter 26000/55000, training for: 301.17s\n",
            "Epoch 7, step (batch no.): 1430 -- acc: 0.98, loss 0.08 -- iter 27500/55000, training for: 302.36s\n",
            "Epoch 7, step (batch no.): 1435 -- acc: 0.98, loss 0.09 -- iter 28750/55000, training for: 303.36s\n",
            "Epoch 7, step (batch no.): 1441 -- acc: 0.98, loss 0.10 -- iter 30250/55000, training for: 304.53s\n",
            "Epoch 7, step (batch no.): 1446 -- acc: 0.97, loss 0.11 -- iter 31500/55000, training for: 305.60s\n",
            "Epoch 7, step (batch no.): 1451 -- acc: 0.98, loss 0.10 -- iter 32750/55000, training for: 306.60s\n",
            "Epoch 7, step (batch no.): 1457 -- acc: 0.98, loss 0.09 -- iter 34250/55000, training for: 307.78s\n",
            "Epoch 7, step (batch no.): 1462 -- acc: 0.98, loss 0.09 -- iter 35500/55000, training for: 308.78s\n",
            "Epoch 7, step (batch no.): 1468 -- acc: 0.98, loss 0.07 -- iter 37000/55000, training for: 309.96s\n",
            "Epoch 7, step (batch no.): 1474 -- acc: 0.98, loss 0.09 -- iter 38500/55000, training for: 311.13s\n",
            "Epoch 7, step (batch no.): 1480 -- acc: 0.98, loss 0.09 -- iter 40000/55000, training for: 312.29s\n",
            "Epoch 7, step (batch no.): 1486 -- acc: 0.98, loss 0.10 -- iter 41500/55000, training for: 313.48s\n",
            "Epoch 7, step (batch no.): 1492 -- acc: 0.98, loss 0.09 -- iter 43000/55000, training for: 314.66s\n",
            "Epoch 7, step (batch no.): 1498 -- acc: 0.98, loss 0.07 -- iter 44500/55000, training for: 315.83s\n",
            "Epoch 7, step (batch no.): 1503 -- acc: 0.98, loss 0.10 -- iter 45750/55000, training for: 316.86s\n",
            "Epoch 7, step (batch no.): 1508 -- acc: 0.98, loss 0.09 -- iter 47000/55000, training for: 317.87s\n",
            "Epoch 7, step (batch no.): 1514 -- acc: 0.98, loss 0.09 -- iter 48500/55000, training for: 319.06s\n",
            "Epoch 7, step (batch no.): 1520 -- acc: 0.98, loss 0.10 -- iter 50000/55000, training for: 320.24s\n",
            "Epoch 7, step (batch no.): 1526 -- acc: 0.97, loss 0.12 -- iter 51500/55000, training for: 321.42s\n",
            "Epoch 7, step (batch no.): 1532 -- acc: 0.97, loss 0.14 -- iter 53000/55000, training for: 322.60s\n",
            "Epoch 7, step (batch no.): 1538 -- acc: 0.97, loss 0.13 -- iter 54500/55000, training for: 323.79s\n",
            "Training Step: 1540  | total loss: \u001b[1m\u001b[32m0.13260\u001b[0m\u001b[0m | time: 46.505s\n",
            "| Adam | epoch: 007 | loss: 0.13260 - acc: 0.9724 | val_loss: 0.09877 - val_acc: 0.9816 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 7, step (batch no.): 1540 -- acc: 0.97, loss 0.13 -- iter 55000/55000, training for: 327.15s\n",
            "Epoch 8, step (batch no.): 1546 -- acc: 0.98, loss 0.10 -- iter 01500/55000, training for: 328.34s\n",
            "Epoch 8, step (batch no.): 1552 -- acc: 0.98, loss 0.08 -- iter 03000/55000, training for: 329.52s\n",
            "Epoch 8, step (batch no.): 1557 -- acc: 0.98, loss 0.08 -- iter 04250/55000, training for: 330.52s\n",
            "Epoch 8, step (batch no.): 1563 -- acc: 0.98, loss 0.10 -- iter 05750/55000, training for: 331.72s\n",
            "Epoch 8, step (batch no.): 1568 -- acc: 0.98, loss 0.11 -- iter 07000/55000, training for: 332.73s\n",
            "Epoch 8, step (batch no.): 1574 -- acc: 0.98, loss 0.12 -- iter 08500/55000, training for: 333.93s\n",
            "Epoch 8, step (batch no.): 1579 -- acc: 0.98, loss 0.10 -- iter 09750/55000, training for: 334.96s\n",
            "Epoch 8, step (batch no.): 1585 -- acc: 0.98, loss 0.10 -- iter 11250/55000, training for: 336.14s\n",
            "Epoch 8, step (batch no.): 1590 -- acc: 0.97, loss 0.13 -- iter 12500/55000, training for: 337.14s\n",
            "Epoch 8, step (batch no.): 1595 -- acc: 0.98, loss 0.12 -- iter 13750/55000, training for: 338.18s\n",
            "Epoch 8, step (batch no.): 1600 -- acc: 0.98, loss 0.13 -- iter 15000/55000, training for: 339.18s\n",
            "Epoch 8, step (batch no.): 1606 -- acc: 0.97, loss 0.13 -- iter 16500/55000, training for: 340.37s\n",
            "Epoch 8, step (batch no.): 1612 -- acc: 0.98, loss 0.12 -- iter 18000/55000, training for: 341.54s\n",
            "Epoch 8, step (batch no.): 1618 -- acc: 0.97, loss 0.12 -- iter 19500/55000, training for: 342.72s\n",
            "Epoch 8, step (batch no.): 1624 -- acc: 0.98, loss 0.10 -- iter 21000/55000, training for: 343.91s\n",
            "Epoch 8, step (batch no.): 1630 -- acc: 0.98, loss 0.10 -- iter 22500/55000, training for: 345.09s\n",
            "Epoch 8, step (batch no.): 1635 -- acc: 0.98, loss 0.09 -- iter 23750/55000, training for: 346.09s\n",
            "Epoch 8, step (batch no.): 1641 -- acc: 0.98, loss 0.08 -- iter 25250/55000, training for: 347.28s\n",
            "Epoch 8, step (batch no.): 1646 -- acc: 0.98, loss 0.07 -- iter 26500/55000, training for: 348.30s\n",
            "Epoch 8, step (batch no.): 1652 -- acc: 0.98, loss 0.06 -- iter 28000/55000, training for: 349.48s\n",
            "Epoch 8, step (batch no.): 1658 -- acc: 0.98, loss 0.06 -- iter 29500/55000, training for: 350.67s\n",
            "Epoch 8, step (batch no.): 1663 -- acc: 0.98, loss 0.07 -- iter 30750/55000, training for: 351.67s\n",
            "Epoch 8, step (batch no.): 1669 -- acc: 0.98, loss 0.10 -- iter 32250/55000, training for: 352.86s\n",
            "Epoch 8, step (batch no.): 1675 -- acc: 0.98, loss 0.10 -- iter 33750/55000, training for: 354.05s\n",
            "Epoch 8, step (batch no.): 1680 -- acc: 0.98, loss 0.10 -- iter 35000/55000, training for: 355.06s\n",
            "Epoch 8, step (batch no.): 1686 -- acc: 0.98, loss 0.09 -- iter 36500/55000, training for: 356.24s\n",
            "Epoch 8, step (batch no.): 1692 -- acc: 0.98, loss 0.09 -- iter 38000/55000, training for: 357.42s\n",
            "Epoch 8, step (batch no.): 1698 -- acc: 0.98, loss 0.09 -- iter 39500/55000, training for: 358.60s\n",
            "Epoch 8, step (batch no.): 1704 -- acc: 0.98, loss 0.08 -- iter 41000/55000, training for: 359.77s\n",
            "Epoch 8, step (batch no.): 1710 -- acc: 0.98, loss 0.07 -- iter 42500/55000, training for: 360.96s\n",
            "Epoch 8, step (batch no.): 1716 -- acc: 0.98, loss 0.09 -- iter 44000/55000, training for: 362.13s\n",
            "Epoch 8, step (batch no.): 1722 -- acc: 0.98, loss 0.09 -- iter 45500/55000, training for: 363.32s\n",
            "Epoch 8, step (batch no.): 1728 -- acc: 0.98, loss 0.08 -- iter 47000/55000, training for: 364.50s\n",
            "Epoch 8, step (batch no.): 1733 -- acc: 0.98, loss 0.08 -- iter 48250/55000, training for: 365.53s\n",
            "Epoch 8, step (batch no.): 1738 -- acc: 0.98, loss 0.08 -- iter 49500/55000, training for: 366.54s\n",
            "Epoch 8, step (batch no.): 1744 -- acc: 0.98, loss 0.08 -- iter 51000/55000, training for: 367.72s\n",
            "Epoch 8, step (batch no.): 1749 -- acc: 0.98, loss 0.06 -- iter 52250/55000, training for: 368.74s\n",
            "Epoch 8, step (batch no.): 1755 -- acc: 0.98, loss 0.08 -- iter 53750/55000, training for: 369.93s\n",
            "Training Step: 1760  | total loss: \u001b[1m\u001b[32m0.06332\u001b[0m\u001b[0m | time: 46.798s\n",
            "| Adam | epoch: 008 | loss: 0.06332 - acc: 0.9845 | val_loss: 0.09521 - val_acc: 0.9783 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 8, step (batch no.): 1760 -- acc: 0.98, loss 0.06 -- iter 55000/55000, training for: 373.96s\n",
            "Epoch 9, step (batch no.): 1765 -- acc: 0.98, loss 0.07 -- iter 01250/55000, training for: 374.99s\n",
            "Epoch 9, step (batch no.): 1771 -- acc: 0.99, loss 0.07 -- iter 02750/55000, training for: 376.17s\n",
            "Epoch 9, step (batch no.): 1776 -- acc: 0.99, loss 0.06 -- iter 04000/55000, training for: 377.20s\n",
            "Epoch 9, step (batch no.): 1781 -- acc: 0.98, loss 0.08 -- iter 05250/55000, training for: 378.21s\n",
            "Epoch 9, step (batch no.): 1787 -- acc: 0.98, loss 0.08 -- iter 06750/55000, training for: 379.39s\n",
            "Epoch 9, step (batch no.): 1793 -- acc: 0.98, loss 0.09 -- iter 08250/55000, training for: 380.58s\n",
            "Epoch 9, step (batch no.): 1799 -- acc: 0.98, loss 0.09 -- iter 09750/55000, training for: 381.75s\n",
            "Epoch 9, step (batch no.): 1805 -- acc: 0.98, loss 0.10 -- iter 11250/55000, training for: 382.92s\n",
            "Epoch 9, step (batch no.): 1811 -- acc: 0.98, loss 0.10 -- iter 12750/55000, training for: 384.10s\n",
            "Epoch 9, step (batch no.): 1817 -- acc: 0.98, loss 0.10 -- iter 14250/55000, training for: 385.29s\n",
            "Epoch 9, step (batch no.): 1823 -- acc: 0.98, loss 0.10 -- iter 15750/55000, training for: 386.48s\n",
            "Epoch 9, step (batch no.): 1829 -- acc: 0.98, loss 0.09 -- iter 17250/55000, training for: 387.67s\n",
            "Epoch 9, step (batch no.): 1835 -- acc: 0.98, loss 0.09 -- iter 18750/55000, training for: 388.85s\n",
            "Epoch 9, step (batch no.): 1840 -- acc: 0.98, loss 0.08 -- iter 20000/55000, training for: 389.85s\n",
            "Epoch 9, step (batch no.): 1846 -- acc: 0.98, loss 0.08 -- iter 21500/55000, training for: 391.03s\n",
            "Epoch 9, step (batch no.): 1852 -- acc: 0.98, loss 0.07 -- iter 23000/55000, training for: 392.21s\n",
            "Epoch 9, step (batch no.): 1858 -- acc: 0.98, loss 0.07 -- iter 24500/55000, training for: 393.39s\n",
            "Epoch 9, step (batch no.): 1864 -- acc: 0.98, loss 0.07 -- iter 26000/55000, training for: 394.58s\n",
            "Epoch 9, step (batch no.): 1870 -- acc: 0.98, loss 0.06 -- iter 27500/55000, training for: 395.78s\n",
            "Epoch 9, step (batch no.): 1876 -- acc: 0.98, loss 0.07 -- iter 29000/55000, training for: 396.94s\n",
            "Epoch 9, step (batch no.): 1882 -- acc: 0.98, loss 0.08 -- iter 30500/55000, training for: 398.13s\n",
            "Epoch 9, step (batch no.): 1888 -- acc: 0.98, loss 0.08 -- iter 32000/55000, training for: 399.31s\n",
            "Epoch 9, step (batch no.): 1894 -- acc: 0.98, loss 0.08 -- iter 33500/55000, training for: 400.49s\n",
            "Epoch 9, step (batch no.): 1900 -- acc: 0.98, loss 0.10 -- iter 35000/55000, training for: 401.67s\n",
            "Epoch 9, step (batch no.): 1906 -- acc: 0.98, loss 0.09 -- iter 36500/55000, training for: 402.85s\n",
            "Epoch 9, step (batch no.): 1911 -- acc: 0.98, loss 0.07 -- iter 37750/55000, training for: 403.91s\n",
            "Epoch 9, step (batch no.): 1916 -- acc: 0.98, loss 0.08 -- iter 39000/55000, training for: 404.96s\n",
            "Epoch 9, step (batch no.): 1922 -- acc: 0.98, loss 0.08 -- iter 40500/55000, training for: 406.15s\n",
            "Epoch 9, step (batch no.): 1927 -- acc: 0.98, loss 0.07 -- iter 41750/55000, training for: 407.19s\n",
            "Epoch 9, step (batch no.): 1932 -- acc: 0.99, loss 0.07 -- iter 43000/55000, training for: 408.24s\n",
            "Epoch 9, step (batch no.): 1938 -- acc: 0.99, loss 0.06 -- iter 44500/55000, training for: 409.43s\n",
            "Epoch 9, step (batch no.): 1944 -- acc: 0.98, loss 0.07 -- iter 46000/55000, training for: 410.61s\n",
            "Epoch 9, step (batch no.): 1950 -- acc: 0.98, loss 0.08 -- iter 47500/55000, training for: 411.79s\n",
            "Epoch 9, step (batch no.): 1956 -- acc: 0.98, loss 0.08 -- iter 49000/55000, training for: 412.97s\n",
            "Epoch 9, step (batch no.): 1962 -- acc: 0.99, loss 0.07 -- iter 50500/55000, training for: 414.16s\n",
            "Epoch 9, step (batch no.): 1967 -- acc: 0.99, loss 0.05 -- iter 51750/55000, training for: 415.17s\n",
            "Epoch 9, step (batch no.): 1972 -- acc: 0.99, loss 0.05 -- iter 53000/55000, training for: 416.25s\n",
            "Epoch 9, step (batch no.): 1977 -- acc: 0.99, loss 0.04 -- iter 54250/55000, training for: 417.26s\n",
            "Training Step: 1980  | total loss: \u001b[1m\u001b[32m0.04878\u001b[0m\u001b[0m | time: 46.855s\n",
            "| Adam | epoch: 009 | loss: 0.04878 - acc: 0.9871 | val_loss: 0.07815 - val_acc: 0.9843 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 9, step (batch no.): 1980 -- acc: 0.99, loss 0.05 -- iter 55000/55000, training for: 420.84s\n",
            "Epoch 10, step (batch no.): 1985 -- acc: 0.99, loss 0.06 -- iter 01250/55000, training for: 421.86s\n",
            "Epoch 10, step (batch no.): 1990 -- acc: 0.99, loss 0.05 -- iter 02500/55000, training for: 422.87s\n",
            "Epoch 10, step (batch no.): 1996 -- acc: 0.98, loss 0.08 -- iter 04000/55000, training for: 424.06s\n",
            "Epoch 10, step (batch no.): 2002 -- acc: 0.99, loss 0.07 -- iter 05500/55000, training for: 425.23s\n",
            "Epoch 10, step (batch no.): 2007 -- acc: 0.99, loss 0.07 -- iter 06750/55000, training for: 426.23s\n",
            "Epoch 10, step (batch no.): 2013 -- acc: 0.99, loss 0.06 -- iter 08250/55000, training for: 427.42s\n",
            "Epoch 10, step (batch no.): 2019 -- acc: 0.99, loss 0.06 -- iter 09750/55000, training for: 428.60s\n",
            "Epoch 10, step (batch no.): 2025 -- acc: 0.99, loss 0.06 -- iter 11250/55000, training for: 429.79s\n",
            "Epoch 10, step (batch no.): 2031 -- acc: 0.98, loss 0.06 -- iter 12750/55000, training for: 430.98s\n",
            "Epoch 10, step (batch no.): 2036 -- acc: 0.98, loss 0.07 -- iter 14000/55000, training for: 431.98s\n",
            "Epoch 10, step (batch no.): 2042 -- acc: 0.99, loss 0.07 -- iter 15500/55000, training for: 433.17s\n",
            "Epoch 10, step (batch no.): 2047 -- acc: 0.98, loss 0.07 -- iter 16750/55000, training for: 434.17s\n",
            "Epoch 10, step (batch no.): 2053 -- acc: 0.98, loss 0.06 -- iter 18250/55000, training for: 435.37s\n",
            "Epoch 10, step (batch no.): 2058 -- acc: 0.98, loss 0.07 -- iter 19500/55000, training for: 436.40s\n",
            "Epoch 10, step (batch no.): 2063 -- acc: 0.98, loss 0.06 -- iter 20750/55000, training for: 437.49s\n",
            "Epoch 10, step (batch no.): 2068 -- acc: 0.98, loss 0.08 -- iter 22000/55000, training for: 438.49s\n",
            "Epoch 10, step (batch no.): 2074 -- acc: 0.99, loss 0.09 -- iter 23500/55000, training for: 439.67s\n",
            "Epoch 10, step (batch no.): 2080 -- acc: 0.98, loss 0.10 -- iter 25000/55000, training for: 440.85s\n",
            "Epoch 10, step (batch no.): 2086 -- acc: 0.98, loss 0.10 -- iter 26500/55000, training for: 442.03s\n",
            "Epoch 10, step (batch no.): 2092 -- acc: 0.98, loss 0.10 -- iter 28000/55000, training for: 443.20s\n",
            "Epoch 10, step (batch no.): 2098 -- acc: 0.98, loss 0.10 -- iter 29500/55000, training for: 444.37s\n",
            "Epoch 10, step (batch no.): 2104 -- acc: 0.98, loss 0.10 -- iter 31000/55000, training for: 445.55s\n",
            "Epoch 10, step (batch no.): 2110 -- acc: 0.98, loss 0.10 -- iter 32500/55000, training for: 446.73s\n",
            "Epoch 10, step (batch no.): 2116 -- acc: 0.98, loss 0.09 -- iter 34000/55000, training for: 447.91s\n",
            "Epoch 10, step (batch no.): 2122 -- acc: 0.98, loss 0.09 -- iter 35500/55000, training for: 449.09s\n",
            "Epoch 10, step (batch no.): 2128 -- acc: 0.97, loss 0.11 -- iter 37000/55000, training for: 450.28s\n",
            "Epoch 10, step (batch no.): 2134 -- acc: 0.97, loss 0.12 -- iter 38500/55000, training for: 451.47s\n",
            "Epoch 10, step (batch no.): 2140 -- acc: 0.97, loss 0.13 -- iter 40000/55000, training for: 452.66s\n",
            "Epoch 10, step (batch no.): 2146 -- acc: 0.97, loss 0.14 -- iter 41500/55000, training for: 453.84s\n",
            "Epoch 10, step (batch no.): 2152 -- acc: 0.98, loss 0.12 -- iter 43000/55000, training for: 455.02s\n",
            "Epoch 10, step (batch no.): 2158 -- acc: 0.97, loss 0.14 -- iter 44500/55000, training for: 456.20s\n",
            "Epoch 10, step (batch no.): 2164 -- acc: 0.97, loss 0.15 -- iter 46000/55000, training for: 457.38s\n",
            "Epoch 10, step (batch no.): 2170 -- acc: 0.97, loss 0.13 -- iter 47500/55000, training for: 458.57s\n",
            "Epoch 10, step (batch no.): 2176 -- acc: 0.98, loss 0.11 -- iter 49000/55000, training for: 459.74s\n",
            "Epoch 10, step (batch no.): 2182 -- acc: 0.98, loss 0.10 -- iter 50500/55000, training for: 460.90s\n",
            "Epoch 10, step (batch no.): 2188 -- acc: 0.98, loss 0.10 -- iter 52000/55000, training for: 462.08s\n",
            "Epoch 10, step (batch no.): 2194 -- acc: 0.97, loss 0.12 -- iter 53500/55000, training for: 463.25s\n",
            "Training Step: 2200  | total loss: \u001b[1m\u001b[32m0.13315\u001b[0m\u001b[0m | time: 46.540s\n",
            "| Adam | epoch: 010 | loss: 0.13315 - acc: 0.9747 | val_loss: 0.10706 - val_acc: 0.9775 -- iter: 55000/55000\n",
            "--\n",
            "Epoch 10, step (batch no.): 2200 -- acc: 0.97, loss 0.13 -- iter 55000/55000, training for: 467.40s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OX8igExWXYD"
      },
      "source": [
        "# Zadanie C: overfitting\n",
        "Narysuj wykres zależności accuracy dla danych **treningowych** (acc) i **walidacyjnych** (val_acc) od liczby epok (50 epok), dla sieci która ma tylko jedną ukrytą pełną warstwę z 1000 neuronów z aktywacją `relu` i warstwę wyjściową z 10 neuronami. \n",
        "\n",
        "Obliczenia wykonaj używając tylko 1000 pierwszych próbek z zestawu treningowego, jak pokazano poniżej:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQzvTR9Gm4Dx"
      },
      "source": [
        "\n",
        "# !!! UWAGA - przy każdych nowych obliczeniach należy zresetować graf sieci\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Wyłączamy warningi z tensorflow\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "\n",
        "# Warstwa wejściowa - musi mieć takie same wymiary jak dane\n",
        "network = input_data(shape=[None, 28, 28, 1], name='input') # None oznacza, że ta \n",
        "\n",
        "network = fully_connected(network, 1000, activation='relu') # 256 neuronów, aktywacja \"relu\"\n",
        "\n",
        "network = fully_connected(network, 10, activation='softmax')\n",
        "\n",
        "# tu definiujemy w jaki sposób optymalizować sieć (regression nie oznacza, że robimy regresję)\n",
        "# nie będziemy modyfikować tych argumentów; stosujemy optymizator Adam\n",
        "network = regression(network, optimizer='adam', learning_rate=0.01,\n",
        "                     batch_size=250,\n",
        "                     loss='categorical_crossentropy', name='target')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTIy2YLtWXYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86bb6f4-28fd-47cf-c701-2c24fb50a21f"
      },
      "source": [
        "scores = Stats(1000)\n",
        "model = tflearn.DNN(network, tensorboard_verbose=0)\n",
        "model.fit({'input': X[:1000]}, {'target': Y[:1000]}, n_epoch=50,\n",
        "           validation_set=({'input': testX}, {'target': testY}), show_metric=True, callbacks=[scores])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: 9IWNXH\n",
            "Log directory: /tmp/tflearn_logs/\n",
            "---------------------------------\n",
            "Training samples: 1000\n",
            "Validation samples: 10000\n",
            "--\n",
            "Epoch 1, step (batch no.): 1 -- acc: 0.00, loss 0.00 -- iter 00250/01000, training for: 0.09s\n",
            "Training Step: 4  | total loss: \u001b[1m\u001b[32m2.36133\u001b[0m\u001b[0m | time: 1.174s\n",
            "| Adam | epoch: 001 | loss: 2.36133 - acc: 0.4308 | val_loss: 1.21002 - val_acc: 0.6817 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 1, step (batch no.): 4 -- acc: 0.43, loss 2.36 -- iter 01000/01000, training for: 1.18s\n",
            "Training Step: 8  | total loss: \u001b[1m\u001b[32m1.66798\u001b[0m\u001b[0m | time: 1.080s\n",
            "| Adam | epoch: 002 | loss: 1.66798 - acc: 0.6131 | val_loss: 0.68584 - val_acc: 0.7743 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 2, step (batch no.): 8 -- acc: 0.61, loss 1.67 -- iter 01000/01000, training for: 2.26s\n",
            "Training Step: 12  | total loss: \u001b[1m\u001b[32m0.64558\u001b[0m\u001b[0m | time: 1.089s\n",
            "| Adam | epoch: 003 | loss: 0.64558 - acc: 0.8033 | val_loss: 0.63980 - val_acc: 0.7779 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 3, step (batch no.): 12 -- acc: 0.80, loss 0.65 -- iter 01000/01000, training for: 3.35s\n",
            "Training Step: 16  | total loss: \u001b[1m\u001b[32m0.70277\u001b[0m\u001b[0m | time: 1.091s\n",
            "| Adam | epoch: 004 | loss: 0.70277 - acc: 0.8430 | val_loss: 0.53965 - val_acc: 0.8429 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 4, step (batch no.): 16 -- acc: 0.84, loss 0.70 -- iter 01000/01000, training for: 4.44s\n",
            "Training Step: 20  | total loss: \u001b[1m\u001b[32m0.63485\u001b[0m\u001b[0m | time: 1.083s\n",
            "| Adam | epoch: 005 | loss: 0.63485 - acc: 0.8729 | val_loss: 0.49646 - val_acc: 0.8563 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 5, step (batch no.): 20 -- acc: 0.87, loss 0.63 -- iter 01000/01000, training for: 5.53s\n",
            "Training Step: 24  | total loss: \u001b[1m\u001b[32m0.63371\u001b[0m\u001b[0m | time: 1.103s\n",
            "| Adam | epoch: 006 | loss: 0.63371 - acc: 0.8799 | val_loss: 0.47112 - val_acc: 0.8658 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 6, step (batch no.): 24 -- acc: 0.88, loss 0.63 -- iter 01000/01000, training for: 6.63s\n",
            "Training Step: 28  | total loss: \u001b[1m\u001b[32m0.73195\u001b[0m\u001b[0m | time: 1.085s\n",
            "| Adam | epoch: 007 | loss: 0.73195 - acc: 0.8751 | val_loss: 0.38652 - val_acc: 0.8946 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 7, step (batch no.): 28 -- acc: 0.88, loss 0.73 -- iter 01000/01000, training for: 7.72s\n",
            "Training Step: 32  | total loss: \u001b[1m\u001b[32m0.34820\u001b[0m\u001b[0m | time: 1.093s\n",
            "| Adam | epoch: 008 | loss: 0.34820 - acc: 0.9438 | val_loss: 0.37722 - val_acc: 0.8990 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 8, step (batch no.): 32 -- acc: 0.94, loss 0.35 -- iter 01000/01000, training for: 8.82s\n",
            "Training Step: 36  | total loss: \u001b[1m\u001b[32m0.37315\u001b[0m\u001b[0m | time: 1.094s\n",
            "| Adam | epoch: 009 | loss: 0.37315 - acc: 0.9441 | val_loss: 0.36133 - val_acc: 0.9017 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 9, step (batch no.): 36 -- acc: 0.94, loss 0.37 -- iter 01000/01000, training for: 9.91s\n",
            "Training Step: 40  | total loss: \u001b[1m\u001b[32m0.20881\u001b[0m\u001b[0m | time: 1.088s\n",
            "| Adam | epoch: 010 | loss: 0.20881 - acc: 0.9733 | val_loss: 0.36200 - val_acc: 0.8906 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 10, step (batch no.): 40 -- acc: 0.97, loss 0.21 -- iter 01000/01000, training for: 11.00s\n",
            "Training Step: 44  | total loss: \u001b[1m\u001b[32m0.12224\u001b[0m\u001b[0m | time: 1.100s\n",
            "| Adam | epoch: 011 | loss: 0.12224 - acc: 0.9839 | val_loss: 0.33766 - val_acc: 0.8994 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 11, step (batch no.): 44 -- acc: 0.98, loss 0.12 -- iter 01000/01000, training for: 12.10s\n",
            "Training Step: 48  | total loss: \u001b[1m\u001b[32m0.07365\u001b[0m\u001b[0m | time: 1.088s\n",
            "| Adam | epoch: 012 | loss: 0.07365 - acc: 0.9911 | val_loss: 0.34265 - val_acc: 0.8994 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 12, step (batch no.): 48 -- acc: 0.99, loss 0.07 -- iter 01000/01000, training for: 13.19s\n",
            "Training Step: 52  | total loss: \u001b[1m\u001b[32m0.04575\u001b[0m\u001b[0m | time: 1.094s\n",
            "| Adam | epoch: 013 | loss: 0.04575 - acc: 0.9954 | val_loss: 0.34144 - val_acc: 0.9047 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 13, step (batch no.): 52 -- acc: 1.00, loss 0.05 -- iter 01000/01000, training for: 14.28s\n",
            "Training Step: 56  | total loss: \u001b[1m\u001b[32m0.02973\u001b[0m\u001b[0m | time: 1.090s\n",
            "| Adam | epoch: 014 | loss: 0.02973 - acc: 0.9972 | val_loss: 0.35385 - val_acc: 0.9061 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 14, step (batch no.): 56 -- acc: 1.00, loss 0.03 -- iter 01000/01000, training for: 15.38s\n",
            "Training Step: 60  | total loss: \u001b[1m\u001b[32m0.01962\u001b[0m\u001b[0m | time: 1.092s\n",
            "| Adam | epoch: 015 | loss: 0.01962 - acc: 0.9984 | val_loss: 0.36480 - val_acc: 0.9051 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 15, step (batch no.): 60 -- acc: 1.00, loss 0.02 -- iter 01000/01000, training for: 16.47s\n",
            "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.01367\u001b[0m\u001b[0m | time: 1.086s\n",
            "| Adam | epoch: 016 | loss: 0.01367 - acc: 0.9986 | val_loss: 0.36786 - val_acc: 0.9055 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 16, step (batch no.): 64 -- acc: 1.00, loss 0.01 -- iter 01000/01000, training for: 17.56s\n",
            "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.00930\u001b[0m\u001b[0m | time: 1.087s\n",
            "| Adam | epoch: 017 | loss: 0.00930 - acc: 0.9992 | val_loss: 0.37573 - val_acc: 0.9053 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 17, step (batch no.): 68 -- acc: 1.00, loss 0.01 -- iter 01000/01000, training for: 18.65s\n",
            "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.00658\u001b[0m\u001b[0m | time: 1.094s\n",
            "| Adam | epoch: 018 | loss: 0.00658 - acc: 0.9995 | val_loss: 0.38558 - val_acc: 0.8959 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 18, step (batch no.): 72 -- acc: 1.00, loss 0.01 -- iter 01000/01000, training for: 19.74s\n",
            "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.29712\u001b[0m\u001b[0m | time: 1.094s\n",
            "| Adam | epoch: 019 | loss: 0.29712 - acc: 0.9787 | val_loss: 0.45610 - val_acc: 0.8587 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 19, step (batch no.): 76 -- acc: 0.98, loss 0.30 -- iter 01000/01000, training for: 20.84s\n",
            "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.20901\u001b[0m\u001b[0m | time: 1.090s\n",
            "| Adam | epoch: 020 | loss: 0.20901 - acc: 0.9851 | val_loss: 0.40997 - val_acc: 0.8788 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 20, step (batch no.): 80 -- acc: 0.99, loss 0.21 -- iter 01000/01000, training for: 21.93s\n",
            "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.14953\u001b[0m\u001b[0m | time: 1.100s\n",
            "| Adam | epoch: 021 | loss: 0.14953 - acc: 0.9902 | val_loss: 0.38407 - val_acc: 0.8848 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 21, step (batch no.): 84 -- acc: 0.99, loss 0.15 -- iter 01000/01000, training for: 23.03s\n",
            "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.34273\u001b[0m\u001b[0m | time: 1.095s\n",
            "| Adam | epoch: 022 | loss: 0.34273 - acc: 0.9684 | val_loss: 0.42208 - val_acc: 0.8754 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 22, step (batch no.): 88 -- acc: 0.97, loss 0.34 -- iter 01000/01000, training for: 24.13s\n",
            "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.25162\u001b[0m\u001b[0m | time: 1.081s\n",
            "| Adam | epoch: 023 | loss: 0.25162 - acc: 0.9786 | val_loss: 0.44921 - val_acc: 0.8851 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 23, step (batch no.): 92 -- acc: 0.98, loss 0.25 -- iter 01000/01000, training for: 25.21s\n",
            "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.37748\u001b[0m\u001b[0m | time: 1.087s\n",
            "| Adam | epoch: 024 | loss: 0.37748 - acc: 0.9618 | val_loss: 0.47833 - val_acc: 0.8832 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 24, step (batch no.): 96 -- acc: 0.96, loss 0.38 -- iter 01000/01000, training for: 26.30s\n",
            "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.43988\u001b[0m\u001b[0m | time: 1.089s\n",
            "| Adam | epoch: 025 | loss: 0.43988 - acc: 0.9480 | val_loss: 0.42238 - val_acc: 0.8969 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 25, step (batch no.): 100 -- acc: 0.95, loss 0.44 -- iter 01000/01000, training for: 27.39s\n",
            "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.39940\u001b[0m\u001b[0m | time: 1.087s\n",
            "| Adam | epoch: 026 | loss: 0.39940 - acc: 0.9515 | val_loss: 0.38353 - val_acc: 0.8915 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 26, step (batch no.): 104 -- acc: 0.95, loss 0.40 -- iter 01000/01000, training for: 28.47s\n",
            "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.28032\u001b[0m\u001b[0m | time: 1.093s\n",
            "| Adam | epoch: 027 | loss: 0.28032 - acc: 0.9679 | val_loss: 0.35222 - val_acc: 0.8966 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 27, step (batch no.): 108 -- acc: 0.97, loss 0.28 -- iter 01000/01000, training for: 29.57s\n",
            "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.19202\u001b[0m\u001b[0m | time: 1.092s\n",
            "| Adam | epoch: 028 | loss: 0.19202 - acc: 0.9789 | val_loss: 0.36679 - val_acc: 0.8929 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 28, step (batch no.): 112 -- acc: 0.98, loss 0.19 -- iter 01000/01000, training for: 30.66s\n",
            "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.51564\u001b[0m\u001b[0m | time: 1.084s\n",
            "| Adam | epoch: 029 | loss: 0.51564 - acc: 0.9478 | val_loss: 0.47057 - val_acc: 0.8865 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 29, step (batch no.): 116 -- acc: 0.95, loss 0.52 -- iter 01000/01000, training for: 31.75s\n",
            "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.58331\u001b[0m\u001b[0m | time: 1.091s\n",
            "| Adam | epoch: 030 | loss: 0.58331 - acc: 0.9317 | val_loss: 0.56558 - val_acc: 0.8585 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 30, step (batch no.): 120 -- acc: 0.93, loss 0.58 -- iter 01000/01000, training for: 32.84s\n",
            "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.84155\u001b[0m\u001b[0m | time: 1.093s\n",
            "| Adam | epoch: 031 | loss: 0.84155 - acc: 0.8751 | val_loss: 0.66332 - val_acc: 0.8835 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 31, step (batch no.): 124 -- acc: 0.88, loss 0.84 -- iter 01000/01000, training for: 33.93s\n",
            "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.79711\u001b[0m\u001b[0m | time: 1.091s\n",
            "| Adam | epoch: 032 | loss: 0.79711 - acc: 0.8845 | val_loss: 0.54015 - val_acc: 0.8692 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 32, step (batch no.): 128 -- acc: 0.88, loss 0.80 -- iter 01000/01000, training for: 35.03s\n",
            "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.58393\u001b[0m\u001b[0m | time: 1.098s\n",
            "| Adam | epoch: 033 | loss: 0.58393 - acc: 0.9236 | val_loss: 0.42390 - val_acc: 0.8878 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 33, step (batch no.): 132 -- acc: 0.92, loss 0.58 -- iter 01000/01000, training for: 36.13s\n",
            "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.72808\u001b[0m\u001b[0m | time: 1.096s\n",
            "| Adam | epoch: 034 | loss: 0.72808 - acc: 0.8998 | val_loss: 0.49256 - val_acc: 0.8819 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 34, step (batch no.): 136 -- acc: 0.90, loss 0.73 -- iter 01000/01000, training for: 37.22s\n",
            "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.67049\u001b[0m\u001b[0m | time: 1.098s\n",
            "| Adam | epoch: 035 | loss: 0.67049 - acc: 0.9050 | val_loss: 0.48933 - val_acc: 0.8804 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 35, step (batch no.): 140 -- acc: 0.90, loss 0.67 -- iter 01000/01000, training for: 38.33s\n",
            "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.48791\u001b[0m\u001b[0m | time: 1.095s\n",
            "| Adam | epoch: 036 | loss: 0.48791 - acc: 0.9367 | val_loss: 0.35604 - val_acc: 0.8985 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 36, step (batch no.): 144 -- acc: 0.94, loss 0.49 -- iter 01000/01000, training for: 39.42s\n",
            "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.66158\u001b[0m\u001b[0m | time: 1.094s\n",
            "| Adam | epoch: 037 | loss: 0.66158 - acc: 0.9177 | val_loss: 0.39886 - val_acc: 0.8803 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 37, step (batch no.): 148 -- acc: 0.92, loss 0.66 -- iter 01000/01000, training for: 40.52s\n",
            "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.45469\u001b[0m\u001b[0m | time: 1.102s\n",
            "| Adam | epoch: 038 | loss: 0.45469 - acc: 0.9460 | val_loss: 0.40162 - val_acc: 0.8877 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 38, step (batch no.): 152 -- acc: 0.95, loss 0.45 -- iter 01000/01000, training for: 41.62s\n",
            "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.31747\u001b[0m\u001b[0m | time: 1.104s\n",
            "| Adam | epoch: 039 | loss: 0.31747 - acc: 0.9646 | val_loss: 0.39415 - val_acc: 0.8905 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 39, step (batch no.): 156 -- acc: 0.96, loss 0.32 -- iter 01000/01000, training for: 42.73s\n",
            "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.39727\u001b[0m\u001b[0m | time: 1.096s\n",
            "| Adam | epoch: 040 | loss: 0.39727 - acc: 0.9511 | val_loss: 0.44106 - val_acc: 0.8809 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 40, step (batch no.): 160 -- acc: 0.95, loss 0.40 -- iter 01000/01000, training for: 43.82s\n",
            "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.42003\u001b[0m\u001b[0m | time: 1.093s\n",
            "| Adam | epoch: 041 | loss: 0.42003 - acc: 0.9464 | val_loss: 0.45760 - val_acc: 0.8848 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 41, step (batch no.): 164 -- acc: 0.95, loss 0.42 -- iter 01000/01000, training for: 44.92s\n",
            "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.62947\u001b[0m\u001b[0m | time: 1.098s\n",
            "| Adam | epoch: 042 | loss: 0.62947 - acc: 0.9066 | val_loss: 0.58998 - val_acc: 0.8562 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 42, step (batch no.): 168 -- acc: 0.91, loss 0.63 -- iter 01000/01000, training for: 46.02s\n",
            "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.47821\u001b[0m\u001b[0m | time: 1.104s\n",
            "| Adam | epoch: 043 | loss: 0.47821 - acc: 0.9372 | val_loss: 0.51171 - val_acc: 0.8831 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 43, step (batch no.): 172 -- acc: 0.94, loss 0.48 -- iter 01000/01000, training for: 47.12s\n",
            "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.36057\u001b[0m\u001b[0m | time: 1.095s\n",
            "| Adam | epoch: 044 | loss: 0.36057 - acc: 0.9585 | val_loss: 0.38574 - val_acc: 0.8883 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 44, step (batch no.): 176 -- acc: 0.96, loss 0.36 -- iter 01000/01000, training for: 48.22s\n",
            "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.24968\u001b[0m\u001b[0m | time: 1.090s\n",
            "| Adam | epoch: 045 | loss: 0.24968 - acc: 0.9725 | val_loss: 0.34147 - val_acc: 0.8978 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 45, step (batch no.): 180 -- acc: 0.97, loss 0.25 -- iter 01000/01000, training for: 49.31s\n",
            "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.16804\u001b[0m\u001b[0m | time: 1.092s\n",
            "| Adam | epoch: 046 | loss: 0.16804 - acc: 0.9820 | val_loss: 0.35261 - val_acc: 0.9002 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 46, step (batch no.): 184 -- acc: 0.98, loss 0.17 -- iter 01000/01000, training for: 50.40s\n",
            "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.11289\u001b[0m\u001b[0m | time: 1.098s\n",
            "| Adam | epoch: 047 | loss: 0.11289 - acc: 0.9882 | val_loss: 0.34680 - val_acc: 0.9043 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 47, step (batch no.): 188 -- acc: 0.99, loss 0.11 -- iter 01000/01000, training for: 51.50s\n",
            "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.07569\u001b[0m\u001b[0m | time: 1.093s\n",
            "| Adam | epoch: 048 | loss: 0.07569 - acc: 0.9922 | val_loss: 0.34801 - val_acc: 0.9042 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 48, step (batch no.): 192 -- acc: 0.99, loss 0.08 -- iter 01000/01000, training for: 52.60s\n",
            "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.36485\u001b[0m\u001b[0m | time: 1.078s\n",
            "| Adam | epoch: 049 | loss: 0.36485 - acc: 0.9690 | val_loss: 0.43844 - val_acc: 0.8742 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 49, step (batch no.): 196 -- acc: 0.97, loss 0.36 -- iter 01000/01000, training for: 53.68s\n",
            "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.48183\u001b[0m\u001b[0m | time: 1.087s\n",
            "| Adam | epoch: 050 | loss: 0.48183 - acc: 0.9522 | val_loss: 0.56215 - val_acc: 0.8442 -- iter: 1000/1000\n",
            "--\n",
            "Epoch 50, step (batch no.): 200 -- acc: 0.95, loss 0.48 -- iter 01000/01000, training for: 54.76s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "0jyjHxlLnUQC",
        "outputId": "2396073f-1b4d-40c0-9c2a-88fb0b98f852"
      },
      "source": [
        "training_acc_for_epochs = [metrics['val_acc'] for metrics in scores.epoch_data]\n",
        "vall_acc_for_epochs = [metrics['acc'] for metrics in scores.epoch_data]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "plt.plot(range(1,len(training_acc_for_epochs)+1),training_acc_for_epochs, 'k--', range(1,len(vall_acc_for_epochs)+1), vall_acc_for_epochs, 'k:')\n",
        "plt.legend(('training_acc', 'vall_acc', ),\n",
        "           loc='lower center', shadow=True)\n",
        "plt.grid(False)\n",
        "plt.xlabel('Epochs --->')\n",
        "plt.ylabel('Accuracy--->')\n",
        "plt.title('Training vs validatrion accuracy')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Training vs validatrion accuracy')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiYHti3sWXYL"
      },
      "source": [
        "# Przesyłanie rozwiązań\n",
        "\n",
        "Zadania należy wysyłać na adresy `slezak@mimuw.edu.pl` i `m.matraszek@mimuw.edu.pl`  z tytułem o przedrostku [SI20-3]. \n",
        "Przesłane wykresy mogą być w dowolnej formie. Mile widziany byłby odpowiednio zmieniony ten notebook. Poniżej można znaleźć prosty sposób rysowania wykresów."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EegIH-JjWXYN"
      },
      "source": [
        "# Przykład użycia matplotlib do rysowania wykresów:\n",
        "\n",
        "# Wartości w punktach [0, 1, 2, ...]; podajemy tylko Y\n",
        "plt.plot([0, 1, 1.5, 6], label=\"asd\")\n",
        "# X podany w pierwszym argumencie, Y w drugim\n",
        "plt.plot([0, 2, 6], [7, 0, 0.6], label=\"bbb\")\n",
        "# rysuje legendę (linie podpisane jako `label`)\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rlAMKVbWXYY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhsINoKVWXYb"
      },
      "source": [
        "# Zadanie alternatywne\n",
        "\n",
        "Zamiast rozwiązywania powyższych zadań, proponujemy też kontynuację przygody z OpenAI Gym.\n",
        "W tej wersji chcielibyśmy użyć dość prostej metody łączącej uczenie ze wzmocnieniem i sieci neuronowe. Jest to Policy Gradient. W miarę przystępny opis tej metody jest zamieszczony tutaj: http://karpathy.github.io/2016/05/31/rl/\n",
        "\n",
        "Używając tflearn z tą metodą, rozwiąż środowisko CartPole-v0 z OpenAI Gym. A może nawet coś trudniejszego?\n",
        "Wykonanie:\n",
        "```python\n",
        "model.fit({'input': X}, {'target': Y}, n_epoch=1)\n",
        "```\n",
        "pozwoli na jednokrotną iterację propagacji wstecznej na podanych danych. Metodę tę można wykonać ponownie z innymi danymi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZUjO3qgPqM5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}